{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import my_func as mf\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "The training data and testing data have been flattened into m by n matrices, where m is the number of samples, and n number of features. Note that the dummy 1 has been added to the last column of train_data and test_data, so that the bias can be combined with the weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = np.loadtxt('my_train_data.txt', np.float32)\n",
    "test_data = np.loadtxt('my_test_data.txt', np.float32)\n",
    "train_label = np.loadtxt('my_train_label.txt', np.float32)\n",
    "test_label = np.loadtxt('my_test_label.txt', np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_train_data = 25112\n",
    "hm_test_data = 4982\n",
    "hm_classes = 5\n",
    "train_data /= 255\n",
    "test_data /= 255\n",
    "train_data = np.hstack((train_data, np.ones((hm_train_data, 1), dtype=np.float32)))\n",
    "test_data = np.hstack((test_data, np.ones((hm_test_data, 1), dtype=np.float32)))\n",
    "feature_dim = int(28 * 28) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup hyperparams for training the regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.0008\n",
    "learning_epoch = 25\n",
    "eval_interval = 20\n",
    "batch_size = 50\n",
    "reg_lambda = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=(None, feature_dim))\n",
    "y = tf.placeholder(tf.float32, shape=(None, hm_classes))\n",
    "weights = tf.Variable(tf.truncated_normal([feature_dim, hm_classes], 0.1, 0.01) / 10)\n",
    "w = np.zeros([feature_dim, hm_classes], dtype=np.float32)\n",
    "track_train_loss = np.array([])\n",
    "track_test_loss = np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Define the computational operation for the multi-class logistic regressor\n",
    "#### Compute the mean squared loss (Negative Conditional Log-likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loss = mf.compute_loss(weights, train_data, train_label, reg_lambda) / hm_train_data\n",
    "test_loss = mf.compute_loss(weights, test_data, test_label, reg_lambda) / hm_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the training and testing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soft_prob, _ = mf.compute_softmax(weights, x)\n",
    "correct = tf.equal(tf.argmax(soft_prob, 1), tf.argmax(y, 1))\n",
    "acc = tf.reduce_mean(tf.cast(correct, 'float'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the gradient and update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grads = mf.compute_gradients(weights, x, y, reg_lambda)\n",
    "weights = tf.assign_sub(weights, learning_rate * grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Initilize a session, and starting training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "training loss: 1.61061 , training accuracy: [0.22953171]\n",
      "testing loss 1.61073 test accuracy is [0.22681653]\n",
      "Epoch:  1\n",
      "iteration 5\n",
      "training loss: 1.29142 , training accuracy: [0.7879898]\n",
      "testing loss 1.27835 test accuracy is [0.80630267]\n",
      "iteration 25\n",
      "training loss: 0.777955 , training accuracy: [0.84549218]\n",
      "testing loss 0.751473 test accuracy is [0.86370933]\n",
      "iteration 45\n",
      "training loss: 0.595247 , training accuracy: [0.88149095]\n",
      "testing loss 0.563531 test accuracy is [0.90244883]\n",
      "iteration 65\n",
      "training loss: 0.503356 , training accuracy: [0.88905704]\n",
      "testing loss 0.468893 test accuracy is [0.91128063]\n",
      "iteration 85\n",
      "training loss: 0.448621 , training accuracy: [0.90195924]\n",
      "testing loss 0.413386 test accuracy is [0.92051387]\n",
      "iteration 105\n",
      "training loss: 0.412107 , training accuracy: [0.9014017]\n",
      "testing loss 0.379126 test accuracy is [0.91971093]\n",
      "iteration 125\n",
      "training loss: 0.382756 , training accuracy: [0.910043]\n",
      "testing loss 0.348763 test accuracy is [0.92673624]\n",
      "iteration 145\n",
      "training loss: 0.361817 , training accuracy: [0.91235268]\n",
      "testing loss 0.328942 test accuracy is [0.92733842]\n",
      "iteration 165\n",
      "training loss: 0.34539 , training accuracy: [0.91131729]\n",
      "testing loss 0.314817 test accuracy is [0.92673624]\n",
      "iteration 185\n",
      "training loss: 0.333027 , training accuracy: [0.91139692]\n",
      "testing loss 0.303214 test accuracy is [0.92492974]\n",
      "iteration 205\n",
      "training loss: 0.319987 , training accuracy: [0.91705161]\n",
      "testing loss 0.289843 test accuracy is [0.93034923]\n",
      "iteration 225\n",
      "training loss: 0.309964 , training accuracy: [0.91836572]\n",
      "testing loss 0.280848 test accuracy is [0.93034923]\n",
      "iteration 245\n",
      "training loss: 0.301232 , training accuracy: [0.91960019]\n",
      "testing loss 0.272734 test accuracy is [0.93255723]\n",
      "iteration 265\n",
      "training loss: 0.293436 , training accuracy: [0.92210895]\n",
      "testing loss 0.266149 test accuracy is [0.93396229]\n",
      "iteration 285\n",
      "training loss: 0.287572 , training accuracy: [0.92406023]\n",
      "testing loss 0.259666 test accuracy is [0.93596947]\n",
      "iteration 305\n",
      "training loss: 0.283184 , training accuracy: [0.92290539]\n",
      "testing loss 0.255706 test accuracy is [0.93255723]\n",
      "iteration 325\n",
      "training loss: 0.276148 , training accuracy: [0.92505574]\n",
      "testing loss 0.249649 test accuracy is [0.93657166]\n",
      "iteration 345\n",
      "training loss: 0.270319 , training accuracy: [0.9262504]\n",
      "testing loss 0.245418 test accuracy is [0.93657166]\n",
      "iteration 365\n",
      "training loss: 0.266179 , training accuracy: [0.9272061]\n",
      "testing loss 0.241702 test accuracy is [0.93737453]\n",
      "iteration 385\n",
      "training loss: 0.261781 , training accuracy: [0.9279229]\n",
      "testing loss 0.238321 test accuracy is [0.93777597]\n",
      "iteration 405\n",
      "training loss: 0.258577 , training accuracy: [0.92879897]\n",
      "testing loss 0.235038 test accuracy is [0.93737453]\n",
      "iteration 425\n",
      "training loss: 0.254616 , training accuracy: [0.9305113]\n",
      "testing loss 0.232303 test accuracy is [0.93837816]\n",
      "iteration 445\n",
      "training loss: 0.251657 , training accuracy: [0.92967504]\n",
      "testing loss 0.228976 test accuracy is [0.93797672]\n",
      "iteration 465\n",
      "training loss: 0.249082 , training accuracy: [0.92967504]\n",
      "testing loss 0.226689 test accuracy is [0.93697309]\n",
      "iteration 485\n",
      "training loss: 0.246853 , training accuracy: [0.9315865]\n",
      "testing loss 0.224694 test accuracy is [0.94018465]\n",
      "Epoch:  2\n",
      "iteration 5\n",
      "training loss: 0.243806 , training accuracy: [0.93067062]\n",
      "testing loss 0.222197 test accuracy is [0.94078684]\n",
      "iteration 25\n",
      "training loss: 0.240889 , training accuracy: [0.93186522]\n",
      "testing loss 0.219975 test accuracy is [0.94078684]\n",
      "iteration 45\n",
      "training loss: 0.238513 , training accuracy: [0.93286079]\n",
      "testing loss 0.217382 test accuracy is [0.94138902]\n",
      "iteration 65\n",
      "training loss: 0.237483 , training accuracy: [0.93098915]\n",
      "testing loss 0.217523 test accuracy is [0.93958253]\n",
      "iteration 85\n",
      "training loss: 0.23515 , training accuracy: [0.93325901]\n",
      "testing loss 0.215016 test accuracy is [0.94098753]\n",
      "iteration 105\n",
      "training loss: 0.232536 , training accuracy: [0.93389612]\n",
      "testing loss 0.212359 test accuracy is [0.94118828]\n",
      "iteration 125\n",
      "training loss: 0.231561 , training accuracy: [0.93389612]\n",
      "testing loss 0.211824 test accuracy is [0.93978322]\n",
      "iteration 145\n",
      "training loss: 0.229047 , training accuracy: [0.9348917]\n",
      "testing loss 0.209011 test accuracy is [0.94098753]\n",
      "iteration 165\n",
      "training loss: 0.228118 , training accuracy: [0.93461293]\n",
      "testing loss 0.208515 test accuracy is [0.94098753]\n",
      "iteration 185\n",
      "training loss: 0.226225 , training accuracy: [0.93552881]\n",
      "testing loss 0.205948 test accuracy is [0.94118828]\n",
      "iteration 205\n",
      "training loss: 0.224376 , training accuracy: [0.93552881]\n",
      "testing loss 0.205281 test accuracy is [0.94158971]\n",
      "iteration 225\n",
      "training loss: 0.224246 , training accuracy: [0.93616599]\n",
      "testing loss 0.205754 test accuracy is [0.94179046]\n",
      "iteration 245\n",
      "training loss: 0.222058 , training accuracy: [0.9368031]\n",
      "testing loss 0.203533 test accuracy is [0.9403854]\n",
      "iteration 265\n",
      "training loss: 0.220957 , training accuracy: [0.93732083]\n",
      "testing loss 0.202461 test accuracy is [0.94158971]\n",
      "iteration 285\n",
      "training loss: 0.219342 , training accuracy: [0.93740046]\n",
      "testing loss 0.201585 test accuracy is [0.94199115]\n",
      "iteration 305\n",
      "training loss: 0.218167 , training accuracy: [0.93648458]\n",
      "testing loss 0.200885 test accuracy is [0.94199115]\n",
      "iteration 325\n",
      "training loss: 0.217065 , training accuracy: [0.93720132]\n",
      "testing loss 0.199877 test accuracy is [0.94299477]\n",
      "iteration 345\n",
      "training loss: 0.215877 , training accuracy: [0.93740046]\n",
      "testing loss 0.19876 test accuracy is [0.94299477]\n",
      "iteration 365\n",
      "training loss: 0.214784 , training accuracy: [0.93799776]\n",
      "testing loss 0.197926 test accuracy is [0.94239259]\n",
      "iteration 385\n",
      "training loss: 0.214363 , training accuracy: [0.93787831]\n",
      "testing loss 0.197186 test accuracy is [0.94279408]\n",
      "iteration 405\n",
      "training loss: 0.212809 , training accuracy: [0.93827653]\n",
      "testing loss 0.196478 test accuracy is [0.94239259]\n",
      "iteration 425\n",
      "training loss: 0.212327 , training accuracy: [0.93779868]\n",
      "testing loss 0.19682 test accuracy is [0.94439983]\n",
      "iteration 445\n",
      "training loss: 0.211773 , training accuracy: [0.93851542]\n",
      "testing loss 0.196098 test accuracy is [0.94279408]\n",
      "iteration 465\n",
      "training loss: 0.210442 , training accuracy: [0.93891364]\n",
      "testing loss 0.195574 test accuracy is [0.94339621]\n",
      "iteration 485\n",
      "training loss: 0.210095 , training accuracy: [0.93947119]\n",
      "testing loss 0.194552 test accuracy is [0.94199115]\n",
      "Epoch:  3\n",
      "iteration 5\n",
      "training loss: 0.208344 , training accuracy: [0.93959063]\n",
      "testing loss 0.192944 test accuracy is [0.94299477]\n",
      "iteration 25\n",
      "training loss: 0.207472 , training accuracy: [0.94006848]\n",
      "testing loss 0.192373 test accuracy is [0.94299477]\n",
      "iteration 45\n",
      "training loss: 0.207237 , training accuracy: [0.94074547]\n",
      "testing loss 0.191842 test accuracy is [0.94359696]\n",
      "iteration 65\n",
      "training loss: 0.2076 , training accuracy: [0.93927205]\n",
      "testing loss 0.192632 test accuracy is [0.94359696]\n",
      "iteration 85\n",
      "training loss: 0.20631 , training accuracy: [0.9397499]\n",
      "testing loss 0.191711 test accuracy is [0.9439984]\n",
      "iteration 105\n",
      "training loss: 0.205104 , training accuracy: [0.94022781]\n",
      "testing loss 0.190342 test accuracy is [0.94379765]\n",
      "iteration 125\n",
      "training loss: 0.204489 , training accuracy: [0.93998885]\n",
      "testing loss 0.189601 test accuracy is [0.94379765]\n",
      "iteration 145\n",
      "training loss: 0.204422 , training accuracy: [0.93931186]\n",
      "testing loss 0.190231 test accuracy is [0.9421919]\n",
      "iteration 165\n",
      "training loss: 0.204214 , training accuracy: [0.94034725]\n",
      "testing loss 0.189617 test accuracy is [0.94379765]\n",
      "iteration 185\n",
      "training loss: 0.202193 , training accuracy: [0.94090474]\n",
      "testing loss 0.187718 test accuracy is [0.94419914]\n",
      "iteration 205\n",
      "training loss: 0.201918 , training accuracy: [0.94122332]\n",
      "testing loss 0.187951 test accuracy is [0.94359696]\n",
      "iteration 225\n",
      "training loss: 0.201329 , training accuracy: [0.94166136]\n",
      "testing loss 0.187754 test accuracy is [0.94359696]\n",
      "iteration 245\n",
      "training loss: 0.200753 , training accuracy: [0.9414224]\n",
      "testing loss 0.187094 test accuracy is [0.9439984]\n",
      "iteration 265\n",
      "training loss: 0.200279 , training accuracy: [0.94182062]\n",
      "testing loss 0.187014 test accuracy is [0.94339621]\n",
      "iteration 285\n",
      "training loss: 0.199946 , training accuracy: [0.94138259]\n",
      "testing loss 0.186651 test accuracy is [0.94540346]\n",
      "iteration 305\n",
      "training loss: 0.198869 , training accuracy: [0.94178081]\n",
      "testing loss 0.185555 test accuracy is [0.94480127]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 325\n",
      "training loss: 0.198089 , training accuracy: [0.94174099]\n",
      "testing loss 0.184799 test accuracy is [0.94580489]\n",
      "iteration 345\n",
      "training loss: 0.197674 , training accuracy: [0.94174099]\n",
      "testing loss 0.184705 test accuracy is [0.94520271]\n",
      "iteration 365\n",
      "training loss: 0.19768 , training accuracy: [0.94217902]\n",
      "testing loss 0.185569 test accuracy is [0.9439984]\n",
      "iteration 385\n",
      "training loss: 0.196401 , training accuracy: [0.94205958]\n",
      "testing loss 0.183934 test accuracy is [0.94439983]\n",
      "iteration 405\n",
      "training loss: 0.196072 , training accuracy: [0.9428162]\n",
      "testing loss 0.183518 test accuracy is [0.9439984]\n",
      "iteration 425\n",
      "training loss: 0.196028 , training accuracy: [0.9428162]\n",
      "testing loss 0.183082 test accuracy is [0.94439983]\n",
      "iteration 445\n",
      "training loss: 0.195416 , training accuracy: [0.94229853]\n",
      "testing loss 0.182685 test accuracy is [0.94500202]\n",
      "iteration 465\n",
      "training loss: 0.195954 , training accuracy: [0.94209939]\n",
      "testing loss 0.182524 test accuracy is [0.94439983]\n",
      "iteration 485\n",
      "training loss: 0.194263 , training accuracy: [0.94297546]\n",
      "testing loss 0.182415 test accuracy is [0.94439983]\n",
      "Epoch:  4\n",
      "iteration 5\n",
      "training loss: 0.193625 , training accuracy: [0.94249761]\n",
      "testing loss 0.181483 test accuracy is [0.94480127]\n",
      "iteration 25\n",
      "training loss: 0.19374 , training accuracy: [0.94186044]\n",
      "testing loss 0.181695 test accuracy is [0.94439983]\n",
      "iteration 45\n",
      "training loss: 0.193164 , training accuracy: [0.94237816]\n",
      "testing loss 0.181349 test accuracy is [0.9439984]\n",
      "iteration 65\n",
      "training loss: 0.193839 , training accuracy: [0.94249761]\n",
      "testing loss 0.181764 test accuracy is [0.94419914]\n",
      "iteration 85\n",
      "training loss: 0.192288 , training accuracy: [0.94313478]\n",
      "testing loss 0.18105 test accuracy is [0.94460058]\n",
      "iteration 105\n",
      "training loss: 0.191963 , training accuracy: [0.94309491]\n",
      "testing loss 0.180058 test accuracy is [0.94500202]\n",
      "iteration 125\n",
      "training loss: 0.191556 , training accuracy: [0.94353294]\n",
      "testing loss 0.17952 test accuracy is [0.94600564]\n",
      "iteration 145\n",
      "training loss: 0.191096 , training accuracy: [0.94313478]\n",
      "testing loss 0.178816 test accuracy is [0.94520271]\n",
      "iteration 165\n",
      "training loss: 0.19113 , training accuracy: [0.9424578]\n",
      "testing loss 0.179623 test accuracy is [0.94379765]\n",
      "iteration 185\n",
      "training loss: 0.190777 , training accuracy: [0.94249761]\n",
      "testing loss 0.179102 test accuracy is [0.94480127]\n",
      "iteration 205\n",
      "training loss: 0.190196 , training accuracy: [0.94357282]\n",
      "testing loss 0.178591 test accuracy is [0.94419914]\n",
      "iteration 225\n",
      "training loss: 0.18979 , training accuracy: [0.94405067]\n",
      "testing loss 0.178702 test accuracy is [0.94620633]\n",
      "iteration 245\n",
      "training loss: 0.189494 , training accuracy: [0.9431746]\n",
      "testing loss 0.179503 test accuracy is [0.94460058]\n",
      "iteration 265\n",
      "training loss: 0.188901 , training accuracy: [0.94385153]\n",
      "testing loss 0.178859 test accuracy is [0.94520271]\n",
      "iteration 285\n",
      "training loss: 0.188712 , training accuracy: [0.94409049]\n",
      "testing loss 0.178418 test accuracy is [0.94560415]\n",
      "iteration 305\n",
      "training loss: 0.188599 , training accuracy: [0.94452852]\n",
      "testing loss 0.178946 test accuracy is [0.94520271]\n",
      "iteration 325\n",
      "training loss: 0.188324 , training accuracy: [0.94385153]\n",
      "testing loss 0.179197 test accuracy is [0.94500202]\n",
      "iteration 345\n",
      "training loss: 0.187948 , training accuracy: [0.94440907]\n",
      "testing loss 0.178035 test accuracy is [0.94560415]\n",
      "iteration 365\n",
      "training loss: 0.187375 , training accuracy: [0.94460815]\n",
      "testing loss 0.177344 test accuracy is [0.94560415]\n",
      "iteration 385\n",
      "training loss: 0.186829 , training accuracy: [0.94476742]\n",
      "testing loss 0.176688 test accuracy is [0.94600564]\n",
      "iteration 405\n",
      "training loss: 0.186437 , training accuracy: [0.94440907]\n",
      "testing loss 0.176295 test accuracy is [0.94600564]\n",
      "iteration 425\n",
      "training loss: 0.187349 , training accuracy: [0.94357282]\n",
      "testing loss 0.177346 test accuracy is [0.94660777]\n",
      "iteration 445\n",
      "training loss: 0.187702 , training accuracy: [0.94333386]\n",
      "testing loss 0.177447 test accuracy is [0.94640708]\n",
      "iteration 465\n",
      "training loss: 0.185724 , training accuracy: [0.9443692]\n",
      "testing loss 0.176351 test accuracy is [0.94600564]\n",
      "iteration 485\n",
      "training loss: 0.186428 , training accuracy: [0.94349313]\n",
      "testing loss 0.177556 test accuracy is [0.94560415]\n",
      "Epoch:  5\n",
      "iteration 5\n",
      "training loss: 0.185401 , training accuracy: [0.94440907]\n",
      "testing loss 0.176673 test accuracy is [0.94560415]\n",
      "iteration 25\n",
      "training loss: 0.187382 , training accuracy: [0.94293565]\n",
      "testing loss 0.179263 test accuracy is [0.94460058]\n",
      "iteration 45\n",
      "training loss: 0.184757 , training accuracy: [0.94393116]\n",
      "testing loss 0.175967 test accuracy is [0.94480127]\n",
      "iteration 65\n",
      "training loss: 0.184272 , training accuracy: [0.9443692]\n",
      "testing loss 0.175197 test accuracy is [0.94640708]\n",
      "iteration 85\n",
      "training loss: 0.183976 , training accuracy: [0.94468778]\n",
      "testing loss 0.174826 test accuracy is [0.94620633]\n",
      "iteration 105\n",
      "training loss: 0.183893 , training accuracy: [0.94428957]\n",
      "testing loss 0.175241 test accuracy is [0.94560415]\n",
      "iteration 125\n",
      "training loss: 0.184435 , training accuracy: [0.94496655]\n",
      "testing loss 0.175661 test accuracy is [0.94560415]\n",
      "iteration 145\n",
      "training loss: 0.183516 , training accuracy: [0.94500637]\n",
      "testing loss 0.174839 test accuracy is [0.94700921]\n",
      "iteration 165\n",
      "training loss: 0.183211 , training accuracy: [0.94496655]\n",
      "testing loss 0.174152 test accuracy is [0.94680852]\n",
      "iteration 185\n",
      "training loss: 0.183341 , training accuracy: [0.94488692]\n",
      "testing loss 0.175823 test accuracy is [0.94600564]\n",
      "iteration 205\n",
      "training loss: 0.182864 , training accuracy: [0.94516563]\n",
      "testing loss 0.173844 test accuracy is [0.94720995]\n",
      "iteration 225\n",
      "training loss: 0.18206 , training accuracy: [0.94540459]\n",
      "testing loss 0.173635 test accuracy is [0.94640708]\n",
      "iteration 245\n",
      "training loss: 0.182065 , training accuracy: [0.94580281]\n",
      "testing loss 0.17324 test accuracy is [0.9474107]\n",
      "iteration 265\n",
      "training loss: 0.182319 , training accuracy: [0.94548422]\n",
      "testing loss 0.174155 test accuracy is [0.94660777]\n",
      "iteration 285\n",
      "training loss: 0.181862 , training accuracy: [0.94556385]\n",
      "testing loss 0.173888 test accuracy is [0.94580489]\n",
      "iteration 305\n",
      "training loss: 0.181803 , training accuracy: [0.94544441]\n",
      "testing loss 0.174165 test accuracy is [0.94580489]\n",
      "iteration 325\n",
      "training loss: 0.181214 , training accuracy: [0.94608158]\n",
      "testing loss 0.173002 test accuracy is [0.94660777]\n",
      "iteration 345\n",
      "training loss: 0.181001 , training accuracy: [0.94604176]\n",
      "testing loss 0.172363 test accuracy is [0.94801283]\n",
      "iteration 365\n",
      "training loss: 0.181553 , training accuracy: [0.94632047]\n",
      "testing loss 0.173333 test accuracy is [0.94680852]\n",
      "iteration 385\n",
      "training loss: 0.180396 , training accuracy: [0.94584262]\n",
      "testing loss 0.172321 test accuracy is [0.94600564]\n",
      "iteration 405\n",
      "training loss: 0.179976 , training accuracy: [0.94628066]\n",
      "testing loss 0.172159 test accuracy is [0.94660777]\n",
      "iteration 425\n",
      "training loss: 0.180971 , training accuracy: [0.94564354]\n",
      "testing loss 0.172596 test accuracy is [0.94680852]\n",
      "iteration 445\n",
      "training loss: 0.179859 , training accuracy: [0.94548422]\n",
      "testing loss 0.171989 test accuracy is [0.94660777]\n",
      "iteration 465\n",
      "training loss: 0.179666 , training accuracy: [0.94576299]\n",
      "testing loss 0.171628 test accuracy is [0.94600564]\n",
      "iteration 485\n",
      "training loss: 0.179347 , training accuracy: [0.94556385]\n",
      "testing loss 0.170751 test accuracy is [0.94720995]\n",
      "Epoch:  6\n",
      "iteration 5\n",
      "training loss: 0.178751 , training accuracy: [0.94604176]\n",
      "testing loss 0.170983 test accuracy is [0.94720995]\n",
      "iteration 25\n",
      "training loss: 0.178559 , training accuracy: [0.94643992]\n",
      "testing loss 0.170742 test accuracy is [0.94761139]\n",
      "iteration 45\n",
      "training loss: 0.178588 , training accuracy: [0.94612139]\n",
      "testing loss 0.170481 test accuracy is [0.94781214]\n",
      "iteration 65\n",
      "training loss: 0.178362 , training accuracy: [0.94663906]\n",
      "testing loss 0.171075 test accuracy is [0.94660777]\n",
      "iteration 85\n",
      "training loss: 0.17891 , training accuracy: [0.94580281]\n",
      "testing loss 0.171262 test accuracy is [0.94700921]\n",
      "iteration 105\n",
      "training loss: 0.178228 , training accuracy: [0.94628066]\n",
      "testing loss 0.170556 test accuracy is [0.9474107]\n",
      "iteration 125\n",
      "training loss: 0.17829 , training accuracy: [0.94588244]\n",
      "testing loss 0.171098 test accuracy is [0.94700921]\n",
      "iteration 145\n",
      "training loss: 0.178353 , training accuracy: [0.94584262]\n",
      "testing loss 0.171142 test accuracy is [0.94700921]\n",
      "iteration 165\n",
      "training loss: 0.177806 , training accuracy: [0.94608158]\n",
      "testing loss 0.170628 test accuracy is [0.94640708]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 185\n",
      "training loss: 0.178577 , training accuracy: [0.94651961]\n",
      "testing loss 0.170578 test accuracy is [0.94660777]\n",
      "iteration 205\n",
      "training loss: 0.177952 , training accuracy: [0.94636029]\n",
      "testing loss 0.170915 test accuracy is [0.94761139]\n",
      "iteration 225\n",
      "training loss: 0.176789 , training accuracy: [0.94687802]\n",
      "testing loss 0.169956 test accuracy is [0.94761139]\n",
      "iteration 245\n",
      "training loss: 0.176604 , training accuracy: [0.94727618]\n",
      "testing loss 0.169541 test accuracy is [0.94781214]\n",
      "iteration 265\n",
      "training loss: 0.177143 , training accuracy: [0.94643992]\n",
      "testing loss 0.170154 test accuracy is [0.94660777]\n",
      "iteration 285\n",
      "training loss: 0.176862 , training accuracy: [0.94655943]\n",
      "testing loss 0.169263 test accuracy is [0.9474107]\n",
      "iteration 305\n",
      "training loss: 0.176576 , training accuracy: [0.94659925]\n",
      "testing loss 0.169166 test accuracy is [0.94700921]\n",
      "iteration 325\n",
      "training loss: 0.175898 , training accuracy: [0.9470771]\n",
      "testing loss 0.168916 test accuracy is [0.94781214]\n",
      "iteration 345\n",
      "training loss: 0.176286 , training accuracy: [0.94723636]\n",
      "testing loss 0.16885 test accuracy is [0.94761139]\n",
      "iteration 365\n",
      "training loss: 0.176588 , training accuracy: [0.94636029]\n",
      "testing loss 0.170789 test accuracy is [0.9474107]\n",
      "iteration 385\n",
      "training loss: 0.178142 , training accuracy: [0.94544441]\n",
      "testing loss 0.172859 test accuracy is [0.94680852]\n",
      "iteration 405\n",
      "training loss: 0.175984 , training accuracy: [0.94663906]\n",
      "testing loss 0.169584 test accuracy is [0.94680852]\n",
      "iteration 425\n",
      "training loss: 0.174985 , training accuracy: [0.94747531]\n",
      "testing loss 0.1686 test accuracy is [0.94801283]\n",
      "iteration 445\n",
      "training loss: 0.175083 , training accuracy: [0.94735587]\n",
      "testing loss 0.168561 test accuracy is [0.9474107]\n",
      "iteration 465\n",
      "training loss: 0.17523 , training accuracy: [0.9470771]\n",
      "testing loss 0.169325 test accuracy is [0.94620633]\n",
      "iteration 485\n",
      "training loss: 0.175047 , training accuracy: [0.94703728]\n",
      "testing loss 0.169461 test accuracy is [0.94680852]\n",
      "Epoch:  7\n",
      "iteration 5\n",
      "training loss: 0.174531 , training accuracy: [0.94751513]\n",
      "testing loss 0.168398 test accuracy is [0.94761139]\n",
      "iteration 25\n",
      "training loss: 0.174904 , training accuracy: [0.94735587]\n",
      "testing loss 0.167858 test accuracy is [0.94781214]\n",
      "iteration 45\n",
      "training loss: 0.174769 , training accuracy: [0.94771427]\n",
      "testing loss 0.16779 test accuracy is [0.9474107]\n",
      "iteration 65\n",
      "training loss: 0.174709 , training accuracy: [0.94755495]\n",
      "testing loss 0.168169 test accuracy is [0.94781214]\n",
      "iteration 85\n",
      "training loss: 0.174594 , training accuracy: [0.94695765]\n",
      "testing loss 0.16821 test accuracy is [0.94781214]\n",
      "iteration 105\n",
      "training loss: 0.17387 , training accuracy: [0.94731605]\n",
      "testing loss 0.168211 test accuracy is [0.94761139]\n",
      "iteration 125\n",
      "training loss: 0.173469 , training accuracy: [0.94747531]\n",
      "testing loss 0.167554 test accuracy is [0.94761139]\n",
      "iteration 145\n",
      "training loss: 0.173586 , training accuracy: [0.94755495]\n",
      "testing loss 0.167454 test accuracy is [0.94801283]\n",
      "iteration 165\n",
      "training loss: 0.17372 , training accuracy: [0.94679832]\n",
      "testing loss 0.168511 test accuracy is [0.94660777]\n",
      "iteration 185\n",
      "training loss: 0.173366 , training accuracy: [0.94735587]\n",
      "testing loss 0.167275 test accuracy is [0.94781214]\n",
      "iteration 205\n",
      "training loss: 0.17437 , training accuracy: [0.94671869]\n",
      "testing loss 0.16854 test accuracy is [0.94680852]\n",
      "iteration 225\n",
      "training loss: 0.173216 , training accuracy: [0.94767439]\n",
      "testing loss 0.167045 test accuracy is [0.94761139]\n",
      "iteration 245\n",
      "training loss: 0.173019 , training accuracy: [0.94771427]\n",
      "testing loss 0.166513 test accuracy is [0.94801283]\n",
      "iteration 265\n",
      "training loss: 0.172893 , training accuracy: [0.9474355]\n",
      "testing loss 0.167471 test accuracy is [0.9474107]\n",
      "iteration 285\n",
      "training loss: 0.173093 , training accuracy: [0.94715673]\n",
      "testing loss 0.167775 test accuracy is [0.94720995]\n",
      "iteration 305\n",
      "training loss: 0.1729 , training accuracy: [0.94723636]\n",
      "testing loss 0.167805 test accuracy is [0.94660777]\n",
      "iteration 325\n",
      "training loss: 0.172545 , training accuracy: [0.94715673]\n",
      "testing loss 0.167488 test accuracy is [0.94660777]\n",
      "iteration 345\n",
      "training loss: 0.17203 , training accuracy: [0.94755495]\n",
      "testing loss 0.166604 test accuracy is [0.9474107]\n",
      "iteration 365\n",
      "training loss: 0.172387 , training accuracy: [0.94759476]\n",
      "testing loss 0.16689 test accuracy is [0.94801283]\n",
      "iteration 385\n",
      "training loss: 0.172065 , training accuracy: [0.9480328]\n",
      "testing loss 0.166408 test accuracy is [0.94781214]\n",
      "iteration 405\n",
      "training loss: 0.171683 , training accuracy: [0.94791335]\n",
      "testing loss 0.166342 test accuracy is [0.94801283]\n",
      "iteration 425\n",
      "training loss: 0.171596 , training accuracy: [0.94783372]\n",
      "testing loss 0.166346 test accuracy is [0.94801283]\n",
      "iteration 445\n",
      "training loss: 0.171136 , training accuracy: [0.94807261]\n",
      "testing loss 0.165817 test accuracy is [0.94821358]\n",
      "iteration 465\n",
      "training loss: 0.171892 , training accuracy: [0.94755495]\n",
      "testing loss 0.167343 test accuracy is [0.9474107]\n",
      "iteration 485\n",
      "training loss: 0.171377 , training accuracy: [0.94835138]\n",
      "testing loss 0.165972 test accuracy is [0.94761139]\n",
      "Epoch:  8\n",
      "iteration 5\n",
      "training loss: 0.1711 , training accuracy: [0.94843102]\n",
      "testing loss 0.165822 test accuracy is [0.94720995]\n",
      "iteration 25\n",
      "training loss: 0.170768 , training accuracy: [0.94863015]\n",
      "testing loss 0.16575 test accuracy is [0.94861501]\n",
      "iteration 45\n",
      "training loss: 0.170991 , training accuracy: [0.94851065]\n",
      "testing loss 0.166581 test accuracy is [0.94821358]\n",
      "iteration 65\n",
      "training loss: 0.170477 , training accuracy: [0.94870979]\n",
      "testing loss 0.165702 test accuracy is [0.94801283]\n",
      "iteration 85\n",
      "training loss: 0.170597 , training accuracy: [0.94823194]\n",
      "testing loss 0.16581 test accuracy is [0.94821358]\n",
      "iteration 105\n",
      "training loss: 0.170961 , training accuracy: [0.9481523]\n",
      "testing loss 0.165337 test accuracy is [0.94841427]\n",
      "iteration 125\n",
      "training loss: 0.170684 , training accuracy: [0.94799298]\n",
      "testing loss 0.165791 test accuracy is [0.94761139]\n",
      "iteration 145\n",
      "training loss: 0.170158 , training accuracy: [0.94863015]\n",
      "testing loss 0.16521 test accuracy is [0.94821358]\n",
      "iteration 165\n",
      "training loss: 0.170144 , training accuracy: [0.94894868]\n",
      "testing loss 0.164948 test accuracy is [0.94781214]\n",
      "iteration 185\n",
      "training loss: 0.170505 , training accuracy: [0.94791335]\n",
      "testing loss 0.166374 test accuracy is [0.9474107]\n",
      "iteration 205\n",
      "training loss: 0.170112 , training accuracy: [0.94843102]\n",
      "testing loss 0.166051 test accuracy is [0.94821358]\n",
      "iteration 225\n",
      "training loss: 0.170184 , training accuracy: [0.94894868]\n",
      "testing loss 0.166322 test accuracy is [0.94801283]\n",
      "iteration 245\n",
      "training loss: 0.169597 , training accuracy: [0.94894868]\n",
      "testing loss 0.165094 test accuracy is [0.94801283]\n",
      "iteration 265\n",
      "training loss: 0.169796 , training accuracy: [0.94843102]\n",
      "testing loss 0.166028 test accuracy is [0.94821358]\n",
      "iteration 285\n",
      "training loss: 0.17029 , training accuracy: [0.94835138]\n",
      "testing loss 0.165681 test accuracy is [0.94720995]\n",
      "iteration 305\n",
      "training loss: 0.169201 , training accuracy: [0.94898856]\n",
      "testing loss 0.164378 test accuracy is [0.94901645]\n",
      "iteration 325\n",
      "training loss: 0.169918 , training accuracy: [0.94847083]\n",
      "testing loss 0.165309 test accuracy is [0.94761139]\n",
      "iteration 345\n",
      "training loss: 0.169077 , training accuracy: [0.94878942]\n",
      "testing loss 0.164136 test accuracy is [0.94861501]\n",
      "iteration 365\n",
      "training loss: 0.170384 , training accuracy: [0.94827175]\n",
      "testing loss 0.164849 test accuracy is [0.94801283]\n",
      "iteration 385\n",
      "training loss: 0.169109 , training accuracy: [0.94851065]\n",
      "testing loss 0.164907 test accuracy is [0.94761139]\n",
      "iteration 405\n",
      "training loss: 0.168443 , training accuracy: [0.94926727]\n",
      "testing loss 0.164146 test accuracy is [0.94801283]\n",
      "iteration 425\n",
      "training loss: 0.168431 , training accuracy: [0.94898856]\n",
      "testing loss 0.164011 test accuracy is [0.94861501]\n",
      "iteration 445\n",
      "training loss: 0.168483 , training accuracy: [0.94894868]\n",
      "testing loss 0.163858 test accuracy is [0.94881576]\n",
      "iteration 465\n",
      "training loss: 0.168136 , training accuracy: [0.949108]\n",
      "testing loss 0.164185 test accuracy is [0.94801283]\n",
      "iteration 485\n",
      "training loss: 0.167988 , training accuracy: [0.94914782]\n",
      "testing loss 0.163703 test accuracy is [0.94801283]\n",
      "Epoch:  9\n",
      "iteration 5\n",
      "training loss: 0.169845 , training accuracy: [0.9493469]\n",
      "testing loss 0.165769 test accuracy is [0.9492172]\n",
      "iteration 25\n",
      "training loss: 0.168101 , training accuracy: [0.94966549]\n",
      "testing loss 0.163897 test accuracy is [0.9492172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 45\n",
      "training loss: 0.168901 , training accuracy: [0.94855052]\n",
      "testing loss 0.163903 test accuracy is [0.94861501]\n",
      "iteration 65\n",
      "training loss: 0.168227 , training accuracy: [0.94958586]\n",
      "testing loss 0.164212 test accuracy is [0.94901645]\n",
      "iteration 85\n",
      "training loss: 0.168123 , training accuracy: [0.95010352]\n",
      "testing loss 0.164524 test accuracy is [0.94961864]\n",
      "iteration 105\n",
      "training loss: 0.168095 , training accuracy: [0.94954604]\n",
      "testing loss 0.16346 test accuracy is [0.94881576]\n",
      "iteration 125\n",
      "training loss: 0.167401 , training accuracy: [0.94950622]\n",
      "testing loss 0.163823 test accuracy is [0.94901645]\n",
      "iteration 145\n",
      "training loss: 0.167358 , training accuracy: [0.94930708]\n",
      "testing loss 0.163578 test accuracy is [0.94801283]\n",
      "iteration 165\n",
      "training loss: 0.167107 , training accuracy: [0.94922745]\n",
      "testing loss 0.163499 test accuracy is [0.94841427]\n",
      "iteration 185\n",
      "training loss: 0.167219 , training accuracy: [0.94950622]\n",
      "testing loss 0.16389 test accuracy is [0.94821358]\n",
      "iteration 205\n",
      "training loss: 0.16702 , training accuracy: [0.94946641]\n",
      "testing loss 0.163395 test accuracy is [0.94901645]\n",
      "iteration 225\n",
      "training loss: 0.166986 , training accuracy: [0.94926727]\n",
      "testing loss 0.162944 test accuracy is [0.94821358]\n",
      "iteration 245\n",
      "training loss: 0.166612 , training accuracy: [0.94914782]\n",
      "testing loss 0.163381 test accuracy is [0.94841427]\n",
      "iteration 265\n",
      "training loss: 0.166676 , training accuracy: [0.94942659]\n",
      "testing loss 0.162664 test accuracy is [0.94861501]\n",
      "iteration 285\n",
      "training loss: 0.166623 , training accuracy: [0.94906819]\n",
      "testing loss 0.163119 test accuracy is [0.9492172]\n",
      "iteration 305\n",
      "training loss: 0.16692 , training accuracy: [0.949108]\n",
      "testing loss 0.162986 test accuracy is [0.94881576]\n",
      "iteration 325\n",
      "training loss: 0.166222 , training accuracy: [0.94966549]\n",
      "testing loss 0.162396 test accuracy is [0.94881576]\n",
      "iteration 345\n",
      "training loss: 0.167167 , training accuracy: [0.94918764]\n",
      "testing loss 0.162458 test accuracy is [0.94861501]\n",
      "iteration 365\n",
      "training loss: 0.166463 , training accuracy: [0.94926727]\n",
      "testing loss 0.162448 test accuracy is [0.94861501]\n",
      "iteration 385\n",
      "training loss: 0.166315 , training accuracy: [0.94958586]\n",
      "testing loss 0.162775 test accuracy is [0.94941789]\n",
      "iteration 405\n",
      "training loss: 0.166372 , training accuracy: [0.94902837]\n",
      "testing loss 0.163549 test accuracy is [0.94801283]\n",
      "iteration 425\n",
      "training loss: 0.166517 , training accuracy: [0.94863015]\n",
      "testing loss 0.163574 test accuracy is [0.94761139]\n",
      "iteration 445\n",
      "training loss: 0.166136 , training accuracy: [0.94886905]\n",
      "testing loss 0.162985 test accuracy is [0.94821358]\n",
      "iteration 465\n",
      "training loss: 0.165788 , training accuracy: [0.9493469]\n",
      "testing loss 0.162568 test accuracy is [0.94801283]\n",
      "iteration 485\n",
      "training loss: 0.165685 , training accuracy: [0.94938678]\n",
      "testing loss 0.162691 test accuracy is [0.94881576]\n",
      "Epoch:  10\n",
      "iteration 5\n",
      "training loss: 0.166506 , training accuracy: [0.94898856]\n",
      "testing loss 0.164039 test accuracy is [0.94781214]\n",
      "iteration 25\n",
      "training loss: 0.167156 , training accuracy: [0.94890887]\n",
      "testing loss 0.164234 test accuracy is [0.94801283]\n",
      "iteration 45\n",
      "training loss: 0.166356 , training accuracy: [0.94882923]\n",
      "testing loss 0.163489 test accuracy is [0.94801283]\n",
      "iteration 65\n",
      "training loss: 0.168468 , training accuracy: [0.94930708]\n",
      "testing loss 0.165409 test accuracy is [0.9474107]\n",
      "iteration 85\n",
      "training loss: 0.16643 , training accuracy: [0.94926727]\n",
      "testing loss 0.162761 test accuracy is [0.94781214]\n",
      "iteration 105\n",
      "training loss: 0.166312 , training accuracy: [0.94938678]\n",
      "testing loss 0.16328 test accuracy is [0.94821358]\n",
      "iteration 125\n",
      "training loss: 0.165798 , training accuracy: [0.94958586]\n",
      "testing loss 0.162871 test accuracy is [0.9492172]\n",
      "iteration 145\n",
      "training loss: 0.16522 , training accuracy: [0.94938678]\n",
      "testing loss 0.161904 test accuracy is [0.9492172]\n",
      "iteration 165\n",
      "training loss: 0.164966 , training accuracy: [0.94994426]\n",
      "testing loss 0.161535 test accuracy is [0.95042151]\n",
      "iteration 185\n",
      "training loss: 0.164669 , training accuracy: [0.94962567]\n",
      "testing loss 0.161628 test accuracy is [0.95042151]\n",
      "iteration 205\n",
      "training loss: 0.165169 , training accuracy: [0.94994426]\n",
      "testing loss 0.161952 test accuracy is [0.95002007]\n",
      "iteration 225\n",
      "training loss: 0.165726 , training accuracy: [0.95002389]\n",
      "testing loss 0.162317 test accuracy is [0.94861501]\n",
      "iteration 245\n",
      "training loss: 0.164544 , training accuracy: [0.94994426]\n",
      "testing loss 0.161746 test accuracy is [0.94941789]\n",
      "iteration 265\n",
      "training loss: 0.165919 , training accuracy: [0.94986463]\n",
      "testing loss 0.162882 test accuracy is [0.94861501]\n",
      "iteration 285\n",
      "training loss: 0.164254 , training accuracy: [0.94994426]\n",
      "testing loss 0.161537 test accuracy is [0.95062226]\n",
      "iteration 305\n",
      "training loss: 0.164642 , training accuracy: [0.95002389]\n",
      "testing loss 0.163088 test accuracy is [0.94861501]\n",
      "iteration 325\n",
      "training loss: 0.16535 , training accuracy: [0.94950622]\n",
      "testing loss 0.163092 test accuracy is [0.94881576]\n",
      "iteration 345\n",
      "training loss: 0.16441 , training accuracy: [0.94950622]\n",
      "testing loss 0.162366 test accuracy is [0.94861501]\n",
      "iteration 365\n",
      "training loss: 0.164475 , training accuracy: [0.94958586]\n",
      "testing loss 0.162485 test accuracy is [0.94801283]\n",
      "iteration 385\n",
      "training loss: 0.164571 , training accuracy: [0.95034248]\n",
      "testing loss 0.163196 test accuracy is [0.94901645]\n",
      "iteration 405\n",
      "training loss: 0.163791 , training accuracy: [0.95034248]\n",
      "testing loss 0.161632 test accuracy is [0.94941789]\n",
      "iteration 425\n",
      "training loss: 0.164251 , training accuracy: [0.94974512]\n",
      "testing loss 0.161377 test accuracy is [0.94881576]\n",
      "iteration 445\n",
      "training loss: 0.163638 , training accuracy: [0.95026284]\n",
      "testing loss 0.161625 test accuracy is [0.94941789]\n",
      "iteration 465\n",
      "training loss: 0.16433 , training accuracy: [0.94998407]\n",
      "testing loss 0.162958 test accuracy is [0.94901645]\n",
      "iteration 485\n",
      "training loss: 0.164375 , training accuracy: [0.95054156]\n",
      "testing loss 0.162271 test accuracy is [0.94901645]\n",
      "Epoch:  11\n",
      "iteration 5\n",
      "training loss: 0.163407 , training accuracy: [0.95042211]\n",
      "testing loss 0.16103 test accuracy is [0.94961864]\n",
      "iteration 25\n",
      "training loss: 0.163656 , training accuracy: [0.95018315]\n",
      "testing loss 0.161133 test accuracy is [0.94941789]\n",
      "iteration 45\n",
      "training loss: 0.16391 , training accuracy: [0.95026284]\n",
      "testing loss 0.1613 test accuracy is [0.94961864]\n",
      "iteration 65\n",
      "training loss: 0.163139 , training accuracy: [0.94998407]\n",
      "testing loss 0.160537 test accuracy is [0.94941789]\n",
      "iteration 85\n",
      "training loss: 0.163153 , training accuracy: [0.95006371]\n",
      "testing loss 0.160902 test accuracy is [0.94861501]\n",
      "iteration 105\n",
      "training loss: 0.162992 , training accuracy: [0.95026284]\n",
      "testing loss 0.160393 test accuracy is [0.94901645]\n",
      "iteration 125\n",
      "training loss: 0.163176 , training accuracy: [0.95038229]\n",
      "testing loss 0.160244 test accuracy is [0.95002007]\n",
      "iteration 145\n",
      "training loss: 0.163045 , training accuracy: [0.95054156]\n",
      "testing loss 0.160212 test accuracy is [0.94961864]\n",
      "iteration 165\n",
      "training loss: 0.16296 , training accuracy: [0.95034248]\n",
      "testing loss 0.161021 test accuracy is [0.94961864]\n",
      "iteration 185\n",
      "training loss: 0.163034 , training accuracy: [0.95054156]\n",
      "testing loss 0.160794 test accuracy is [0.94901645]\n",
      "iteration 205\n",
      "training loss: 0.162693 , training accuracy: [0.95046192]\n",
      "testing loss 0.160021 test accuracy is [0.9492172]\n",
      "iteration 225\n",
      "training loss: 0.162807 , training accuracy: [0.95010352]\n",
      "testing loss 0.15994 test accuracy is [0.94861501]\n",
      "iteration 245\n",
      "training loss: 0.162998 , training accuracy: [0.95002389]\n",
      "testing loss 0.159935 test accuracy is [0.94861501]\n",
      "iteration 265\n",
      "training loss: 0.162459 , training accuracy: [0.95082033]\n",
      "testing loss 0.160557 test accuracy is [0.95042151]\n",
      "iteration 285\n",
      "training loss: 0.162593 , training accuracy: [0.95082033]\n",
      "testing loss 0.161456 test accuracy is [0.94941789]\n",
      "iteration 305\n",
      "training loss: 0.162913 , training accuracy: [0.95070088]\n",
      "testing loss 0.16236 test accuracy is [0.94881576]\n",
      "iteration 325\n",
      "training loss: 0.163594 , training accuracy: [0.95062119]\n",
      "testing loss 0.16356 test accuracy is [0.94941789]\n",
      "iteration 345\n",
      "training loss: 0.162381 , training accuracy: [0.95082033]\n",
      "testing loss 0.161486 test accuracy is [0.94841427]\n",
      "iteration 365\n",
      "training loss: 0.162505 , training accuracy: [0.95062119]\n",
      "testing loss 0.161166 test accuracy is [0.95002007]\n",
      "iteration 385\n",
      "training loss: 0.162492 , training accuracy: [0.95046192]\n",
      "testing loss 0.161049 test accuracy is [0.94961864]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 405\n",
      "training loss: 0.162405 , training accuracy: [0.95062119]\n",
      "testing loss 0.160217 test accuracy is [0.94941789]\n",
      "iteration 425\n",
      "training loss: 0.162589 , training accuracy: [0.95018315]\n",
      "testing loss 0.160301 test accuracy is [0.94981933]\n",
      "iteration 445\n",
      "training loss: 0.161912 , training accuracy: [0.95022303]\n",
      "testing loss 0.159783 test accuracy is [0.94981933]\n",
      "iteration 465\n",
      "training loss: 0.161611 , training accuracy: [0.95062119]\n",
      "testing loss 0.16016 test accuracy is [0.94941789]\n",
      "iteration 485\n",
      "training loss: 0.161889 , training accuracy: [0.95070088]\n",
      "testing loss 0.160867 test accuracy is [0.94901645]\n",
      "Epoch:  12\n",
      "iteration 5\n",
      "training loss: 0.161539 , training accuracy: [0.95058137]\n",
      "testing loss 0.160186 test accuracy is [0.94961864]\n",
      "iteration 25\n",
      "training loss: 0.161509 , training accuracy: [0.95062119]\n",
      "testing loss 0.160197 test accuracy is [0.95042151]\n",
      "iteration 45\n",
      "training loss: 0.161654 , training accuracy: [0.95066106]\n",
      "testing loss 0.160859 test accuracy is [0.94981933]\n",
      "iteration 65\n",
      "training loss: 0.16211 , training accuracy: [0.95042211]\n",
      "testing loss 0.160783 test accuracy is [0.94981933]\n",
      "iteration 85\n",
      "training loss: 0.161477 , training accuracy: [0.95089996]\n",
      "testing loss 0.160017 test accuracy is [0.95062226]\n",
      "iteration 105\n",
      "training loss: 0.16165 , training accuracy: [0.95086014]\n",
      "testing loss 0.160328 test accuracy is [0.94841427]\n",
      "iteration 125\n",
      "training loss: 0.161254 , training accuracy: [0.95101941]\n",
      "testing loss 0.160486 test accuracy is [0.94961864]\n",
      "iteration 145\n",
      "training loss: 0.161203 , training accuracy: [0.95117873]\n",
      "testing loss 0.160063 test accuracy is [0.95002007]\n",
      "iteration 165\n",
      "training loss: 0.161082 , training accuracy: [0.95113891]\n",
      "testing loss 0.159864 test accuracy is [0.95122439]\n",
      "iteration 185\n",
      "training loss: 0.16125 , training accuracy: [0.95105928]\n",
      "testing loss 0.160172 test accuracy is [0.95022082]\n",
      "iteration 205\n",
      "training loss: 0.161695 , training accuracy: [0.95093977]\n",
      "testing loss 0.160948 test accuracy is [0.94901645]\n",
      "iteration 225\n",
      "training loss: 0.160781 , training accuracy: [0.95141762]\n",
      "testing loss 0.160094 test accuracy is [0.9492172]\n",
      "iteration 245\n",
      "training loss: 0.162556 , training accuracy: [0.94994426]\n",
      "testing loss 0.163282 test accuracy is [0.94961864]\n",
      "iteration 265\n",
      "training loss: 0.162213 , training accuracy: [0.95133799]\n",
      "testing loss 0.161663 test accuracy is [0.94901645]\n",
      "iteration 285\n",
      "training loss: 0.160851 , training accuracy: [0.95121855]\n",
      "testing loss 0.160083 test accuracy is [0.94961864]\n",
      "iteration 305\n",
      "training loss: 0.160696 , training accuracy: [0.95101941]\n",
      "testing loss 0.160631 test accuracy is [0.94941789]\n",
      "iteration 325\n",
      "training loss: 0.160738 , training accuracy: [0.95141762]\n",
      "testing loss 0.160298 test accuracy is [0.94981933]\n",
      "iteration 345\n",
      "training loss: 0.160524 , training accuracy: [0.9516964]\n",
      "testing loss 0.160157 test accuracy is [0.95142514]\n",
      "iteration 365\n",
      "training loss: 0.160337 , training accuracy: [0.95137781]\n",
      "testing loss 0.159953 test accuracy is [0.95122439]\n",
      "iteration 385\n",
      "training loss: 0.161361 , training accuracy: [0.95070088]\n",
      "testing loss 0.160456 test accuracy is [0.94881576]\n",
      "iteration 405\n",
      "training loss: 0.16068 , training accuracy: [0.95074069]\n",
      "testing loss 0.159754 test accuracy is [0.95082295]\n",
      "iteration 425\n",
      "training loss: 0.160265 , training accuracy: [0.95185566]\n",
      "testing loss 0.15953 test accuracy is [0.95142514]\n",
      "iteration 445\n",
      "training loss: 0.160594 , training accuracy: [0.95141762]\n",
      "testing loss 0.16028 test accuracy is [0.95022082]\n",
      "iteration 465\n",
      "training loss: 0.160421 , training accuracy: [0.95125836]\n",
      "testing loss 0.158976 test accuracy is [0.95122439]\n",
      "iteration 485\n",
      "training loss: 0.160261 , training accuracy: [0.95145744]\n",
      "testing loss 0.158777 test accuracy is [0.9510237]\n",
      "Epoch:  13\n",
      "iteration 5\n",
      "training loss: 0.160025 , training accuracy: [0.95113891]\n",
      "testing loss 0.15849 test accuracy is [0.95042151]\n",
      "iteration 25\n",
      "training loss: 0.15979 , training accuracy: [0.95129818]\n",
      "testing loss 0.159043 test accuracy is [0.95022082]\n",
      "iteration 45\n",
      "training loss: 0.160787 , training accuracy: [0.95078051]\n",
      "testing loss 0.160781 test accuracy is [0.94941789]\n",
      "iteration 65\n",
      "training loss: 0.16117 , training accuracy: [0.95074069]\n",
      "testing loss 0.160997 test accuracy is [0.94981933]\n",
      "iteration 85\n",
      "training loss: 0.160294 , training accuracy: [0.95129818]\n",
      "testing loss 0.16039 test accuracy is [0.94981933]\n",
      "iteration 105\n",
      "training loss: 0.160142 , training accuracy: [0.95133799]\n",
      "testing loss 0.160169 test accuracy is [0.94941789]\n",
      "iteration 125\n",
      "training loss: 0.159984 , training accuracy: [0.95121855]\n",
      "testing loss 0.159807 test accuracy is [0.9492172]\n",
      "iteration 145\n",
      "training loss: 0.160538 , training accuracy: [0.95030266]\n",
      "testing loss 0.160174 test accuracy is [0.9492172]\n",
      "iteration 165\n",
      "training loss: 0.160072 , training accuracy: [0.95058137]\n",
      "testing loss 0.158785 test accuracy is [0.95022082]\n",
      "iteration 185\n",
      "training loss: 0.159697 , training accuracy: [0.95121855]\n",
      "testing loss 0.158595 test accuracy is [0.95062226]\n",
      "iteration 205\n",
      "training loss: 0.159484 , training accuracy: [0.95145744]\n",
      "testing loss 0.158114 test accuracy is [0.95082295]\n",
      "iteration 225\n",
      "training loss: 0.159627 , training accuracy: [0.95177603]\n",
      "testing loss 0.158624 test accuracy is [0.95142514]\n",
      "iteration 245\n",
      "training loss: 0.15959 , training accuracy: [0.95145744]\n",
      "testing loss 0.158501 test accuracy is [0.94961864]\n",
      "iteration 265\n",
      "training loss: 0.159661 , training accuracy: [0.95117873]\n",
      "testing loss 0.158784 test accuracy is [0.95062226]\n",
      "iteration 285\n",
      "training loss: 0.159188 , training accuracy: [0.95137781]\n",
      "testing loss 0.158301 test accuracy is [0.95002007]\n",
      "iteration 305\n",
      "training loss: 0.159606 , training accuracy: [0.95129818]\n",
      "testing loss 0.158809 test accuracy is [0.94941789]\n",
      "iteration 325\n",
      "training loss: 0.15886 , training accuracy: [0.95165658]\n",
      "testing loss 0.158263 test accuracy is [0.95002007]\n",
      "iteration 345\n",
      "training loss: 0.158856 , training accuracy: [0.95137781]\n",
      "testing loss 0.158447 test accuracy is [0.95042151]\n",
      "iteration 365\n",
      "training loss: 0.158799 , training accuracy: [0.95153713]\n",
      "testing loss 0.158161 test accuracy is [0.95002007]\n",
      "iteration 385\n",
      "training loss: 0.159233 , training accuracy: [0.95165658]\n",
      "testing loss 0.158306 test accuracy is [0.94961864]\n",
      "iteration 405\n",
      "training loss: 0.158967 , training accuracy: [0.95153713]\n",
      "testing loss 0.158393 test accuracy is [0.95002007]\n",
      "iteration 425\n",
      "training loss: 0.158953 , training accuracy: [0.95129818]\n",
      "testing loss 0.158543 test accuracy is [0.94981933]\n",
      "iteration 445\n",
      "training loss: 0.159262 , training accuracy: [0.9520548]\n",
      "testing loss 0.158952 test accuracy is [0.95042151]\n",
      "iteration 465\n",
      "training loss: 0.159247 , training accuracy: [0.95225388]\n",
      "testing loss 0.159218 test accuracy is [0.95142514]\n",
      "iteration 485\n",
      "training loss: 0.158654 , training accuracy: [0.9526521]\n",
      "testing loss 0.158238 test accuracy is [0.95122439]\n",
      "Epoch:  14\n",
      "iteration 5\n",
      "training loss: 0.158748 , training accuracy: [0.95129818]\n",
      "testing loss 0.15791 test accuracy is [0.95022082]\n",
      "iteration 25\n",
      "training loss: 0.160132 , training accuracy: [0.95125836]\n",
      "testing loss 0.158639 test accuracy is [0.94861501]\n",
      "iteration 45\n",
      "training loss: 0.159291 , training accuracy: [0.95145744]\n",
      "testing loss 0.157955 test accuracy is [0.95022082]\n",
      "iteration 65\n",
      "training loss: 0.158104 , training accuracy: [0.95197517]\n",
      "testing loss 0.157692 test accuracy is [0.95062226]\n",
      "iteration 85\n",
      "training loss: 0.15838 , training accuracy: [0.95181584]\n",
      "testing loss 0.158574 test accuracy is [0.95042151]\n",
      "iteration 105\n",
      "training loss: 0.159312 , training accuracy: [0.95125836]\n",
      "testing loss 0.158256 test accuracy is [0.95002007]\n",
      "iteration 125\n",
      "training loss: 0.158181 , training accuracy: [0.95181584]\n",
      "testing loss 0.157496 test accuracy is [0.95122439]\n",
      "iteration 145\n",
      "training loss: 0.158243 , training accuracy: [0.9516964]\n",
      "testing loss 0.1582 test accuracy is [0.95002007]\n",
      "iteration 165\n",
      "training loss: 0.160738 , training accuracy: [0.95066106]\n",
      "testing loss 0.161394 test accuracy is [0.9492172]\n",
      "iteration 185\n",
      "training loss: 0.157813 , training accuracy: [0.95197517]\n",
      "testing loss 0.157287 test accuracy is [0.95142514]\n",
      "iteration 205\n",
      "training loss: 0.157758 , training accuracy: [0.9516964]\n",
      "testing loss 0.157321 test accuracy is [0.9510237]\n",
      "iteration 225\n",
      "training loss: 0.157915 , training accuracy: [0.95145744]\n",
      "testing loss 0.157407 test accuracy is [0.95082295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 245\n",
      "training loss: 0.15779 , training accuracy: [0.95165658]\n",
      "testing loss 0.157342 test accuracy is [0.95042151]\n",
      "iteration 265\n",
      "training loss: 0.157706 , training accuracy: [0.95213443]\n",
      "testing loss 0.157368 test accuracy is [0.95122439]\n",
      "iteration 285\n",
      "training loss: 0.157483 , training accuracy: [0.95229375]\n",
      "testing loss 0.156898 test accuracy is [0.9510237]\n",
      "iteration 305\n",
      "training loss: 0.157852 , training accuracy: [0.95213443]\n",
      "testing loss 0.157305 test accuracy is [0.95082295]\n",
      "iteration 325\n",
      "training loss: 0.157441 , training accuracy: [0.95229375]\n",
      "testing loss 0.157033 test accuracy is [0.9510237]\n",
      "iteration 345\n",
      "training loss: 0.158056 , training accuracy: [0.95281142]\n",
      "testing loss 0.158104 test accuracy is [0.95082295]\n",
      "iteration 365\n",
      "training loss: 0.157932 , training accuracy: [0.9527716]\n",
      "testing loss 0.157327 test accuracy is [0.95122439]\n",
      "iteration 385\n",
      "training loss: 0.158992 , training accuracy: [0.95201498]\n",
      "testing loss 0.158794 test accuracy is [0.95082295]\n",
      "iteration 405\n",
      "training loss: 0.157358 , training accuracy: [0.95245302]\n",
      "testing loss 0.157028 test accuracy is [0.95142514]\n",
      "iteration 425\n",
      "training loss: 0.15836 , training accuracy: [0.95145744]\n",
      "testing loss 0.159768 test accuracy is [0.94981933]\n",
      "iteration 445\n",
      "training loss: 0.157332 , training accuracy: [0.95225388]\n",
      "testing loss 0.157859 test accuracy is [0.94981933]\n",
      "iteration 465\n",
      "training loss: 0.157063 , training accuracy: [0.95257246]\n",
      "testing loss 0.157158 test accuracy is [0.95082295]\n",
      "iteration 485\n",
      "training loss: 0.159116 , training accuracy: [0.95269191]\n",
      "testing loss 0.160072 test accuracy is [0.9492172]\n",
      "Epoch:  15\n",
      "iteration 5\n",
      "training loss: 0.156979 , training accuracy: [0.95249283]\n",
      "testing loss 0.157797 test accuracy is [0.95062226]\n",
      "iteration 25\n",
      "training loss: 0.157847 , training accuracy: [0.95269191]\n",
      "testing loss 0.159516 test accuracy is [0.95022082]\n",
      "iteration 45\n",
      "training loss: 0.15688 , training accuracy: [0.95273179]\n",
      "testing loss 0.157581 test accuracy is [0.95062226]\n",
      "iteration 65\n",
      "training loss: 0.156939 , training accuracy: [0.95225388]\n",
      "testing loss 0.157784 test accuracy is [0.95062226]\n",
      "iteration 85\n",
      "training loss: 0.157638 , training accuracy: [0.95249283]\n",
      "testing loss 0.159273 test accuracy is [0.9510237]\n",
      "iteration 105\n",
      "training loss: 0.156944 , training accuracy: [0.95225388]\n",
      "testing loss 0.157832 test accuracy is [0.95022082]\n",
      "iteration 125\n",
      "training loss: 0.15707 , training accuracy: [0.9526521]\n",
      "testing loss 0.158103 test accuracy is [0.95122439]\n",
      "iteration 145\n",
      "training loss: 0.156888 , training accuracy: [0.95237339]\n",
      "testing loss 0.157971 test accuracy is [0.95062226]\n",
      "iteration 165\n",
      "training loss: 0.15702 , training accuracy: [0.95245302]\n",
      "testing loss 0.158438 test accuracy is [0.95022082]\n",
      "iteration 185\n",
      "training loss: 0.156811 , training accuracy: [0.95229375]\n",
      "testing loss 0.157123 test accuracy is [0.95082295]\n",
      "iteration 205\n",
      "training loss: 0.15665 , training accuracy: [0.95237339]\n",
      "testing loss 0.157053 test accuracy is [0.95042151]\n",
      "iteration 225\n",
      "training loss: 0.156475 , training accuracy: [0.95297068]\n",
      "testing loss 0.157058 test accuracy is [0.95162582]\n",
      "iteration 245\n",
      "training loss: 0.156614 , training accuracy: [0.95229375]\n",
      "testing loss 0.157616 test accuracy is [0.9510237]\n",
      "iteration 265\n",
      "training loss: 0.156822 , training accuracy: [0.9527716]\n",
      "testing loss 0.157587 test accuracy is [0.95082295]\n",
      "iteration 285\n",
      "training loss: 0.156772 , training accuracy: [0.95237339]\n",
      "testing loss 0.157108 test accuracy is [0.9510237]\n",
      "iteration 305\n",
      "training loss: 0.15678 , training accuracy: [0.95217425]\n",
      "testing loss 0.156839 test accuracy is [0.9510237]\n",
      "iteration 325\n",
      "training loss: 0.156663 , training accuracy: [0.9524132]\n",
      "testing loss 0.156569 test accuracy is [0.95162582]\n",
      "iteration 345\n",
      "training loss: 0.156319 , training accuracy: [0.95273179]\n",
      "testing loss 0.157081 test accuracy is [0.95182657]\n",
      "iteration 365\n",
      "training loss: 0.156177 , training accuracy: [0.95297068]\n",
      "testing loss 0.156989 test accuracy is [0.9510237]\n",
      "iteration 385\n",
      "training loss: 0.156391 , training accuracy: [0.95273179]\n",
      "testing loss 0.157283 test accuracy is [0.95022082]\n",
      "iteration 405\n",
      "training loss: 0.156551 , training accuracy: [0.95237339]\n",
      "testing loss 0.157832 test accuracy is [0.9492172]\n",
      "iteration 425\n",
      "training loss: 0.15654 , training accuracy: [0.95293087]\n",
      "testing loss 0.157924 test accuracy is [0.95062226]\n",
      "iteration 445\n",
      "training loss: 0.156594 , training accuracy: [0.95233357]\n",
      "testing loss 0.157498 test accuracy is [0.9510237]\n",
      "iteration 465\n",
      "training loss: 0.15628 , training accuracy: [0.95261228]\n",
      "testing loss 0.15703 test accuracy is [0.95002007]\n",
      "iteration 485\n",
      "training loss: 0.15675 , training accuracy: [0.95257246]\n",
      "testing loss 0.157135 test accuracy is [0.95022082]\n",
      "Epoch:  16\n",
      "iteration 5\n",
      "training loss: 0.156154 , training accuracy: [0.95313001]\n",
      "testing loss 0.157045 test accuracy is [0.95082295]\n",
      "iteration 25\n",
      "training loss: 0.156296 , training accuracy: [0.95289105]\n",
      "testing loss 0.156164 test accuracy is [0.95142514]\n",
      "iteration 45\n",
      "training loss: 0.156357 , training accuracy: [0.95281142]\n",
      "testing loss 0.156119 test accuracy is [0.95082295]\n",
      "iteration 65\n",
      "training loss: 0.157037 , training accuracy: [0.95249283]\n",
      "testing loss 0.15653 test accuracy is [0.95022082]\n",
      "iteration 85\n",
      "training loss: 0.156227 , training accuracy: [0.95261228]\n",
      "testing loss 0.156305 test accuracy is [0.95002007]\n",
      "iteration 105\n",
      "training loss: 0.155937 , training accuracy: [0.95316982]\n",
      "testing loss 0.156576 test accuracy is [0.95202732]\n",
      "iteration 125\n",
      "training loss: 0.155884 , training accuracy: [0.95324945]\n",
      "testing loss 0.156585 test accuracy is [0.95082295]\n",
      "iteration 145\n",
      "training loss: 0.155763 , training accuracy: [0.95340872]\n",
      "testing loss 0.156986 test accuracy is [0.95082295]\n",
      "iteration 165\n",
      "training loss: 0.1554 , training accuracy: [0.95328927]\n",
      "testing loss 0.156732 test accuracy is [0.95122439]\n",
      "iteration 185\n",
      "training loss: 0.155776 , training accuracy: [0.9526521]\n",
      "testing loss 0.157646 test accuracy is [0.95002007]\n",
      "iteration 205\n",
      "training loss: 0.155755 , training accuracy: [0.9527716]\n",
      "testing loss 0.157457 test accuracy is [0.95062226]\n",
      "iteration 225\n",
      "training loss: 0.155507 , training accuracy: [0.9526521]\n",
      "testing loss 0.156857 test accuracy is [0.95122439]\n",
      "iteration 245\n",
      "training loss: 0.155403 , training accuracy: [0.95249283]\n",
      "testing loss 0.156422 test accuracy is [0.9510237]\n",
      "iteration 265\n",
      "training loss: 0.155628 , training accuracy: [0.9524132]\n",
      "testing loss 0.156268 test accuracy is [0.95162582]\n",
      "iteration 285\n",
      "training loss: 0.155121 , training accuracy: [0.95289105]\n",
      "testing loss 0.156488 test accuracy is [0.95162582]\n",
      "iteration 305\n",
      "training loss: 0.155229 , training accuracy: [0.95332909]\n",
      "testing loss 0.155994 test accuracy is [0.95082295]\n",
      "iteration 325\n",
      "training loss: 0.155673 , training accuracy: [0.95245302]\n",
      "testing loss 0.156233 test accuracy is [0.94961864]\n",
      "iteration 345\n",
      "training loss: 0.155037 , training accuracy: [0.95261228]\n",
      "testing loss 0.156332 test accuracy is [0.95142514]\n",
      "iteration 365\n",
      "training loss: 0.155476 , training accuracy: [0.95285124]\n",
      "testing loss 0.157433 test accuracy is [0.95062226]\n",
      "iteration 385\n",
      "training loss: 0.155315 , training accuracy: [0.95273179]\n",
      "testing loss 0.157156 test accuracy is [0.95202732]\n",
      "iteration 405\n",
      "training loss: 0.155215 , training accuracy: [0.95269191]\n",
      "testing loss 0.157268 test accuracy is [0.95182657]\n",
      "iteration 425\n",
      "training loss: 0.155022 , training accuracy: [0.9530105]\n",
      "testing loss 0.156462 test accuracy is [0.94961864]\n",
      "iteration 445\n",
      "training loss: 0.154857 , training accuracy: [0.95340872]\n",
      "testing loss 0.155512 test accuracy is [0.95122439]\n",
      "iteration 465\n",
      "training loss: 0.154689 , training accuracy: [0.95328927]\n",
      "testing loss 0.155983 test accuracy is [0.95042151]\n",
      "iteration 485\n",
      "training loss: 0.154783 , training accuracy: [0.95316982]\n",
      "testing loss 0.155894 test accuracy is [0.95082295]\n",
      "Epoch:  17\n",
      "iteration 5\n",
      "training loss: 0.154955 , training accuracy: [0.95316982]\n",
      "testing loss 0.15682 test accuracy is [0.95142514]\n",
      "iteration 25\n",
      "training loss: 0.155145 , training accuracy: [0.95313001]\n",
      "testing loss 0.157291 test accuracy is [0.95122439]\n",
      "iteration 45\n",
      "training loss: 0.154987 , training accuracy: [0.95289105]\n",
      "testing loss 0.156613 test accuracy is [0.94981933]\n",
      "iteration 65\n",
      "training loss: 0.155427 , training accuracy: [0.95233357]\n",
      "testing loss 0.156653 test accuracy is [0.94981933]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 85\n",
      "training loss: 0.154513 , training accuracy: [0.95376712]\n",
      "testing loss 0.156214 test accuracy is [0.95162582]\n",
      "iteration 105\n",
      "training loss: 0.154851 , training accuracy: [0.95388657]\n",
      "testing loss 0.156624 test accuracy is [0.95082295]\n",
      "iteration 125\n",
      "training loss: 0.154512 , training accuracy: [0.95332909]\n",
      "testing loss 0.156983 test accuracy is [0.95142514]\n",
      "iteration 145\n",
      "training loss: 0.154455 , training accuracy: [0.95332909]\n",
      "testing loss 0.156746 test accuracy is [0.95162582]\n",
      "iteration 165\n",
      "training loss: 0.154235 , training accuracy: [0.9533689]\n",
      "testing loss 0.155902 test accuracy is [0.95142514]\n",
      "iteration 185\n",
      "training loss: 0.154342 , training accuracy: [0.95332909]\n",
      "testing loss 0.156366 test accuracy is [0.9510237]\n",
      "iteration 205\n",
      "training loss: 0.15492 , training accuracy: [0.95281142]\n",
      "testing loss 0.15598 test accuracy is [0.95122439]\n",
      "iteration 225\n",
      "training loss: 0.154298 , training accuracy: [0.95376712]\n",
      "testing loss 0.155888 test accuracy is [0.95142514]\n",
      "iteration 245\n",
      "training loss: 0.154684 , training accuracy: [0.9530105]\n",
      "testing loss 0.155486 test accuracy is [0.95042151]\n",
      "iteration 265\n",
      "training loss: 0.154305 , training accuracy: [0.95324945]\n",
      "testing loss 0.155654 test accuracy is [0.95082295]\n",
      "iteration 285\n",
      "training loss: 0.15407 , training accuracy: [0.9537273]\n",
      "testing loss 0.155285 test accuracy is [0.95162582]\n",
      "iteration 305\n",
      "training loss: 0.154057 , training accuracy: [0.95352817]\n",
      "testing loss 0.155344 test accuracy is [0.95122439]\n",
      "iteration 325\n",
      "training loss: 0.154246 , training accuracy: [0.95340872]\n",
      "testing loss 0.155274 test accuracy is [0.95182657]\n",
      "iteration 345\n",
      "training loss: 0.15383 , training accuracy: [0.95392638]\n",
      "testing loss 0.155499 test accuracy is [0.95182657]\n",
      "iteration 365\n",
      "training loss: 0.153945 , training accuracy: [0.9533689]\n",
      "testing loss 0.155567 test accuracy is [0.95142514]\n",
      "iteration 385\n",
      "training loss: 0.155042 , training accuracy: [0.95344853]\n",
      "testing loss 0.156984 test accuracy is [0.95082295]\n",
      "iteration 405\n",
      "training loss: 0.154591 , training accuracy: [0.95376712]\n",
      "testing loss 0.156498 test accuracy is [0.95082295]\n",
      "iteration 425\n",
      "training loss: 0.154073 , training accuracy: [0.95340872]\n",
      "testing loss 0.156675 test accuracy is [0.95142514]\n",
      "iteration 445\n",
      "training loss: 0.15394 , training accuracy: [0.95380694]\n",
      "testing loss 0.156206 test accuracy is [0.95242876]\n",
      "iteration 465\n",
      "training loss: 0.154582 , training accuracy: [0.9537273]\n",
      "testing loss 0.157313 test accuracy is [0.95162582]\n",
      "iteration 485\n",
      "training loss: 0.153834 , training accuracy: [0.95340872]\n",
      "testing loss 0.155717 test accuracy is [0.9510237]\n",
      "Epoch:  18\n",
      "iteration 5\n",
      "training loss: 0.153729 , training accuracy: [0.95344853]\n",
      "testing loss 0.155598 test accuracy is [0.95042151]\n",
      "iteration 25\n",
      "training loss: 0.15359 , training accuracy: [0.95388657]\n",
      "testing loss 0.155499 test accuracy is [0.95162582]\n",
      "iteration 45\n",
      "training loss: 0.153603 , training accuracy: [0.95392638]\n",
      "testing loss 0.155964 test accuracy is [0.95202732]\n",
      "iteration 65\n",
      "training loss: 0.153445 , training accuracy: [0.95368749]\n",
      "testing loss 0.155916 test accuracy is [0.95182657]\n",
      "iteration 85\n",
      "training loss: 0.153461 , training accuracy: [0.95388657]\n",
      "testing loss 0.155552 test accuracy is [0.95122439]\n",
      "iteration 105\n",
      "training loss: 0.153365 , training accuracy: [0.95360786]\n",
      "testing loss 0.155646 test accuracy is [0.95062226]\n",
      "iteration 125\n",
      "training loss: 0.153645 , training accuracy: [0.95340872]\n",
      "testing loss 0.156122 test accuracy is [0.95022082]\n",
      "iteration 145\n",
      "training loss: 0.153914 , training accuracy: [0.95340872]\n",
      "testing loss 0.156743 test accuracy is [0.95142514]\n",
      "iteration 165\n",
      "training loss: 0.153373 , training accuracy: [0.95376712]\n",
      "testing loss 0.15615 test accuracy is [0.95082295]\n",
      "iteration 185\n",
      "training loss: 0.154368 , training accuracy: [0.95297068]\n",
      "testing loss 0.157473 test accuracy is [0.95022082]\n",
      "iteration 205\n",
      "training loss: 0.153991 , training accuracy: [0.95297068]\n",
      "testing loss 0.156999 test accuracy is [0.95082295]\n",
      "iteration 225\n",
      "training loss: 0.153618 , training accuracy: [0.95348835]\n",
      "testing loss 0.156205 test accuracy is [0.9510237]\n",
      "iteration 245\n",
      "training loss: 0.15346 , training accuracy: [0.95313001]\n",
      "testing loss 0.156098 test accuracy is [0.95062226]\n",
      "iteration 265\n",
      "training loss: 0.153273 , training accuracy: [0.95392638]\n",
      "testing loss 0.155255 test accuracy is [0.95142514]\n",
      "iteration 285\n",
      "training loss: 0.153593 , training accuracy: [0.9533689]\n",
      "testing loss 0.154949 test accuracy is [0.9510237]\n",
      "iteration 305\n",
      "training loss: 0.153304 , training accuracy: [0.9533689]\n",
      "testing loss 0.155658 test accuracy is [0.9510237]\n",
      "iteration 325\n",
      "training loss: 0.153802 , training accuracy: [0.95344853]\n",
      "testing loss 0.157038 test accuracy is [0.95062226]\n",
      "iteration 345\n",
      "training loss: 0.153087 , training accuracy: [0.95356804]\n",
      "testing loss 0.155053 test accuracy is [0.9510237]\n",
      "iteration 365\n",
      "training loss: 0.153283 , training accuracy: [0.95388657]\n",
      "testing loss 0.15573 test accuracy is [0.95142514]\n",
      "iteration 385\n",
      "training loss: 0.153789 , training accuracy: [0.9533689]\n",
      "testing loss 0.156581 test accuracy is [0.95142514]\n",
      "iteration 405\n",
      "training loss: 0.153466 , training accuracy: [0.95344853]\n",
      "testing loss 0.155957 test accuracy is [0.95182657]\n",
      "iteration 425\n",
      "training loss: 0.153342 , training accuracy: [0.95368749]\n",
      "testing loss 0.154853 test accuracy is [0.95022082]\n",
      "iteration 445\n",
      "training loss: 0.152742 , training accuracy: [0.95392638]\n",
      "testing loss 0.155166 test accuracy is [0.95182657]\n",
      "iteration 465\n",
      "training loss: 0.152859 , training accuracy: [0.95368749]\n",
      "testing loss 0.154736 test accuracy is [0.95122439]\n",
      "iteration 485\n",
      "training loss: 0.152833 , training accuracy: [0.95344853]\n",
      "testing loss 0.154507 test accuracy is [0.95082295]\n",
      "Epoch:  19\n",
      "iteration 5\n",
      "training loss: 0.152824 , training accuracy: [0.95448393]\n",
      "testing loss 0.155304 test accuracy is [0.95222801]\n",
      "iteration 25\n",
      "training loss: 0.153021 , training accuracy: [0.95428479]\n",
      "testing loss 0.155502 test accuracy is [0.95202732]\n",
      "iteration 45\n",
      "training loss: 0.153103 , training accuracy: [0.95448393]\n",
      "testing loss 0.154798 test accuracy is [0.95242876]\n",
      "iteration 65\n",
      "training loss: 0.152759 , training accuracy: [0.95416534]\n",
      "testing loss 0.154823 test accuracy is [0.95162582]\n",
      "iteration 85\n",
      "training loss: 0.153014 , training accuracy: [0.95416534]\n",
      "testing loss 0.155863 test accuracy is [0.95222801]\n",
      "iteration 105\n",
      "training loss: 0.152548 , training accuracy: [0.95412552]\n",
      "testing loss 0.15494 test accuracy is [0.95182657]\n",
      "iteration 125\n",
      "training loss: 0.152421 , training accuracy: [0.9543246]\n",
      "testing loss 0.155063 test accuracy is [0.95122439]\n",
      "iteration 145\n",
      "training loss: 0.152483 , training accuracy: [0.95412552]\n",
      "testing loss 0.154958 test accuracy is [0.95062226]\n",
      "iteration 165\n",
      "training loss: 0.152828 , training accuracy: [0.9530105]\n",
      "testing loss 0.155723 test accuracy is [0.9510237]\n",
      "iteration 185\n",
      "training loss: 0.152746 , training accuracy: [0.9537273]\n",
      "testing loss 0.155596 test accuracy is [0.95182657]\n",
      "iteration 205\n",
      "training loss: 0.152428 , training accuracy: [0.95436442]\n",
      "testing loss 0.155621 test accuracy is [0.95222801]\n",
      "iteration 225\n",
      "training loss: 0.152503 , training accuracy: [0.95380694]\n",
      "testing loss 0.15508 test accuracy is [0.95182657]\n",
      "iteration 245\n",
      "training loss: 0.153059 , training accuracy: [0.95316982]\n",
      "testing loss 0.155176 test accuracy is [0.9510237]\n",
      "iteration 265\n",
      "training loss: 0.152152 , training accuracy: [0.95444411]\n",
      "testing loss 0.154716 test accuracy is [0.95122439]\n",
      "iteration 285\n",
      "training loss: 0.152599 , training accuracy: [0.95424497]\n",
      "testing loss 0.155392 test accuracy is [0.95142514]\n",
      "iteration 305\n",
      "training loss: 0.152709 , training accuracy: [0.95360786]\n",
      "testing loss 0.154332 test accuracy is [0.9510237]\n",
      "iteration 325\n",
      "training loss: 0.152459 , training accuracy: [0.95424497]\n",
      "testing loss 0.154967 test accuracy is [0.95122439]\n",
      "iteration 345\n",
      "training loss: 0.152108 , training accuracy: [0.95416534]\n",
      "testing loss 0.154377 test accuracy is [0.95022082]\n",
      "iteration 365\n",
      "training loss: 0.152403 , training accuracy: [0.95340872]\n",
      "testing loss 0.15473 test accuracy is [0.95122439]\n",
      "iteration 385\n",
      "training loss: 0.152115 , training accuracy: [0.95396626]\n",
      "testing loss 0.154599 test accuracy is [0.95122439]\n",
      "iteration 405\n",
      "training loss: 0.152413 , training accuracy: [0.95420516]\n",
      "testing loss 0.155228 test accuracy is [0.95142514]\n",
      "iteration 425\n",
      "training loss: 0.152039 , training accuracy: [0.95420516]\n",
      "testing loss 0.155272 test accuracy is [0.95142514]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 445\n",
      "training loss: 0.152021 , training accuracy: [0.95380694]\n",
      "testing loss 0.155034 test accuracy is [0.95142514]\n",
      "iteration 465\n",
      "training loss: 0.151988 , training accuracy: [0.95392638]\n",
      "testing loss 0.154063 test accuracy is [0.95062226]\n",
      "iteration 485\n",
      "training loss: 0.151805 , training accuracy: [0.95424497]\n",
      "testing loss 0.154069 test accuracy is [0.95082295]\n",
      "Epoch:  20\n",
      "iteration 5\n",
      "training loss: 0.152159 , training accuracy: [0.95376712]\n",
      "testing loss 0.154645 test accuracy is [0.95122439]\n",
      "iteration 25\n",
      "training loss: 0.152378 , training accuracy: [0.95352817]\n",
      "testing loss 0.154762 test accuracy is [0.95142514]\n",
      "iteration 45\n",
      "training loss: 0.151782 , training accuracy: [0.95400608]\n",
      "testing loss 0.154452 test accuracy is [0.95242876]\n",
      "iteration 65\n",
      "training loss: 0.151708 , training accuracy: [0.95420516]\n",
      "testing loss 0.154884 test accuracy is [0.95222801]\n",
      "iteration 85\n",
      "training loss: 0.152997 , training accuracy: [0.95428479]\n",
      "testing loss 0.155906 test accuracy is [0.95122439]\n",
      "iteration 105\n",
      "training loss: 0.151617 , training accuracy: [0.95416534]\n",
      "testing loss 0.154451 test accuracy is [0.95222801]\n",
      "iteration 125\n",
      "training loss: 0.152094 , training accuracy: [0.95412552]\n",
      "testing loss 0.155186 test accuracy is [0.95182657]\n",
      "iteration 145\n",
      "training loss: 0.151636 , training accuracy: [0.9543246]\n",
      "testing loss 0.154629 test accuracy is [0.95202732]\n",
      "iteration 165\n",
      "training loss: 0.151907 , training accuracy: [0.9537273]\n",
      "testing loss 0.154578 test accuracy is [0.9510237]\n",
      "iteration 185\n",
      "training loss: 0.151513 , training accuracy: [0.95412552]\n",
      "testing loss 0.154593 test accuracy is [0.95142514]\n",
      "iteration 205\n",
      "training loss: 0.151417 , training accuracy: [0.95404589]\n",
      "testing loss 0.154014 test accuracy is [0.95142514]\n",
      "iteration 225\n",
      "training loss: 0.151452 , training accuracy: [0.9543246]\n",
      "testing loss 0.154541 test accuracy is [0.95242876]\n",
      "iteration 245\n",
      "training loss: 0.15145 , training accuracy: [0.9543246]\n",
      "testing loss 0.154846 test accuracy is [0.95182657]\n",
      "iteration 265\n",
      "training loss: 0.151544 , training accuracy: [0.95420516]\n",
      "testing loss 0.15482 test accuracy is [0.95182657]\n",
      "iteration 285\n",
      "training loss: 0.151502 , training accuracy: [0.95392638]\n",
      "testing loss 0.15402 test accuracy is [0.9510237]\n",
      "iteration 305\n",
      "training loss: 0.151211 , training accuracy: [0.95392638]\n",
      "testing loss 0.154394 test accuracy is [0.95202732]\n",
      "iteration 325\n",
      "training loss: 0.151872 , training accuracy: [0.95364767]\n",
      "testing loss 0.154344 test accuracy is [0.95142514]\n",
      "iteration 345\n",
      "training loss: 0.151467 , training accuracy: [0.95380694]\n",
      "testing loss 0.153807 test accuracy is [0.95122439]\n",
      "iteration 365\n",
      "training loss: 0.151526 , training accuracy: [0.9533689]\n",
      "testing loss 0.153974 test accuracy is [0.95042151]\n",
      "iteration 385\n",
      "training loss: 0.151178 , training accuracy: [0.95436442]\n",
      "testing loss 0.154107 test accuracy is [0.95182657]\n",
      "iteration 405\n",
      "training loss: 0.151119 , training accuracy: [0.95420516]\n",
      "testing loss 0.154189 test accuracy is [0.95162582]\n",
      "iteration 425\n",
      "training loss: 0.151657 , training accuracy: [0.95436442]\n",
      "testing loss 0.155147 test accuracy is [0.9510237]\n",
      "iteration 445\n",
      "training loss: 0.151151 , training accuracy: [0.95444411]\n",
      "testing loss 0.154387 test accuracy is [0.95142514]\n",
      "iteration 465\n",
      "training loss: 0.151076 , training accuracy: [0.95404589]\n",
      "testing loss 0.15469 test accuracy is [0.95162582]\n",
      "iteration 485\n",
      "training loss: 0.151231 , training accuracy: [0.95396626]\n",
      "testing loss 0.154289 test accuracy is [0.95122439]\n",
      "Epoch:  21\n",
      "iteration 5\n",
      "training loss: 0.15137 , training accuracy: [0.95376712]\n",
      "testing loss 0.153828 test accuracy is [0.9510237]\n",
      "iteration 25\n",
      "training loss: 0.150924 , training accuracy: [0.95444411]\n",
      "testing loss 0.153848 test accuracy is [0.95162582]\n",
      "iteration 45\n",
      "training loss: 0.150983 , training accuracy: [0.95468301]\n",
      "testing loss 0.154191 test accuracy is [0.95142514]\n",
      "iteration 65\n",
      "training loss: 0.15124 , training accuracy: [0.95448393]\n",
      "testing loss 0.154113 test accuracy is [0.95062226]\n",
      "iteration 85\n",
      "training loss: 0.15088 , training accuracy: [0.95547944]\n",
      "testing loss 0.154208 test accuracy is [0.95182657]\n",
      "iteration 105\n",
      "training loss: 0.151046 , training accuracy: [0.95460337]\n",
      "testing loss 0.154799 test accuracy is [0.95162582]\n",
      "iteration 125\n",
      "training loss: 0.151737 , training accuracy: [0.95464319]\n",
      "testing loss 0.155269 test accuracy is [0.95222801]\n",
      "iteration 145\n",
      "training loss: 0.151389 , training accuracy: [0.95448393]\n",
      "testing loss 0.154693 test accuracy is [0.9510237]\n",
      "iteration 165\n",
      "training loss: 0.151175 , training accuracy: [0.95484233]\n",
      "testing loss 0.154971 test accuracy is [0.9528302]\n",
      "iteration 185\n",
      "training loss: 0.150835 , training accuracy: [0.95488214]\n",
      "testing loss 0.154294 test accuracy is [0.95202732]\n",
      "iteration 205\n",
      "training loss: 0.151273 , training accuracy: [0.95472282]\n",
      "testing loss 0.155123 test accuracy is [0.95242876]\n",
      "iteration 225\n",
      "training loss: 0.152201 , training accuracy: [0.95408571]\n",
      "testing loss 0.154975 test accuracy is [0.9510237]\n",
      "iteration 245\n",
      "training loss: 0.150718 , training accuracy: [0.95440429]\n",
      "testing loss 0.154035 test accuracy is [0.95122439]\n",
      "iteration 265\n",
      "training loss: 0.150746 , training accuracy: [0.95400608]\n",
      "testing loss 0.1539 test accuracy is [0.95122439]\n",
      "iteration 285\n",
      "training loss: 0.150811 , training accuracy: [0.95416534]\n",
      "testing loss 0.154925 test accuracy is [0.95082295]\n",
      "iteration 305\n",
      "training loss: 0.150627 , training accuracy: [0.95448393]\n",
      "testing loss 0.154975 test accuracy is [0.9510237]\n",
      "iteration 325\n",
      "training loss: 0.150633 , training accuracy: [0.95456356]\n",
      "testing loss 0.154356 test accuracy is [0.95062226]\n",
      "iteration 345\n",
      "training loss: 0.150963 , training accuracy: [0.95416534]\n",
      "testing loss 0.15482 test accuracy is [0.95122439]\n",
      "iteration 365\n",
      "training loss: 0.151515 , training accuracy: [0.95400608]\n",
      "testing loss 0.155781 test accuracy is [0.95082295]\n",
      "iteration 385\n",
      "training loss: 0.150794 , training accuracy: [0.95352817]\n",
      "testing loss 0.154385 test accuracy is [0.95122439]\n",
      "iteration 405\n",
      "training loss: 0.150612 , training accuracy: [0.95448393]\n",
      "testing loss 0.153575 test accuracy is [0.95122439]\n",
      "iteration 425\n",
      "training loss: 0.150363 , training accuracy: [0.95464319]\n",
      "testing loss 0.153785 test accuracy is [0.95222801]\n",
      "iteration 445\n",
      "training loss: 0.150419 , training accuracy: [0.95424497]\n",
      "testing loss 0.154067 test accuracy is [0.95242876]\n",
      "iteration 465\n",
      "training loss: 0.150286 , training accuracy: [0.95476264]\n",
      "testing loss 0.153693 test accuracy is [0.95202732]\n",
      "iteration 485\n",
      "training loss: 0.150986 , training accuracy: [0.95412552]\n",
      "testing loss 0.153573 test accuracy is [0.95122439]\n",
      "Epoch:  22\n",
      "iteration 5\n",
      "training loss: 0.150397 , training accuracy: [0.95460337]\n",
      "testing loss 0.153728 test accuracy is [0.9528302]\n",
      "iteration 25\n",
      "training loss: 0.150563 , training accuracy: [0.95452374]\n",
      "testing loss 0.154436 test accuracy is [0.95262945]\n",
      "iteration 45\n",
      "training loss: 0.151376 , training accuracy: [0.95420516]\n",
      "testing loss 0.155984 test accuracy is [0.95303088]\n",
      "iteration 65\n",
      "training loss: 0.150791 , training accuracy: [0.95488214]\n",
      "testing loss 0.154918 test accuracy is [0.95242876]\n",
      "iteration 85\n",
      "training loss: 0.149998 , training accuracy: [0.95476264]\n",
      "testing loss 0.153455 test accuracy is [0.95142514]\n",
      "iteration 105\n",
      "training loss: 0.149959 , training accuracy: [0.95464319]\n",
      "testing loss 0.15377 test accuracy is [0.95162582]\n",
      "iteration 125\n",
      "training loss: 0.150078 , training accuracy: [0.95412552]\n",
      "testing loss 0.153834 test accuracy is [0.95122439]\n",
      "iteration 145\n",
      "training loss: 0.149887 , training accuracy: [0.95452374]\n",
      "testing loss 0.153709 test accuracy is [0.95162582]\n",
      "iteration 165\n",
      "training loss: 0.14988 , training accuracy: [0.95468301]\n",
      "testing loss 0.1543 test accuracy is [0.95062226]\n",
      "iteration 185\n",
      "training loss: 0.150413 , training accuracy: [0.95476264]\n",
      "testing loss 0.15488 test accuracy is [0.95222801]\n",
      "iteration 205\n",
      "training loss: 0.149939 , training accuracy: [0.95480251]\n",
      "testing loss 0.153837 test accuracy is [0.9510237]\n",
      "iteration 225\n",
      "training loss: 0.150014 , training accuracy: [0.95480251]\n",
      "testing loss 0.154209 test accuracy is [0.9510237]\n",
      "iteration 245\n",
      "training loss: 0.149994 , training accuracy: [0.95492196]\n",
      "testing loss 0.153913 test accuracy is [0.95162582]\n",
      "iteration 265\n",
      "training loss: 0.150089 , training accuracy: [0.95452374]\n",
      "testing loss 0.153454 test accuracy is [0.95082295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 285\n",
      "training loss: 0.150475 , training accuracy: [0.95444411]\n",
      "testing loss 0.153713 test accuracy is [0.95022082]\n",
      "iteration 305\n",
      "training loss: 0.15001 , training accuracy: [0.95472282]\n",
      "testing loss 0.154078 test accuracy is [0.95182657]\n",
      "iteration 325\n",
      "training loss: 0.149892 , training accuracy: [0.95456356]\n",
      "testing loss 0.153828 test accuracy is [0.95202732]\n",
      "iteration 345\n",
      "training loss: 0.149978 , training accuracy: [0.95424497]\n",
      "testing loss 0.154129 test accuracy is [0.95262945]\n",
      "iteration 365\n",
      "training loss: 0.15153 , training accuracy: [0.95404589]\n",
      "testing loss 0.155687 test accuracy is [0.95122439]\n",
      "iteration 385\n",
      "training loss: 0.149643 , training accuracy: [0.95460337]\n",
      "testing loss 0.153768 test accuracy is [0.95262945]\n",
      "iteration 405\n",
      "training loss: 0.149719 , training accuracy: [0.95424497]\n",
      "testing loss 0.153615 test accuracy is [0.95202732]\n",
      "iteration 425\n",
      "training loss: 0.149929 , training accuracy: [0.95464319]\n",
      "testing loss 0.154935 test accuracy is [0.95182657]\n",
      "iteration 445\n",
      "training loss: 0.149442 , training accuracy: [0.95512104]\n",
      "testing loss 0.15329 test accuracy is [0.9510237]\n",
      "iteration 465\n",
      "training loss: 0.149373 , training accuracy: [0.95508122]\n",
      "testing loss 0.153184 test accuracy is [0.9510237]\n",
      "iteration 485\n",
      "training loss: 0.149994 , training accuracy: [0.95452374]\n",
      "testing loss 0.153344 test accuracy is [0.95162582]\n",
      "Epoch:  23\n",
      "iteration 5\n",
      "training loss: 0.14965 , training accuracy: [0.95492196]\n",
      "testing loss 0.154002 test accuracy is [0.95062226]\n",
      "iteration 25\n",
      "training loss: 0.149443 , training accuracy: [0.95464319]\n",
      "testing loss 0.153384 test accuracy is [0.95162582]\n",
      "iteration 45\n",
      "training loss: 0.150213 , training accuracy: [0.95424497]\n",
      "testing loss 0.154065 test accuracy is [0.95222801]\n",
      "iteration 65\n",
      "training loss: 0.149508 , training accuracy: [0.95492196]\n",
      "testing loss 0.153549 test accuracy is [0.9510237]\n",
      "iteration 85\n",
      "training loss: 0.150368 , training accuracy: [0.95500159]\n",
      "testing loss 0.154818 test accuracy is [0.95182657]\n",
      "iteration 105\n",
      "training loss: 0.149949 , training accuracy: [0.95464319]\n",
      "testing loss 0.153403 test accuracy is [0.95182657]\n",
      "iteration 125\n",
      "training loss: 0.14931 , training accuracy: [0.95512104]\n",
      "testing loss 0.152859 test accuracy is [0.95262945]\n",
      "iteration 145\n",
      "training loss: 0.150003 , training accuracy: [0.95476264]\n",
      "testing loss 0.154422 test accuracy is [0.95262945]\n",
      "iteration 165\n",
      "training loss: 0.150507 , training accuracy: [0.95496178]\n",
      "testing loss 0.154229 test accuracy is [0.95042151]\n",
      "iteration 185\n",
      "training loss: 0.151252 , training accuracy: [0.95416534]\n",
      "testing loss 0.156164 test accuracy is [0.95142514]\n",
      "iteration 205\n",
      "training loss: 0.15008 , training accuracy: [0.95532018]\n",
      "testing loss 0.154832 test accuracy is [0.95222801]\n",
      "iteration 225\n",
      "training loss: 0.150515 , training accuracy: [0.95408571]\n",
      "testing loss 0.153901 test accuracy is [0.94961864]\n",
      "iteration 245\n",
      "training loss: 0.149576 , training accuracy: [0.95484233]\n",
      "testing loss 0.153489 test accuracy is [0.95222801]\n",
      "iteration 265\n",
      "training loss: 0.149151 , training accuracy: [0.95492196]\n",
      "testing loss 0.15316 test accuracy is [0.95162582]\n",
      "iteration 285\n",
      "training loss: 0.149091 , training accuracy: [0.95452374]\n",
      "testing loss 0.153757 test accuracy is [0.95142514]\n",
      "iteration 305\n",
      "training loss: 0.149348 , training accuracy: [0.95504141]\n",
      "testing loss 0.154069 test accuracy is [0.9510237]\n",
      "iteration 325\n",
      "training loss: 0.148992 , training accuracy: [0.95520067]\n",
      "testing loss 0.153711 test accuracy is [0.95122439]\n",
      "iteration 345\n",
      "training loss: 0.148805 , training accuracy: [0.95504141]\n",
      "testing loss 0.153185 test accuracy is [0.95182657]\n",
      "iteration 365\n",
      "training loss: 0.149482 , training accuracy: [0.95352817]\n",
      "testing loss 0.153655 test accuracy is [0.9510237]\n",
      "iteration 385\n",
      "training loss: 0.148902 , training accuracy: [0.95484233]\n",
      "testing loss 0.153623 test accuracy is [0.95142514]\n",
      "iteration 405\n",
      "training loss: 0.14886 , training accuracy: [0.95468301]\n",
      "testing loss 0.153077 test accuracy is [0.95162582]\n",
      "iteration 425\n",
      "training loss: 0.148771 , training accuracy: [0.95504141]\n",
      "testing loss 0.153223 test accuracy is [0.95142514]\n",
      "iteration 445\n",
      "training loss: 0.149511 , training accuracy: [0.95444411]\n",
      "testing loss 0.153166 test accuracy is [0.95082295]\n",
      "iteration 465\n",
      "training loss: 0.148674 , training accuracy: [0.95536]\n",
      "testing loss 0.153207 test accuracy is [0.95202732]\n",
      "iteration 485\n",
      "training loss: 0.149101 , training accuracy: [0.95500159]\n",
      "testing loss 0.153559 test accuracy is [0.95122439]\n",
      "Epoch:  24\n",
      "iteration 5\n",
      "training loss: 0.148649 , training accuracy: [0.95492196]\n",
      "testing loss 0.153008 test accuracy is [0.95182657]\n",
      "iteration 25\n",
      "training loss: 0.148935 , training accuracy: [0.95476264]\n",
      "testing loss 0.152943 test accuracy is [0.9510237]\n",
      "iteration 45\n",
      "training loss: 0.148789 , training accuracy: [0.95496178]\n",
      "testing loss 0.153661 test accuracy is [0.95122439]\n",
      "iteration 65\n",
      "training loss: 0.148938 , training accuracy: [0.95448393]\n",
      "testing loss 0.152856 test accuracy is [0.95202732]\n",
      "iteration 85\n",
      "training loss: 0.148893 , training accuracy: [0.95472282]\n",
      "testing loss 0.153484 test accuracy is [0.95082295]\n",
      "iteration 105\n",
      "training loss: 0.148819 , training accuracy: [0.95500159]\n",
      "testing loss 0.152666 test accuracy is [0.95202732]\n",
      "iteration 125\n",
      "training loss: 0.149383 , training accuracy: [0.95456356]\n",
      "testing loss 0.152849 test accuracy is [0.95242876]\n",
      "iteration 145\n",
      "training loss: 0.148991 , training accuracy: [0.95496178]\n",
      "testing loss 0.152661 test accuracy is [0.9528302]\n",
      "iteration 165\n",
      "training loss: 0.148527 , training accuracy: [0.95492196]\n",
      "testing loss 0.152932 test accuracy is [0.95182657]\n",
      "iteration 185\n",
      "training loss: 0.149088 , training accuracy: [0.9557184]\n",
      "testing loss 0.154303 test accuracy is [0.9510237]\n",
      "iteration 205\n",
      "training loss: 0.148664 , training accuracy: [0.95532018]\n",
      "testing loss 0.153857 test accuracy is [0.95222801]\n",
      "iteration 225\n",
      "training loss: 0.148582 , training accuracy: [0.95488214]\n",
      "testing loss 0.15341 test accuracy is [0.95082295]\n",
      "iteration 245\n",
      "training loss: 0.149921 , training accuracy: [0.95492196]\n",
      "testing loss 0.154613 test accuracy is [0.95122439]\n",
      "iteration 265\n",
      "training loss: 0.148337 , training accuracy: [0.95532018]\n",
      "testing loss 0.152906 test accuracy is [0.95162582]\n",
      "iteration 285\n",
      "training loss: 0.148394 , training accuracy: [0.95528036]\n",
      "testing loss 0.152904 test accuracy is [0.9528302]\n",
      "iteration 305\n",
      "training loss: 0.148892 , training accuracy: [0.95528036]\n",
      "testing loss 0.154423 test accuracy is [0.95182657]\n",
      "iteration 325\n",
      "training loss: 0.148698 , training accuracy: [0.95472282]\n",
      "testing loss 0.152883 test accuracy is [0.95162582]\n",
      "iteration 345\n",
      "training loss: 0.148997 , training accuracy: [0.95512104]\n",
      "testing loss 0.153155 test accuracy is [0.95122439]\n",
      "iteration 365\n",
      "training loss: 0.148575 , training accuracy: [0.95539981]\n",
      "testing loss 0.153531 test accuracy is [0.95242876]\n",
      "iteration 385\n",
      "training loss: 0.148473 , training accuracy: [0.95555907]\n",
      "testing loss 0.153175 test accuracy is [0.95222801]\n",
      "iteration 405\n",
      "training loss: 0.149277 , training accuracy: [0.95428479]\n",
      "testing loss 0.153818 test accuracy is [0.95162582]\n",
      "iteration 425\n",
      "training loss: 0.148316 , training accuracy: [0.95516086]\n",
      "testing loss 0.152747 test accuracy is [0.95262945]\n",
      "iteration 445\n",
      "training loss: 0.148148 , training accuracy: [0.95563877]\n",
      "testing loss 0.152303 test accuracy is [0.95242876]\n",
      "iteration 465\n",
      "training loss: 0.148517 , training accuracy: [0.95504141]\n",
      "testing loss 0.152233 test accuracy is [0.9510237]\n",
      "iteration 485\n",
      "training loss: 0.148027 , training accuracy: [0.95504141]\n",
      "testing loss 0.152603 test accuracy is [0.95162582]\n",
      "Epoch:  25\n",
      "iteration 5\n",
      "training loss: 0.148253 , training accuracy: [0.95508122]\n",
      "testing loss 0.152212 test accuracy is [0.95122439]\n",
      "iteration 25\n",
      "training loss: 0.148225 , training accuracy: [0.95512104]\n",
      "testing loss 0.15219 test accuracy is [0.95062226]\n",
      "iteration 45\n",
      "training loss: 0.148109 , training accuracy: [0.95543963]\n",
      "testing loss 0.152502 test accuracy is [0.95242876]\n",
      "iteration 65\n",
      "training loss: 0.14786 , training accuracy: [0.95559889]\n",
      "testing loss 0.152499 test accuracy is [0.95242876]\n",
      "iteration 85\n",
      "training loss: 0.147896 , training accuracy: [0.95528036]\n",
      "testing loss 0.15207 test accuracy is [0.95182657]\n",
      "iteration 105\n",
      "training loss: 0.148333 , training accuracy: [0.95539981]\n",
      "testing loss 0.152097 test accuracy is [0.95122439]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 125\n",
      "training loss: 0.148009 , training accuracy: [0.95587766]\n",
      "testing loss 0.152213 test accuracy is [0.95182657]\n",
      "iteration 145\n",
      "training loss: 0.148068 , training accuracy: [0.95555907]\n",
      "testing loss 0.153276 test accuracy is [0.95242876]\n",
      "iteration 165\n",
      "training loss: 0.148754 , training accuracy: [0.95547944]\n",
      "testing loss 0.154174 test accuracy is [0.95222801]\n",
      "iteration 185\n",
      "training loss: 0.148565 , training accuracy: [0.95492196]\n",
      "testing loss 0.154279 test accuracy is [0.95162582]\n",
      "iteration 205\n",
      "training loss: 0.147819 , training accuracy: [0.95512104]\n",
      "testing loss 0.152843 test accuracy is [0.95162582]\n",
      "iteration 225\n",
      "training loss: 0.14795 , training accuracy: [0.95532018]\n",
      "testing loss 0.152386 test accuracy is [0.95162582]\n",
      "iteration 245\n",
      "training loss: 0.147994 , training accuracy: [0.95500159]\n",
      "testing loss 0.15282 test accuracy is [0.95082295]\n",
      "iteration 265\n",
      "training loss: 0.148087 , training accuracy: [0.9557184]\n",
      "testing loss 0.153399 test accuracy is [0.95142514]\n",
      "iteration 285\n",
      "training loss: 0.147785 , training accuracy: [0.95587766]\n",
      "testing loss 0.15228 test accuracy is [0.95222801]\n",
      "iteration 305\n",
      "training loss: 0.148708 , training accuracy: [0.95512104]\n",
      "testing loss 0.152576 test accuracy is [0.95202732]\n",
      "iteration 325\n",
      "training loss: 0.147646 , training accuracy: [0.95563877]\n",
      "testing loss 0.151965 test accuracy is [0.95242876]\n",
      "iteration 345\n",
      "training loss: 0.148103 , training accuracy: [0.95536]\n",
      "testing loss 0.153023 test accuracy is [0.95162582]\n",
      "iteration 365\n",
      "training loss: 0.147744 , training accuracy: [0.95528036]\n",
      "testing loss 0.153259 test accuracy is [0.9510237]\n",
      "iteration 385\n",
      "training loss: 0.147677 , training accuracy: [0.95551926]\n",
      "testing loss 0.152724 test accuracy is [0.95122439]\n",
      "iteration 405\n",
      "training loss: 0.147466 , training accuracy: [0.95500159]\n",
      "testing loss 0.152664 test accuracy is [0.95122439]\n",
      "iteration 425\n",
      "training loss: 0.147839 , training accuracy: [0.95504141]\n",
      "testing loss 0.15258 test accuracy is [0.95142514]\n",
      "iteration 445\n",
      "training loss: 0.147348 , training accuracy: [0.95579803]\n",
      "testing loss 0.152568 test accuracy is [0.95122439]\n",
      "iteration 465\n",
      "training loss: 0.147413 , training accuracy: [0.95539981]\n",
      "testing loss 0.15303 test accuracy is [0.95162582]\n",
      "iteration 485\n",
      "training loss: 0.147779 , training accuracy: [0.95543963]\n",
      "testing loss 0.153442 test accuracy is [0.95122439]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for s in np.arange(learning_epoch):   \n",
    "    \n",
    "    # compute the initial training, testing loss and accuracy\n",
    "    if s == 0:\n",
    "        trl, tel = sess.run([train_loss, test_loss])\n",
    "        tr_acc = sess.run([acc], feed_dict={x: train_data, y: train_label})\n",
    "        te_acc = sess.run([acc], feed_dict={x: test_data, y: test_label})\n",
    "        print('iteration', s)\n",
    "        print('training loss:', trl, ', training accuracy:', tr_acc)\n",
    "        print('testing loss', tel, 'test accuracy is', te_acc)\n",
    "        \n",
    "        track_train_loss = np.hstack((track_train_loss, trl))\n",
    "        track_test_loss = np.hstack((track_test_loss, tel))\n",
    "    \n",
    "    # decay the learning rate every epoch\n",
    "    print('Epoch: ', s + 1)\n",
    "    learning_rate *= 0.8\n",
    "    \n",
    "    re_order = np.random.permutation(hm_train_data)\n",
    "    for i in np.arange(int(np.floor(hm_train_data / batch_size))):\n",
    "            batch_x = train_data[re_order[i * batch_size:(i + 1) * batch_size], :]\n",
    "            batch_y = train_label[re_order[i * batch_size:(i + 1) * batch_size], :]\n",
    "            \n",
    "            # stochastic gradient descent method\n",
    "            w = sess.run([weights], feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "            if i % eval_interval == 5:\n",
    "                trl, tel = sess.run([train_loss, test_loss])\n",
    "                tr_acc = sess.run([acc], feed_dict={x: train_data, y: train_label})\n",
    "                te_acc = sess.run([acc], feed_dict={x: test_data, y: test_label})\n",
    "                print('iteration', i)\n",
    "                print('training loss:', trl, ', training accuracy:', tr_acc)\n",
    "                print('testing loss', tel, 'test accuracy is', te_acc)\n",
    "        \n",
    "                track_train_loss = np.hstack((track_train_loss, trl))\n",
    "                track_test_loss = np.hstack((track_test_loss, tel))\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the weights as an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHQAAUgIGdAAASAn/H+zRaBK87xOZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1209bd240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "plt.subplot(231)\n",
    "img = w[0][0:28*28, 0].reshape(28,28)\n",
    "plt.imshow(img)\n",
    "plt.title('Digit 1')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(232)\n",
    "img = w[0][0:28*28, 1].reshape(28,28)\n",
    "plt.imshow(img)\n",
    "plt.title('Digit 2')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(233)\n",
    "img = w[0][0:28*28, 2].reshape(28,28)\n",
    "plt.imshow(img)\n",
    "plt.title('Digit 3')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(234)\n",
    "img = w[0][0:28*28, 3].reshape(28,28)\n",
    "plt.imshow(img)\n",
    "plt.title('Digit 4')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(235)\n",
    "img = w[0][0:28*28, 4].reshape(28,28)\n",
    "plt.imshow(img)\n",
    "plt.title('Digit 5')\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the final classification accuracy for each digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit 1 testing accuracy: 0.9859022556390977\n",
      "Digit 2 testing accuracy: 0.9535353535353536\n",
      "Digit 3 testing accuracy: 0.9310679611650485\n",
      "Digit 4 testing accuracy: 0.9735503560528993\n",
      "Digit 5 testing accuracy: 0.9092896174863387\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(5):\n",
    "    rows, _ = np.nonzero(test_label[:, i][:, None])\n",
    "    exp_terms = np.exp(np.dot(test_data[rows, :], w[0]))\n",
    "    prob = exp_terms / np.sum(exp_terms, axis=1, keepdims=True)\n",
    "    max_ind = np.argmax(prob, axis=1)\n",
    "    one_hot_max = np.zeros([np.size(rows), hm_classes])\n",
    "    for j in np.arange(np.size(rows)):\n",
    "        one_hot_max[j, int(max_ind[j])] = 1\n",
    "        \n",
    "    error_rate = np.count_nonzero(one_hot_max - test_label[rows, :]) / (2 * np.size(rows))\n",
    "    print('Digit', i + 1, 'testing accuracy:', 1 - error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Plot the learning curves w.r.t. the training and testing loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4NkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x122d1f898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xl = np.arange(np.size(track_train_loss))\n",
    "plt.plot(xl, track_train_loss, label='training loss')\n",
    "plt.plot(xl, track_test_loss, label='testing loss')\n",
    "plt.title('Loss vs. iteration')\n",
    "\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss function')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
