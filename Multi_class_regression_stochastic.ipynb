{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import my_func as mf\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "The training data and testing data have been flattened into m by n matrices, where m is the number of samples, and n number of features. Note that the dummy 1 has been added to the last column of train_data and test_data, so that the bias can be combined with the weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = np.loadtxt('my_train_data.txt', np.float32)\n",
    "test_data = np.loadtxt('my_test_data.txt', np.float32)\n",
    "train_label = np.loadtxt('my_train_label.txt', np.float32)\n",
    "test_label = np.loadtxt('my_test_label.txt', np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hm_train_data = 25112\n",
    "hm_test_data = 4982\n",
    "hm_classes = 5\n",
    "train_data /= 255\n",
    "test_data /= 255\n",
    "train_data = np.hstack((train_data, np.ones((hm_train_data, 1), dtype=np.float32)))\n",
    "test_data = np.hstack((test_data, np.ones((hm_test_data, 1), dtype=np.float32)))\n",
    "feature_dim = int(28 * 28) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup hyperparams for training the regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.0008\n",
    "learning_epoch = 20\n",
    "eval_interval = 20\n",
    "batch_size = 50\n",
    "reg_lambda = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=(None, feature_dim))\n",
    "y = tf.placeholder(tf.float32, shape=(None, hm_classes))\n",
    "weights = tf.Variable(tf.truncated_normal([feature_dim, hm_classes], 0.1, 0.01) / 10)\n",
    "w = np.zeros([feature_dim, hm_classes], dtype=np.float32)\n",
    "track_train_loss = np.array([])\n",
    "track_test_loss = np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Define the computational operation for the multi-class logistic regressor\n",
    "#### Compute the mean squared loss (Negative Conditional Log-likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = mf.compute_loss(weights, train_data, train_label, reg_lambda) / hm_train_data\n",
    "test_loss = mf.compute_loss(weights, test_data, test_label, reg_lambda) / hm_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the training and testing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soft_prob, _ = mf.compute_softmax(weights, x)\n",
    "correct = tf.equal(tf.argmax(soft_prob, 1), tf.argmax(y, 1))\n",
    "acc = tf.reduce_mean(tf.cast(correct, 'float'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the gradient and update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grads = mf.compute_gradients(weights, x, y, reg_lambda)\n",
    "weights = tf.assign_sub(weights, learning_rate * grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Initilize a session, and starting training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "training loss: 1.60932 , training accuracy: [0.20097961]\n",
      "testing loss 1.60929 test accuracy is [0.21336812]\n",
      "Epoch:  1\n",
      "iteration 5\n",
      "training loss: 1.2883 , training accuracy: [0.76756132]\n",
      "testing loss 1.2748 test accuracy is [0.78181452]\n",
      "iteration 25\n",
      "training loss: 0.773426 , training accuracy: [0.84644789]\n",
      "testing loss 0.746706 test accuracy is [0.86872745]\n",
      "iteration 45\n",
      "training loss: 0.591793 , training accuracy: [0.87229216]\n",
      "testing loss 0.559804 test accuracy is [0.89160979]\n",
      "iteration 65\n",
      "training loss: 0.501564 , training accuracy: [0.89072955]\n",
      "testing loss 0.467507 test accuracy is [0.91208351]\n",
      "iteration 85\n",
      "training loss: 0.446022 , training accuracy: [0.9031539]\n",
      "testing loss 0.411507 test accuracy is [0.92211962]\n",
      "iteration 105\n",
      "training loss: 0.409042 , training accuracy: [0.90554315]\n",
      "testing loss 0.375782 test accuracy is [0.92372543]\n",
      "iteration 125\n",
      "training loss: 0.383343 , training accuracy: [0.90996337]\n",
      "testing loss 0.350141 test accuracy is [0.92352468]\n",
      "iteration 145\n",
      "training loss: 0.361019 , training accuracy: [0.91091907]\n",
      "testing loss 0.3297 test accuracy is [0.9245283]\n",
      "iteration 165\n",
      "training loss: 0.343997 , training accuracy: [0.9170118]\n",
      "testing loss 0.312769 test accuracy is [0.92834204]\n",
      "iteration 185\n",
      "training loss: 0.329836 , training accuracy: [0.91677284]\n",
      "testing loss 0.299631 test accuracy is [0.92854273]\n",
      "iteration 205\n",
      "training loss: 0.31975 , training accuracy: [0.91828609]\n",
      "testing loss 0.289707 test accuracy is [0.93075073]\n",
      "iteration 225\n",
      "training loss: 0.310242 , training accuracy: [0.91844535]\n",
      "testing loss 0.281552 test accuracy is [0.93135285]\n",
      "iteration 245\n",
      "training loss: 0.300667 , training accuracy: [0.92111343]\n",
      "testing loss 0.273283 test accuracy is [0.93315935]\n",
      "iteration 265\n",
      "training loss: 0.292764 , training accuracy: [0.92406023]\n",
      "testing loss 0.266539 test accuracy is [0.93396229]\n",
      "iteration 285\n",
      "training loss: 0.286653 , training accuracy: [0.92358238]\n",
      "testing loss 0.260356 test accuracy is [0.93436372]\n",
      "iteration 305\n",
      "training loss: 0.281342 , training accuracy: [0.92425931]\n",
      "testing loss 0.255292 test accuracy is [0.93416297]\n",
      "iteration 325\n",
      "training loss: 0.275048 , training accuracy: [0.9252947]\n",
      "testing loss 0.248876 test accuracy is [0.93637091]\n",
      "iteration 345\n",
      "training loss: 0.270569 , training accuracy: [0.92597163]\n",
      "testing loss 0.245343 test accuracy is [0.93416297]\n",
      "iteration 365\n",
      "training loss: 0.265501 , training accuracy: [0.92644948]\n",
      "testing loss 0.240348 test accuracy is [0.93596947]\n",
      "iteration 385\n",
      "training loss: 0.261785 , training accuracy: [0.92836094]\n",
      "testing loss 0.237387 test accuracy is [0.93777597]\n",
      "iteration 405\n",
      "training loss: 0.258937 , training accuracy: [0.92915738]\n",
      "testing loss 0.234735 test accuracy is [0.93837816]\n",
      "iteration 425\n",
      "training loss: 0.255482 , training accuracy: [0.92748487]\n",
      "testing loss 0.231738 test accuracy is [0.93556803]\n",
      "iteration 445\n",
      "training loss: 0.252581 , training accuracy: [0.92800254]\n",
      "testing loss 0.229566 test accuracy is [0.93677241]\n",
      "iteration 465\n",
      "training loss: 0.249848 , training accuracy: [0.92915738]\n",
      "testing loss 0.226266 test accuracy is [0.93717384]\n",
      "iteration 485\n",
      "training loss: 0.246717 , training accuracy: [0.93138736]\n",
      "testing loss 0.223687 test accuracy is [0.94058609]\n",
      "Epoch:  2\n",
      "iteration 5\n",
      "training loss: 0.243635 , training accuracy: [0.93079007]\n",
      "testing loss 0.222148 test accuracy is [0.93938178]\n",
      "iteration 25\n",
      "training loss: 0.240923 , training accuracy: [0.93170595]\n",
      "testing loss 0.219341 test accuracy is [0.93998396]\n",
      "iteration 45\n",
      "training loss: 0.239117 , training accuracy: [0.93222362]\n",
      "testing loss 0.218562 test accuracy is [0.94078684]\n",
      "iteration 65\n",
      "training loss: 0.236993 , training accuracy: [0.93222362]\n",
      "testing loss 0.21648 test accuracy is [0.93998396]\n",
      "iteration 85\n",
      "training loss: 0.234999 , training accuracy: [0.93325901]\n",
      "testing loss 0.214571 test accuracy is [0.93938178]\n",
      "iteration 105\n",
      "training loss: 0.233891 , training accuracy: [0.93282098]\n",
      "testing loss 0.213943 test accuracy is [0.9403854]\n",
      "iteration 125\n",
      "training loss: 0.232343 , training accuracy: [0.93321919]\n",
      "testing loss 0.213023 test accuracy is [0.94158971]\n",
      "iteration 145\n",
      "training loss: 0.233157 , training accuracy: [0.93186522]\n",
      "testing loss 0.215024 test accuracy is [0.93697309]\n",
      "iteration 165\n",
      "training loss: 0.228043 , training accuracy: [0.93425453]\n",
      "testing loss 0.208233 test accuracy is [0.93998396]\n",
      "iteration 185\n",
      "training loss: 0.22695 , training accuracy: [0.93493152]\n",
      "testing loss 0.207331 test accuracy is [0.94078684]\n",
      "iteration 205\n",
      "training loss: 0.225038 , training accuracy: [0.93560845]\n",
      "testing loss 0.206023 test accuracy is [0.94078684]\n",
      "iteration 225\n",
      "training loss: 0.224167 , training accuracy: [0.93493152]\n",
      "testing loss 0.205131 test accuracy is [0.94018465]\n",
      "iteration 245\n",
      "training loss: 0.22489 , training accuracy: [0.93389612]\n",
      "testing loss 0.206813 test accuracy is [0.93697309]\n",
      "iteration 265\n",
      "training loss: 0.221289 , training accuracy: [0.93564832]\n",
      "testing loss 0.203197 test accuracy is [0.94158971]\n",
      "iteration 285\n",
      "training loss: 0.219818 , training accuracy: [0.93732083]\n",
      "testing loss 0.202818 test accuracy is [0.94179046]\n",
      "iteration 305\n",
      "training loss: 0.218479 , training accuracy: [0.93755972]\n",
      "testing loss 0.20146 test accuracy is [0.94259334]\n",
      "iteration 325\n",
      "training loss: 0.217599 , training accuracy: [0.93740046]\n",
      "testing loss 0.200567 test accuracy is [0.9421919]\n",
      "iteration 345\n",
      "training loss: 0.21801 , training accuracy: [0.93787831]\n",
      "testing loss 0.20071 test accuracy is [0.94359696]\n",
      "iteration 365\n",
      "training loss: 0.215796 , training accuracy: [0.93771905]\n",
      "testing loss 0.199659 test accuracy is [0.94179046]\n",
      "iteration 385\n",
      "training loss: 0.215069 , training accuracy: [0.93744028]\n",
      "testing loss 0.199095 test accuracy is [0.94239259]\n",
      "iteration 405\n",
      "training loss: 0.213642 , training accuracy: [0.93815708]\n",
      "testing loss 0.197683 test accuracy is [0.94199115]\n",
      "iteration 425\n",
      "training loss: 0.21228 , training accuracy: [0.93899333]\n",
      "testing loss 0.195823 test accuracy is [0.94339621]\n",
      "iteration 445\n",
      "training loss: 0.212308 , training accuracy: [0.93867475]\n",
      "testing loss 0.195065 test accuracy is [0.9439984]\n",
      "iteration 465\n",
      "training loss: 0.210794 , training accuracy: [0.9387942]\n",
      "testing loss 0.194299 test accuracy is [0.9439984]\n",
      "iteration 485\n",
      "training loss: 0.209304 , training accuracy: [0.93939155]\n",
      "testing loss 0.193227 test accuracy is [0.94419914]\n",
      "Epoch:  3\n",
      "iteration 5\n",
      "training loss: 0.208546 , training accuracy: [0.93967026]\n",
      "testing loss 0.193302 test accuracy is [0.94379765]\n",
      "iteration 25\n",
      "training loss: 0.208402 , training accuracy: [0.93911278]\n",
      "testing loss 0.193716 test accuracy is [0.9439984]\n",
      "iteration 45\n",
      "training loss: 0.207033 , training accuracy: [0.93994904]\n",
      "testing loss 0.191964 test accuracy is [0.94359696]\n",
      "iteration 65\n",
      "training loss: 0.20616 , training accuracy: [0.94038707]\n",
      "testing loss 0.191146 test accuracy is [0.94460058]\n",
      "iteration 85\n",
      "training loss: 0.20586 , training accuracy: [0.9404667]\n",
      "testing loss 0.191566 test accuracy is [0.94359696]\n",
      "iteration 105\n",
      "training loss: 0.205372 , training accuracy: [0.94014812]\n",
      "testing loss 0.190988 test accuracy is [0.94520271]\n",
      "iteration 125\n",
      "training loss: 0.20439 , training accuracy: [0.94014812]\n",
      "testing loss 0.190122 test accuracy is [0.94319552]\n",
      "iteration 145\n",
      "training loss: 0.205111 , training accuracy: [0.94050652]\n",
      "testing loss 0.190722 test accuracy is [0.94419914]\n",
      "iteration 165\n",
      "training loss: 0.203695 , training accuracy: [0.9401083]\n",
      "testing loss 0.18944 test accuracy is [0.94319552]\n",
      "iteration 185\n",
      "training loss: 0.20217 , training accuracy: [0.94022781]\n",
      "testing loss 0.187631 test accuracy is [0.94520271]\n",
      "iteration 205\n",
      "training loss: 0.203619 , training accuracy: [0.94014812]\n",
      "testing loss 0.189281 test accuracy is [0.94359696]\n",
      "iteration 225\n",
      "training loss: 0.201124 , training accuracy: [0.94066584]\n",
      "testing loss 0.187126 test accuracy is [0.94540346]\n",
      "iteration 245\n",
      "training loss: 0.200534 , training accuracy: [0.94138259]\n",
      "testing loss 0.186822 test accuracy is [0.94419914]\n",
      "iteration 265\n",
      "training loss: 0.200444 , training accuracy: [0.94130296]\n",
      "testing loss 0.186744 test accuracy is [0.94520271]\n",
      "iteration 285\n",
      "training loss: 0.199241 , training accuracy: [0.94186044]\n",
      "testing loss 0.185773 test accuracy is [0.94520271]\n",
      "iteration 305\n",
      "training loss: 0.198352 , training accuracy: [0.94174099]\n",
      "testing loss 0.185279 test accuracy is [0.94439983]\n",
      "iteration 325\n",
      "training loss: 0.198036 , training accuracy: [0.94186044]\n",
      "testing loss 0.184875 test accuracy is [0.94500202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 345\n",
      "training loss: 0.197673 , training accuracy: [0.94190031]\n",
      "testing loss 0.18437 test accuracy is [0.94520271]\n",
      "iteration 365\n",
      "training loss: 0.196967 , training accuracy: [0.94201976]\n",
      "testing loss 0.184006 test accuracy is [0.94540346]\n",
      "iteration 385\n",
      "training loss: 0.196648 , training accuracy: [0.94126314]\n",
      "testing loss 0.184504 test accuracy is [0.94379765]\n",
      "iteration 405\n",
      "training loss: 0.196405 , training accuracy: [0.94178081]\n",
      "testing loss 0.184006 test accuracy is [0.94439983]\n",
      "iteration 425\n",
      "training loss: 0.195415 , training accuracy: [0.94257724]\n",
      "testing loss 0.182854 test accuracy is [0.94520271]\n",
      "iteration 445\n",
      "training loss: 0.195575 , training accuracy: [0.94233835]\n",
      "testing loss 0.183037 test accuracy is [0.94520271]\n",
      "iteration 465\n",
      "training loss: 0.195238 , training accuracy: [0.94233835]\n",
      "testing loss 0.182878 test accuracy is [0.94480127]\n",
      "iteration 485\n",
      "training loss: 0.194411 , training accuracy: [0.94273657]\n",
      "testing loss 0.182378 test accuracy is [0.94480127]\n",
      "Epoch:  4\n",
      "iteration 5\n",
      "training loss: 0.194126 , training accuracy: [0.9428162]\n",
      "testing loss 0.18206 test accuracy is [0.94460058]\n",
      "iteration 25\n",
      "training loss: 0.193357 , training accuracy: [0.94253743]\n",
      "testing loss 0.181286 test accuracy is [0.94500202]\n",
      "iteration 45\n",
      "training loss: 0.193409 , training accuracy: [0.94197994]\n",
      "testing loss 0.182503 test accuracy is [0.94520271]\n",
      "iteration 65\n",
      "training loss: 0.192751 , training accuracy: [0.94182062]\n",
      "testing loss 0.181498 test accuracy is [0.94500202]\n",
      "iteration 85\n",
      "training loss: 0.193098 , training accuracy: [0.94201976]\n",
      "testing loss 0.182082 test accuracy is [0.94480127]\n",
      "iteration 105\n",
      "training loss: 0.191603 , training accuracy: [0.9428162]\n",
      "testing loss 0.180195 test accuracy is [0.94500202]\n",
      "iteration 125\n",
      "training loss: 0.192098 , training accuracy: [0.94333386]\n",
      "testing loss 0.180736 test accuracy is [0.94460058]\n",
      "iteration 145\n",
      "training loss: 0.19101 , training accuracy: [0.94329405]\n",
      "testing loss 0.179732 test accuracy is [0.94500202]\n",
      "iteration 165\n",
      "training loss: 0.191049 , training accuracy: [0.9431746]\n",
      "testing loss 0.179875 test accuracy is [0.94540346]\n",
      "iteration 185\n",
      "training loss: 0.191576 , training accuracy: [0.94305509]\n",
      "testing loss 0.181092 test accuracy is [0.94600564]\n",
      "iteration 205\n",
      "training loss: 0.190251 , training accuracy: [0.94369227]\n",
      "testing loss 0.179611 test accuracy is [0.94480127]\n",
      "iteration 225\n",
      "training loss: 0.189742 , training accuracy: [0.94401085]\n",
      "testing loss 0.178786 test accuracy is [0.94520271]\n",
      "iteration 245\n",
      "training loss: 0.190088 , training accuracy: [0.94333386]\n",
      "testing loss 0.178179 test accuracy is [0.94580489]\n",
      "iteration 265\n",
      "training loss: 0.188843 , training accuracy: [0.94345331]\n",
      "testing loss 0.177569 test accuracy is [0.94560415]\n",
      "iteration 285\n",
      "training loss: 0.188684 , training accuracy: [0.94345331]\n",
      "testing loss 0.17749 test accuracy is [0.94480127]\n",
      "iteration 305\n",
      "training loss: 0.189269 , training accuracy: [0.94333386]\n",
      "testing loss 0.178537 test accuracy is [0.94560415]\n",
      "iteration 325\n",
      "training loss: 0.188514 , training accuracy: [0.94409049]\n",
      "testing loss 0.177221 test accuracy is [0.94500202]\n",
      "iteration 345\n",
      "training loss: 0.188743 , training accuracy: [0.9434135]\n",
      "testing loss 0.177162 test accuracy is [0.94480127]\n",
      "iteration 365\n",
      "training loss: 0.187167 , training accuracy: [0.94357282]\n",
      "testing loss 0.176302 test accuracy is [0.94620633]\n",
      "iteration 385\n",
      "training loss: 0.18708 , training accuracy: [0.94428957]\n",
      "testing loss 0.176598 test accuracy is [0.94640708]\n",
      "iteration 405\n",
      "training loss: 0.186747 , training accuracy: [0.94353294]\n",
      "testing loss 0.176505 test accuracy is [0.94580489]\n",
      "iteration 425\n",
      "training loss: 0.186544 , training accuracy: [0.94440907]\n",
      "testing loss 0.175923 test accuracy is [0.94720995]\n",
      "iteration 445\n",
      "training loss: 0.186178 , training accuracy: [0.94468778]\n",
      "testing loss 0.175945 test accuracy is [0.94620633]\n",
      "iteration 465\n",
      "training loss: 0.185741 , training accuracy: [0.94424975]\n",
      "testing loss 0.176195 test accuracy is [0.94600564]\n",
      "iteration 485\n",
      "training loss: 0.185459 , training accuracy: [0.94504619]\n",
      "testing loss 0.175275 test accuracy is [0.94700921]\n",
      "Epoch:  5\n",
      "iteration 5\n",
      "training loss: 0.185004 , training accuracy: [0.94480729]\n",
      "testing loss 0.175027 test accuracy is [0.94700921]\n",
      "iteration 25\n",
      "training loss: 0.184649 , training accuracy: [0.94460815]\n",
      "testing loss 0.174884 test accuracy is [0.94680852]\n",
      "iteration 45\n",
      "training loss: 0.184784 , training accuracy: [0.94476742]\n",
      "testing loss 0.175198 test accuracy is [0.94720995]\n",
      "iteration 65\n",
      "training loss: 0.184933 , training accuracy: [0.94417012]\n",
      "testing loss 0.17571 test accuracy is [0.94700921]\n",
      "iteration 85\n",
      "training loss: 0.184562 , training accuracy: [0.94409049]\n",
      "testing loss 0.175762 test accuracy is [0.94720995]\n",
      "iteration 105\n",
      "training loss: 0.184607 , training accuracy: [0.94424975]\n",
      "testing loss 0.175902 test accuracy is [0.94600564]\n",
      "iteration 125\n",
      "training loss: 0.183668 , training accuracy: [0.945086]\n",
      "testing loss 0.174341 test accuracy is [0.94640708]\n",
      "iteration 145\n",
      "training loss: 0.183171 , training accuracy: [0.94520545]\n",
      "testing loss 0.174269 test accuracy is [0.94620633]\n",
      "iteration 165\n",
      "training loss: 0.183178 , training accuracy: [0.94488692]\n",
      "testing loss 0.174064 test accuracy is [0.94680852]\n",
      "iteration 185\n",
      "training loss: 0.183243 , training accuracy: [0.94548422]\n",
      "testing loss 0.174465 test accuracy is [0.94600564]\n",
      "iteration 205\n",
      "training loss: 0.182581 , training accuracy: [0.94536477]\n",
      "testing loss 0.174493 test accuracy is [0.94580489]\n",
      "iteration 225\n",
      "training loss: 0.182451 , training accuracy: [0.94532496]\n",
      "testing loss 0.174867 test accuracy is [0.94580489]\n",
      "iteration 245\n",
      "training loss: 0.181971 , training accuracy: [0.94476742]\n",
      "testing loss 0.174367 test accuracy is [0.94480127]\n",
      "iteration 265\n",
      "training loss: 0.181781 , training accuracy: [0.94484711]\n",
      "testing loss 0.174367 test accuracy is [0.94540346]\n",
      "iteration 285\n",
      "training loss: 0.181845 , training accuracy: [0.94552404]\n",
      "testing loss 0.173492 test accuracy is [0.94680852]\n",
      "iteration 305\n",
      "training loss: 0.181583 , training accuracy: [0.94608158]\n",
      "testing loss 0.173096 test accuracy is [0.9474107]\n",
      "iteration 325\n",
      "training loss: 0.18117 , training accuracy: [0.94552404]\n",
      "testing loss 0.172366 test accuracy is [0.9474107]\n",
      "iteration 345\n",
      "training loss: 0.182166 , training accuracy: [0.94544441]\n",
      "testing loss 0.172874 test accuracy is [0.94700921]\n",
      "iteration 365\n",
      "training loss: 0.180986 , training accuracy: [0.94596207]\n",
      "testing loss 0.172233 test accuracy is [0.9474107]\n",
      "iteration 385\n",
      "training loss: 0.180433 , training accuracy: [0.94560367]\n",
      "testing loss 0.172028 test accuracy is [0.9474107]\n",
      "iteration 405\n",
      "training loss: 0.180072 , training accuracy: [0.94584262]\n",
      "testing loss 0.172488 test accuracy is [0.9474107]\n",
      "iteration 425\n",
      "training loss: 0.179832 , training accuracy: [0.94588244]\n",
      "testing loss 0.172527 test accuracy is [0.9474107]\n",
      "iteration 445\n",
      "training loss: 0.179946 , training accuracy: [0.94548422]\n",
      "testing loss 0.172615 test accuracy is [0.94620633]\n",
      "iteration 465\n",
      "training loss: 0.17965 , training accuracy: [0.94564354]\n",
      "testing loss 0.171919 test accuracy is [0.94660777]\n",
      "iteration 485\n",
      "training loss: 0.179008 , training accuracy: [0.94628066]\n",
      "testing loss 0.171328 test accuracy is [0.94761139]\n",
      "Epoch:  6\n",
      "iteration 5\n",
      "training loss: 0.178852 , training accuracy: [0.94604176]\n",
      "testing loss 0.171267 test accuracy is [0.9474107]\n",
      "iteration 25\n",
      "training loss: 0.178782 , training accuracy: [0.94675851]\n",
      "testing loss 0.171003 test accuracy is [0.94781214]\n",
      "iteration 45\n",
      "training loss: 0.17937 , training accuracy: [0.94616121]\n",
      "testing loss 0.171413 test accuracy is [0.9474107]\n",
      "iteration 65\n",
      "training loss: 0.179874 , training accuracy: [0.94580281]\n",
      "testing loss 0.172478 test accuracy is [0.94620633]\n",
      "iteration 85\n",
      "training loss: 0.179946 , training accuracy: [0.94532496]\n",
      "testing loss 0.173828 test accuracy is [0.94600564]\n",
      "iteration 105\n",
      "training loss: 0.178677 , training accuracy: [0.94651961]\n",
      "testing loss 0.171713 test accuracy is [0.94640708]\n",
      "iteration 125\n",
      "training loss: 0.178196 , training accuracy: [0.94675851]\n",
      "testing loss 0.170692 test accuracy is [0.94700921]\n",
      "iteration 145\n",
      "training loss: 0.177564 , training accuracy: [0.94628066]\n",
      "testing loss 0.16956 test accuracy is [0.94700921]\n",
      "iteration 165\n",
      "training loss: 0.177684 , training accuracy: [0.9464798]\n",
      "testing loss 0.169558 test accuracy is [0.94861501]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 185\n",
      "training loss: 0.177828 , training accuracy: [0.94632047]\n",
      "testing loss 0.170678 test accuracy is [0.9474107]\n",
      "iteration 205\n",
      "training loss: 0.177599 , training accuracy: [0.94679832]\n",
      "testing loss 0.169534 test accuracy is [0.94861501]\n",
      "iteration 225\n",
      "training loss: 0.17735 , training accuracy: [0.94659925]\n",
      "testing loss 0.170103 test accuracy is [0.94781214]\n",
      "iteration 245\n",
      "training loss: 0.177195 , training accuracy: [0.9464798]\n",
      "testing loss 0.169079 test accuracy is [0.94801283]\n",
      "iteration 265\n",
      "training loss: 0.176745 , training accuracy: [0.94675851]\n",
      "testing loss 0.168799 test accuracy is [0.94761139]\n",
      "iteration 285\n",
      "training loss: 0.176868 , training accuracy: [0.94659925]\n",
      "testing loss 0.169277 test accuracy is [0.94801283]\n",
      "iteration 305\n",
      "training loss: 0.176343 , training accuracy: [0.94624084]\n",
      "testing loss 0.16925 test accuracy is [0.94720995]\n",
      "iteration 325\n",
      "training loss: 0.176051 , training accuracy: [0.94675851]\n",
      "testing loss 0.168592 test accuracy is [0.94720995]\n",
      "iteration 345\n",
      "training loss: 0.176125 , training accuracy: [0.94695765]\n",
      "testing loss 0.168572 test accuracy is [0.9474107]\n",
      "iteration 365\n",
      "training loss: 0.175772 , training accuracy: [0.94723636]\n",
      "testing loss 0.168759 test accuracy is [0.94720995]\n",
      "iteration 385\n",
      "training loss: 0.175475 , training accuracy: [0.94731605]\n",
      "testing loss 0.168777 test accuracy is [0.94680852]\n",
      "iteration 405\n",
      "training loss: 0.175072 , training accuracy: [0.94751513]\n",
      "testing loss 0.168469 test accuracy is [0.94761139]\n",
      "iteration 425\n",
      "training loss: 0.175086 , training accuracy: [0.94719654]\n",
      "testing loss 0.168315 test accuracy is [0.9474107]\n",
      "iteration 445\n",
      "training loss: 0.175104 , training accuracy: [0.94767439]\n",
      "testing loss 0.16797 test accuracy is [0.94801283]\n",
      "iteration 465\n",
      "training loss: 0.175087 , training accuracy: [0.94719654]\n",
      "testing loss 0.168319 test accuracy is [0.94680852]\n",
      "iteration 485\n",
      "training loss: 0.174895 , training accuracy: [0.94731605]\n",
      "testing loss 0.169203 test accuracy is [0.94700921]\n",
      "Epoch:  7\n",
      "iteration 5\n",
      "training loss: 0.174602 , training accuracy: [0.94727618]\n",
      "testing loss 0.169271 test accuracy is [0.9474107]\n",
      "iteration 25\n",
      "training loss: 0.174667 , training accuracy: [0.9470771]\n",
      "testing loss 0.169839 test accuracy is [0.94700921]\n",
      "iteration 45\n",
      "training loss: 0.174186 , training accuracy: [0.94783372]\n",
      "testing loss 0.168505 test accuracy is [0.94821358]\n",
      "iteration 65\n",
      "training loss: 0.174007 , training accuracy: [0.94819212]\n",
      "testing loss 0.168045 test accuracy is [0.9474107]\n",
      "iteration 85\n",
      "training loss: 0.173844 , training accuracy: [0.94755495]\n",
      "testing loss 0.168216 test accuracy is [0.94680852]\n",
      "iteration 105\n",
      "training loss: 0.174679 , training accuracy: [0.94683814]\n",
      "testing loss 0.170136 test accuracy is [0.94680852]\n",
      "iteration 125\n",
      "training loss: 0.173478 , training accuracy: [0.94767439]\n",
      "testing loss 0.16794 test accuracy is [0.94781214]\n",
      "iteration 145\n",
      "training loss: 0.173669 , training accuracy: [0.9477939]\n",
      "testing loss 0.167523 test accuracy is [0.9474107]\n",
      "iteration 165\n",
      "training loss: 0.174374 , training accuracy: [0.9470771]\n",
      "testing loss 0.169132 test accuracy is [0.94640708]\n",
      "iteration 185\n",
      "training loss: 0.173466 , training accuracy: [0.94775409]\n",
      "testing loss 0.167998 test accuracy is [0.94600564]\n",
      "iteration 205\n",
      "training loss: 0.173459 , training accuracy: [0.94771427]\n",
      "testing loss 0.168409 test accuracy is [0.9474107]\n",
      "iteration 225\n",
      "training loss: 0.173283 , training accuracy: [0.9483912]\n",
      "testing loss 0.166635 test accuracy is [0.94881576]\n",
      "iteration 245\n",
      "training loss: 0.173333 , training accuracy: [0.9487496]\n",
      "testing loss 0.166752 test accuracy is [0.94961864]\n",
      "iteration 265\n",
      "training loss: 0.173472 , training accuracy: [0.94863015]\n",
      "testing loss 0.1667 test accuracy is [0.94901645]\n",
      "iteration 285\n",
      "training loss: 0.173298 , training accuracy: [0.94859034]\n",
      "testing loss 0.166678 test accuracy is [0.94801283]\n",
      "iteration 305\n",
      "training loss: 0.172268 , training accuracy: [0.94831157]\n",
      "testing loss 0.166451 test accuracy is [0.9474107]\n",
      "iteration 325\n",
      "training loss: 0.172478 , training accuracy: [0.94843102]\n",
      "testing loss 0.165898 test accuracy is [0.94781214]\n",
      "iteration 345\n",
      "training loss: 0.173244 , training accuracy: [0.94783372]\n",
      "testing loss 0.166082 test accuracy is [0.9474107]\n",
      "iteration 365\n",
      "training loss: 0.173687 , training accuracy: [0.94731605]\n",
      "testing loss 0.166533 test accuracy is [0.94761139]\n",
      "iteration 385\n",
      "training loss: 0.172337 , training accuracy: [0.94791335]\n",
      "testing loss 0.165817 test accuracy is [0.9474107]\n",
      "iteration 405\n",
      "training loss: 0.172249 , training accuracy: [0.94687802]\n",
      "testing loss 0.167299 test accuracy is [0.9474107]\n",
      "iteration 425\n",
      "training loss: 0.173933 , training accuracy: [0.94675851]\n",
      "testing loss 0.169706 test accuracy is [0.94660777]\n",
      "iteration 445\n",
      "training loss: 0.171877 , training accuracy: [0.94771427]\n",
      "testing loss 0.167419 test accuracy is [0.94761139]\n",
      "iteration 465\n",
      "training loss: 0.171649 , training accuracy: [0.9481523]\n",
      "testing loss 0.166022 test accuracy is [0.94720995]\n",
      "iteration 485\n",
      "training loss: 0.17125 , training accuracy: [0.94763458]\n",
      "testing loss 0.166224 test accuracy is [0.94720995]\n",
      "Epoch:  8\n",
      "iteration 5\n",
      "training loss: 0.172269 , training accuracy: [0.9474355]\n",
      "testing loss 0.167603 test accuracy is [0.94761139]\n",
      "iteration 25\n",
      "training loss: 0.171547 , training accuracy: [0.94847083]\n",
      "testing loss 0.166737 test accuracy is [0.9474107]\n",
      "iteration 45\n",
      "training loss: 0.171553 , training accuracy: [0.9480328]\n",
      "testing loss 0.16647 test accuracy is [0.94801283]\n",
      "iteration 65\n",
      "training loss: 0.171599 , training accuracy: [0.94799298]\n",
      "testing loss 0.166514 test accuracy is [0.94761139]\n",
      "iteration 85\n",
      "training loss: 0.171052 , training accuracy: [0.94823194]\n",
      "testing loss 0.166256 test accuracy is [0.94761139]\n",
      "iteration 105\n",
      "training loss: 0.170872 , training accuracy: [0.94767439]\n",
      "testing loss 0.166105 test accuracy is [0.94821358]\n",
      "iteration 125\n",
      "training loss: 0.17028 , training accuracy: [0.94811243]\n",
      "testing loss 0.166175 test accuracy is [0.94761139]\n",
      "iteration 145\n",
      "training loss: 0.170219 , training accuracy: [0.94827175]\n",
      "testing loss 0.165861 test accuracy is [0.94801283]\n",
      "iteration 165\n",
      "training loss: 0.169994 , training accuracy: [0.9481523]\n",
      "testing loss 0.165607 test accuracy is [0.94720995]\n",
      "iteration 185\n",
      "training loss: 0.169812 , training accuracy: [0.94835138]\n",
      "testing loss 0.164794 test accuracy is [0.94841427]\n",
      "iteration 205\n",
      "training loss: 0.169739 , training accuracy: [0.94894868]\n",
      "testing loss 0.164789 test accuracy is [0.94761139]\n",
      "iteration 225\n",
      "training loss: 0.16954 , training accuracy: [0.94866997]\n",
      "testing loss 0.164618 test accuracy is [0.94841427]\n",
      "iteration 245\n",
      "training loss: 0.169438 , training accuracy: [0.94835138]\n",
      "testing loss 0.164935 test accuracy is [0.94841427]\n",
      "iteration 265\n",
      "training loss: 0.169393 , training accuracy: [0.94847083]\n",
      "testing loss 0.164904 test accuracy is [0.94801283]\n",
      "iteration 285\n",
      "training loss: 0.169189 , training accuracy: [0.94847083]\n",
      "testing loss 0.164766 test accuracy is [0.94801283]\n",
      "iteration 305\n",
      "training loss: 0.169076 , training accuracy: [0.94878942]\n",
      "testing loss 0.164757 test accuracy is [0.94801283]\n",
      "iteration 325\n",
      "training loss: 0.169229 , training accuracy: [0.94855052]\n",
      "testing loss 0.16578 test accuracy is [0.94720995]\n",
      "iteration 345\n",
      "training loss: 0.169548 , training accuracy: [0.94811243]\n",
      "testing loss 0.166001 test accuracy is [0.94781214]\n",
      "iteration 365\n",
      "training loss: 0.169311 , training accuracy: [0.94823194]\n",
      "testing loss 0.165741 test accuracy is [0.94861501]\n",
      "iteration 385\n",
      "training loss: 0.169095 , training accuracy: [0.94831157]\n",
      "testing loss 0.165111 test accuracy is [0.94881576]\n",
      "iteration 405\n",
      "training loss: 0.168964 , training accuracy: [0.94878942]\n",
      "testing loss 0.164832 test accuracy is [0.94861501]\n",
      "iteration 425\n",
      "training loss: 0.168385 , training accuracy: [0.94906819]\n",
      "testing loss 0.164467 test accuracy is [0.94861501]\n",
      "iteration 445\n",
      "training loss: 0.168617 , training accuracy: [0.94859034]\n",
      "testing loss 0.164105 test accuracy is [0.9492172]\n",
      "iteration 465\n",
      "training loss: 0.168261 , training accuracy: [0.94882923]\n",
      "testing loss 0.164229 test accuracy is [0.94761139]\n",
      "iteration 485\n",
      "training loss: 0.167997 , training accuracy: [0.9493469]\n",
      "testing loss 0.16422 test accuracy is [0.94881576]\n",
      "Epoch:  9\n",
      "iteration 5\n",
      "training loss: 0.167906 , training accuracy: [0.94902837]\n",
      "testing loss 0.163583 test accuracy is [0.94821358]\n",
      "iteration 25\n",
      "training loss: 0.168116 , training accuracy: [0.94894868]\n",
      "testing loss 0.16359 test accuracy is [0.9492172]\n",
      "iteration 45\n",
      "training loss: 0.167949 , training accuracy: [0.94882923]\n",
      "testing loss 0.163434 test accuracy is [0.94861501]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 65\n",
      "training loss: 0.168484 , training accuracy: [0.94906819]\n",
      "testing loss 0.163418 test accuracy is [0.94881576]\n",
      "iteration 85\n",
      "training loss: 0.167747 , training accuracy: [0.94926727]\n",
      "testing loss 0.163484 test accuracy is [0.94861501]\n",
      "iteration 105\n",
      "training loss: 0.167772 , training accuracy: [0.94902837]\n",
      "testing loss 0.164231 test accuracy is [0.94781214]\n",
      "iteration 125\n",
      "training loss: 0.167544 , training accuracy: [0.94950622]\n",
      "testing loss 0.164352 test accuracy is [0.9492172]\n",
      "iteration 145\n",
      "training loss: 0.167559 , training accuracy: [0.94946641]\n",
      "testing loss 0.164215 test accuracy is [0.94781214]\n",
      "iteration 165\n",
      "training loss: 0.167408 , training accuracy: [0.94958586]\n",
      "testing loss 0.163772 test accuracy is [0.94981933]\n",
      "iteration 185\n",
      "training loss: 0.167566 , training accuracy: [0.94986463]\n",
      "testing loss 0.164378 test accuracy is [0.9492172]\n",
      "iteration 205\n",
      "training loss: 0.167404 , training accuracy: [0.94962567]\n",
      "testing loss 0.164264 test accuracy is [0.94841427]\n",
      "iteration 225\n",
      "training loss: 0.167498 , training accuracy: [0.9497053]\n",
      "testing loss 0.164753 test accuracy is [0.94841427]\n",
      "iteration 245\n",
      "training loss: 0.16691 , training accuracy: [0.94922745]\n",
      "testing loss 0.162855 test accuracy is [0.94901645]\n",
      "iteration 265\n",
      "training loss: 0.16714 , training accuracy: [0.94922745]\n",
      "testing loss 0.162903 test accuracy is [0.94981933]\n",
      "iteration 285\n",
      "training loss: 0.166647 , training accuracy: [0.94978493]\n",
      "testing loss 0.163005 test accuracy is [0.94761139]\n",
      "iteration 305\n",
      "training loss: 0.167637 , training accuracy: [0.94851065]\n",
      "testing loss 0.164153 test accuracy is [0.94821358]\n",
      "iteration 325\n",
      "training loss: 0.166596 , training accuracy: [0.94914782]\n",
      "testing loss 0.163116 test accuracy is [0.94821358]\n",
      "iteration 345\n",
      "training loss: 0.166774 , training accuracy: [0.94914782]\n",
      "testing loss 0.163354 test accuracy is [0.94841427]\n",
      "iteration 365\n",
      "training loss: 0.166671 , training accuracy: [0.94930708]\n",
      "testing loss 0.163339 test accuracy is [0.94841427]\n",
      "iteration 385\n",
      "training loss: 0.166545 , training accuracy: [0.94914782]\n",
      "testing loss 0.163228 test accuracy is [0.94821358]\n",
      "iteration 405\n",
      "training loss: 0.165881 , training accuracy: [0.94922745]\n",
      "testing loss 0.162431 test accuracy is [0.94841427]\n",
      "iteration 425\n",
      "training loss: 0.165973 , training accuracy: [0.94922745]\n",
      "testing loss 0.162509 test accuracy is [0.94761139]\n",
      "iteration 445\n",
      "training loss: 0.165834 , training accuracy: [0.94978493]\n",
      "testing loss 0.162999 test accuracy is [0.9492172]\n",
      "iteration 465\n",
      "training loss: 0.16619 , training accuracy: [0.94966549]\n",
      "testing loss 0.163795 test accuracy is [0.9492172]\n",
      "iteration 485\n",
      "training loss: 0.165561 , training accuracy: [0.9497053]\n",
      "testing loss 0.162496 test accuracy is [0.95002007]\n",
      "Epoch:  10\n",
      "iteration 5\n",
      "training loss: 0.165433 , training accuracy: [0.94946641]\n",
      "testing loss 0.162317 test accuracy is [0.94961864]\n",
      "iteration 25\n",
      "training loss: 0.165566 , training accuracy: [0.94950622]\n",
      "testing loss 0.162738 test accuracy is [0.94901645]\n",
      "iteration 45\n",
      "training loss: 0.165244 , training accuracy: [0.94966549]\n",
      "testing loss 0.162525 test accuracy is [0.94961864]\n",
      "iteration 65\n",
      "training loss: 0.165724 , training accuracy: [0.94994426]\n",
      "testing loss 0.162522 test accuracy is [0.94861501]\n",
      "iteration 85\n",
      "training loss: 0.16586 , training accuracy: [0.94990444]\n",
      "testing loss 0.16257 test accuracy is [0.94801283]\n",
      "iteration 105\n",
      "training loss: 0.16649 , training accuracy: [0.94954604]\n",
      "testing loss 0.162511 test accuracy is [0.94861501]\n",
      "iteration 125\n",
      "training loss: 0.165942 , training accuracy: [0.94954604]\n",
      "testing loss 0.16189 test accuracy is [0.94801283]\n",
      "iteration 145\n",
      "training loss: 0.166116 , training accuracy: [0.94962567]\n",
      "testing loss 0.162241 test accuracy is [0.94861501]\n",
      "iteration 165\n",
      "training loss: 0.165441 , training accuracy: [0.94998407]\n",
      "testing loss 0.161812 test accuracy is [0.94801283]\n",
      "iteration 185\n",
      "training loss: 0.164874 , training accuracy: [0.95030266]\n",
      "testing loss 0.161617 test accuracy is [0.94941789]\n",
      "iteration 205\n",
      "training loss: 0.164805 , training accuracy: [0.95038229]\n",
      "testing loss 0.161989 test accuracy is [0.94881576]\n",
      "iteration 225\n",
      "training loss: 0.164709 , training accuracy: [0.95022303]\n",
      "testing loss 0.161769 test accuracy is [0.94961864]\n",
      "iteration 245\n",
      "training loss: 0.164612 , training accuracy: [0.95022303]\n",
      "testing loss 0.162173 test accuracy is [0.94941789]\n",
      "iteration 265\n",
      "training loss: 0.164446 , training accuracy: [0.95010352]\n",
      "testing loss 0.162092 test accuracy is [0.94861501]\n",
      "iteration 285\n",
      "training loss: 0.165234 , training accuracy: [0.94982481]\n",
      "testing loss 0.163633 test accuracy is [0.94821358]\n",
      "iteration 305\n",
      "training loss: 0.164345 , training accuracy: [0.94994426]\n",
      "testing loss 0.162063 test accuracy is [0.94801283]\n",
      "iteration 325\n",
      "training loss: 0.164222 , training accuracy: [0.95014334]\n",
      "testing loss 0.161982 test accuracy is [0.94821358]\n",
      "iteration 345\n",
      "training loss: 0.164841 , training accuracy: [0.94994426]\n",
      "testing loss 0.163742 test accuracy is [0.94941789]\n",
      "iteration 365\n",
      "training loss: 0.16458 , training accuracy: [0.94994426]\n",
      "testing loss 0.162897 test accuracy is [0.94881576]\n",
      "iteration 385\n",
      "training loss: 0.164256 , training accuracy: [0.95006371]\n",
      "testing loss 0.162065 test accuracy is [0.94781214]\n",
      "iteration 405\n",
      "training loss: 0.163922 , training accuracy: [0.95030266]\n",
      "testing loss 0.161493 test accuracy is [0.94881576]\n",
      "iteration 425\n",
      "training loss: 0.16424 , training accuracy: [0.94998407]\n",
      "testing loss 0.162029 test accuracy is [0.9492172]\n",
      "iteration 445\n",
      "training loss: 0.163706 , training accuracy: [0.95018315]\n",
      "testing loss 0.161787 test accuracy is [0.95062226]\n",
      "iteration 465\n",
      "training loss: 0.163461 , training accuracy: [0.95038229]\n",
      "testing loss 0.161179 test accuracy is [0.9492172]\n",
      "iteration 485\n",
      "training loss: 0.163749 , training accuracy: [0.95042211]\n",
      "testing loss 0.161672 test accuracy is [0.94941789]\n",
      "Epoch:  11\n",
      "iteration 5\n",
      "training loss: 0.163499 , training accuracy: [0.95006371]\n",
      "testing loss 0.161345 test accuracy is [0.94941789]\n",
      "iteration 25\n",
      "training loss: 0.163487 , training accuracy: [0.95026284]\n",
      "testing loss 0.161167 test accuracy is [0.94941789]\n",
      "iteration 45\n",
      "training loss: 0.163779 , training accuracy: [0.95014334]\n",
      "testing loss 0.160957 test accuracy is [0.94881576]\n",
      "iteration 65\n",
      "training loss: 0.163263 , training accuracy: [0.95026284]\n",
      "testing loss 0.160574 test accuracy is [0.94901645]\n",
      "iteration 85\n",
      "training loss: 0.1634 , training accuracy: [0.95026284]\n",
      "testing loss 0.160552 test accuracy is [0.95042151]\n",
      "iteration 105\n",
      "training loss: 0.163034 , training accuracy: [0.95034248]\n",
      "testing loss 0.160614 test accuracy is [0.95062226]\n",
      "iteration 125\n",
      "training loss: 0.163254 , training accuracy: [0.95070088]\n",
      "testing loss 0.160827 test accuracy is [0.95122439]\n",
      "iteration 145\n",
      "training loss: 0.164383 , training accuracy: [0.95093977]\n",
      "testing loss 0.162941 test accuracy is [0.94821358]\n",
      "iteration 165\n",
      "training loss: 0.163753 , training accuracy: [0.95058137]\n",
      "testing loss 0.162195 test accuracy is [0.94841427]\n",
      "iteration 185\n",
      "training loss: 0.162811 , training accuracy: [0.95002389]\n",
      "testing loss 0.160756 test accuracy is [0.94941789]\n",
      "iteration 205\n",
      "training loss: 0.162548 , training accuracy: [0.95058137]\n",
      "testing loss 0.160923 test accuracy is [0.9492172]\n",
      "iteration 225\n",
      "training loss: 0.162757 , training accuracy: [0.95097959]\n",
      "testing loss 0.161459 test accuracy is [0.94981933]\n",
      "iteration 245\n",
      "training loss: 0.16287 , training accuracy: [0.95030266]\n",
      "testing loss 0.161836 test accuracy is [0.94841427]\n",
      "iteration 265\n",
      "training loss: 0.162534 , training accuracy: [0.95018315]\n",
      "testing loss 0.160765 test accuracy is [0.95002007]\n",
      "iteration 285\n",
      "training loss: 0.162516 , training accuracy: [0.94994426]\n",
      "testing loss 0.160702 test accuracy is [0.9492172]\n",
      "iteration 305\n",
      "training loss: 0.163247 , training accuracy: [0.95030266]\n",
      "testing loss 0.161164 test accuracy is [0.94861501]\n",
      "iteration 325\n",
      "training loss: 0.162221 , training accuracy: [0.95014334]\n",
      "testing loss 0.160551 test accuracy is [0.94961864]\n",
      "iteration 345\n",
      "training loss: 0.163104 , training accuracy: [0.94998407]\n",
      "testing loss 0.161095 test accuracy is [0.94861501]\n",
      "iteration 365\n",
      "training loss: 0.162227 , training accuracy: [0.95086014]\n",
      "testing loss 0.160846 test accuracy is [0.94941789]\n",
      "iteration 385\n",
      "training loss: 0.162443 , training accuracy: [0.95089996]\n",
      "testing loss 0.16163 test accuracy is [0.94941789]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 405\n",
      "training loss: 0.162811 , training accuracy: [0.95062119]\n",
      "testing loss 0.161629 test accuracy is [0.9492172]\n",
      "iteration 425\n",
      "training loss: 0.162623 , training accuracy: [0.9497053]\n",
      "testing loss 0.1607 test accuracy is [0.94941789]\n",
      "iteration 445\n",
      "training loss: 0.163721 , training accuracy: [0.95046192]\n",
      "testing loss 0.161244 test accuracy is [0.94801283]\n",
      "iteration 465\n",
      "training loss: 0.162259 , training accuracy: [0.95105928]\n",
      "testing loss 0.16009 test accuracy is [0.9492172]\n",
      "iteration 485\n",
      "training loss: 0.161787 , training accuracy: [0.95125836]\n",
      "testing loss 0.159468 test accuracy is [0.95062226]\n",
      "Epoch:  12\n",
      "iteration 5\n",
      "training loss: 0.162415 , training accuracy: [0.95062119]\n",
      "testing loss 0.160627 test accuracy is [0.9492172]\n",
      "iteration 25\n",
      "training loss: 0.164277 , training accuracy: [0.94954604]\n",
      "testing loss 0.162485 test accuracy is [0.94961864]\n",
      "iteration 45\n",
      "training loss: 0.161336 , training accuracy: [0.95113891]\n",
      "testing loss 0.159474 test accuracy is [0.95082295]\n",
      "iteration 65\n",
      "training loss: 0.1616 , training accuracy: [0.95137781]\n",
      "testing loss 0.160354 test accuracy is [0.95062226]\n",
      "iteration 85\n",
      "training loss: 0.161469 , training accuracy: [0.95121855]\n",
      "testing loss 0.160032 test accuracy is [0.9510237]\n",
      "iteration 105\n",
      "training loss: 0.161426 , training accuracy: [0.95074069]\n",
      "testing loss 0.159654 test accuracy is [0.95022082]\n",
      "iteration 125\n",
      "training loss: 0.161212 , training accuracy: [0.95066106]\n",
      "testing loss 0.159445 test accuracy is [0.95022082]\n",
      "iteration 145\n",
      "training loss: 0.161438 , training accuracy: [0.95070088]\n",
      "testing loss 0.159991 test accuracy is [0.94941789]\n",
      "iteration 165\n",
      "training loss: 0.163118 , training accuracy: [0.95006371]\n",
      "testing loss 0.160851 test accuracy is [0.94801283]\n",
      "iteration 185\n",
      "training loss: 0.161249 , training accuracy: [0.95093977]\n",
      "testing loss 0.159747 test accuracy is [0.95002007]\n",
      "iteration 205\n",
      "training loss: 0.161016 , training accuracy: [0.95121855]\n",
      "testing loss 0.160168 test accuracy is [0.95062226]\n",
      "iteration 225\n",
      "training loss: 0.161462 , training accuracy: [0.95157695]\n",
      "testing loss 0.160722 test accuracy is [0.95082295]\n",
      "iteration 245\n",
      "training loss: 0.161489 , training accuracy: [0.95078051]\n",
      "testing loss 0.16008 test accuracy is [0.95042151]\n",
      "iteration 265\n",
      "training loss: 0.161072 , training accuracy: [0.95089996]\n",
      "testing loss 0.159609 test accuracy is [0.94941789]\n",
      "iteration 285\n",
      "training loss: 0.16169 , training accuracy: [0.95046192]\n",
      "testing loss 0.159987 test accuracy is [0.94901645]\n",
      "iteration 305\n",
      "training loss: 0.160794 , training accuracy: [0.95066106]\n",
      "testing loss 0.159986 test accuracy is [0.95002007]\n",
      "iteration 325\n",
      "training loss: 0.160472 , training accuracy: [0.95101941]\n",
      "testing loss 0.158855 test accuracy is [0.95082295]\n",
      "iteration 345\n",
      "training loss: 0.160805 , training accuracy: [0.95137781]\n",
      "testing loss 0.160165 test accuracy is [0.95062226]\n",
      "iteration 365\n",
      "training loss: 0.160823 , training accuracy: [0.9516964]\n",
      "testing loss 0.160602 test accuracy is [0.95062226]\n",
      "iteration 385\n",
      "training loss: 0.160213 , training accuracy: [0.95165658]\n",
      "testing loss 0.159167 test accuracy is [0.95002007]\n",
      "iteration 405\n",
      "training loss: 0.160082 , training accuracy: [0.95117873]\n",
      "testing loss 0.158971 test accuracy is [0.95042151]\n",
      "iteration 425\n",
      "training loss: 0.161982 , training accuracy: [0.95074069]\n",
      "testing loss 0.160237 test accuracy is [0.94901645]\n",
      "iteration 445\n",
      "training loss: 0.160265 , training accuracy: [0.95145744]\n",
      "testing loss 0.158671 test accuracy is [0.94981933]\n",
      "iteration 465\n",
      "training loss: 0.160379 , training accuracy: [0.95157695]\n",
      "testing loss 0.159836 test accuracy is [0.95002007]\n",
      "iteration 485\n",
      "training loss: 0.160111 , training accuracy: [0.95149732]\n",
      "testing loss 0.160027 test accuracy is [0.94981933]\n",
      "Epoch:  13\n",
      "iteration 5\n",
      "training loss: 0.159979 , training accuracy: [0.95165658]\n",
      "testing loss 0.159698 test accuracy is [0.95062226]\n",
      "iteration 25\n",
      "training loss: 0.160021 , training accuracy: [0.95133799]\n",
      "testing loss 0.160044 test accuracy is [0.95022082]\n",
      "iteration 45\n",
      "training loss: 0.159686 , training accuracy: [0.95141762]\n",
      "testing loss 0.159487 test accuracy is [0.95002007]\n",
      "iteration 65\n",
      "training loss: 0.159681 , training accuracy: [0.95121855]\n",
      "testing loss 0.159499 test accuracy is [0.95002007]\n",
      "iteration 85\n",
      "training loss: 0.160628 , training accuracy: [0.95105928]\n",
      "testing loss 0.160429 test accuracy is [0.94981933]\n",
      "iteration 105\n",
      "training loss: 0.160909 , training accuracy: [0.95129818]\n",
      "testing loss 0.16035 test accuracy is [0.94941789]\n",
      "iteration 125\n",
      "training loss: 0.161149 , training accuracy: [0.95117873]\n",
      "testing loss 0.160494 test accuracy is [0.9492172]\n",
      "iteration 145\n",
      "training loss: 0.159931 , training accuracy: [0.95137781]\n",
      "testing loss 0.160299 test accuracy is [0.94941789]\n",
      "iteration 165\n",
      "training loss: 0.160313 , training accuracy: [0.95125836]\n",
      "testing loss 0.16116 test accuracy is [0.94901645]\n",
      "iteration 185\n",
      "training loss: 0.160039 , training accuracy: [0.95078051]\n",
      "testing loss 0.160578 test accuracy is [0.94901645]\n",
      "iteration 205\n",
      "training loss: 0.159529 , training accuracy: [0.95101941]\n",
      "testing loss 0.158994 test accuracy is [0.95062226]\n",
      "iteration 225\n",
      "training loss: 0.160379 , training accuracy: [0.9516964]\n",
      "testing loss 0.16062 test accuracy is [0.94901645]\n",
      "iteration 245\n",
      "training loss: 0.159394 , training accuracy: [0.95105928]\n",
      "testing loss 0.158724 test accuracy is [0.95022082]\n",
      "iteration 265\n",
      "training loss: 0.159807 , training accuracy: [0.95173621]\n",
      "testing loss 0.158727 test accuracy is [0.95042151]\n",
      "iteration 285\n",
      "training loss: 0.159888 , training accuracy: [0.95165658]\n",
      "testing loss 0.158948 test accuracy is [0.95082295]\n",
      "iteration 305\n",
      "training loss: 0.160016 , training accuracy: [0.95145744]\n",
      "testing loss 0.15914 test accuracy is [0.95062226]\n",
      "iteration 325\n",
      "training loss: 0.159315 , training accuracy: [0.95213443]\n",
      "testing loss 0.159561 test accuracy is [0.95082295]\n",
      "iteration 345\n",
      "training loss: 0.158963 , training accuracy: [0.95201498]\n",
      "testing loss 0.158446 test accuracy is [0.95002007]\n",
      "iteration 365\n",
      "training loss: 0.159117 , training accuracy: [0.95149732]\n",
      "testing loss 0.158744 test accuracy is [0.95122439]\n",
      "iteration 385\n",
      "training loss: 0.159406 , training accuracy: [0.95177603]\n",
      "testing loss 0.159405 test accuracy is [0.95042151]\n",
      "iteration 405\n",
      "training loss: 0.16072 , training accuracy: [0.95117873]\n",
      "testing loss 0.159577 test accuracy is [0.94981933]\n",
      "iteration 425\n",
      "training loss: 0.159035 , training accuracy: [0.95153713]\n",
      "testing loss 0.158369 test accuracy is [0.95042151]\n",
      "iteration 445\n",
      "training loss: 0.158393 , training accuracy: [0.95217425]\n",
      "testing loss 0.157981 test accuracy is [0.95062226]\n",
      "iteration 465\n",
      "training loss: 0.15884 , training accuracy: [0.95141762]\n",
      "testing loss 0.157977 test accuracy is [0.95022082]\n",
      "iteration 485\n",
      "training loss: 0.158663 , training accuracy: [0.95229375]\n",
      "testing loss 0.158187 test accuracy is [0.95122439]\n",
      "Epoch:  14\n",
      "iteration 5\n",
      "training loss: 0.158904 , training accuracy: [0.95209461]\n",
      "testing loss 0.159042 test accuracy is [0.95042151]\n",
      "iteration 25\n",
      "training loss: 0.158662 , training accuracy: [0.95233357]\n",
      "testing loss 0.158225 test accuracy is [0.95142514]\n",
      "iteration 45\n",
      "training loss: 0.158546 , training accuracy: [0.95221406]\n",
      "testing loss 0.157821 test accuracy is [0.95062226]\n",
      "iteration 65\n",
      "training loss: 0.15989 , training accuracy: [0.95070088]\n",
      "testing loss 0.158726 test accuracy is [0.94981933]\n",
      "iteration 85\n",
      "training loss: 0.158222 , training accuracy: [0.95201498]\n",
      "testing loss 0.158253 test accuracy is [0.95042151]\n",
      "iteration 105\n",
      "training loss: 0.158362 , training accuracy: [0.9516964]\n",
      "testing loss 0.158398 test accuracy is [0.9510237]\n",
      "iteration 125\n",
      "training loss: 0.15819 , training accuracy: [0.95145744]\n",
      "testing loss 0.157626 test accuracy is [0.95062226]\n",
      "iteration 145\n",
      "training loss: 0.158514 , training accuracy: [0.9516964]\n",
      "testing loss 0.157404 test accuracy is [0.9492172]\n",
      "iteration 165\n",
      "training loss: 0.157903 , training accuracy: [0.95189553]\n",
      "testing loss 0.157659 test accuracy is [0.95122439]\n",
      "iteration 185\n",
      "training loss: 0.158333 , training accuracy: [0.95229375]\n",
      "testing loss 0.158987 test accuracy is [0.94981933]\n",
      "iteration 205\n",
      "training loss: 0.158644 , training accuracy: [0.95165658]\n",
      "testing loss 0.15875 test accuracy is [0.95042151]\n",
      "iteration 225\n",
      "training loss: 0.158245 , training accuracy: [0.95165658]\n",
      "testing loss 0.158513 test accuracy is [0.9510237]\n",
      "iteration 245\n",
      "training loss: 0.158462 , training accuracy: [0.95197517]\n",
      "testing loss 0.159046 test accuracy is [0.94941789]\n",
      "iteration 265\n",
      "training loss: 0.157789 , training accuracy: [0.95237339]\n",
      "testing loss 0.158042 test accuracy is [0.94981933]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 285\n",
      "training loss: 0.158283 , training accuracy: [0.95181584]\n",
      "testing loss 0.157985 test accuracy is [0.94941789]\n",
      "iteration 305\n",
      "training loss: 0.15749 , training accuracy: [0.95269191]\n",
      "testing loss 0.157585 test accuracy is [0.95042151]\n",
      "iteration 325\n",
      "training loss: 0.157796 , training accuracy: [0.95225388]\n",
      "testing loss 0.157218 test accuracy is [0.95002007]\n",
      "iteration 345\n",
      "training loss: 0.157997 , training accuracy: [0.95233357]\n",
      "testing loss 0.157335 test accuracy is [0.94981933]\n",
      "iteration 365\n",
      "training loss: 0.157615 , training accuracy: [0.95261228]\n",
      "testing loss 0.15733 test accuracy is [0.95002007]\n",
      "iteration 385\n",
      "training loss: 0.157588 , training accuracy: [0.95237339]\n",
      "testing loss 0.157382 test accuracy is [0.9510237]\n",
      "iteration 405\n",
      "training loss: 0.157591 , training accuracy: [0.95209461]\n",
      "testing loss 0.157676 test accuracy is [0.9510237]\n",
      "iteration 425\n",
      "training loss: 0.15774 , training accuracy: [0.95213443]\n",
      "testing loss 0.157483 test accuracy is [0.95062226]\n",
      "iteration 445\n",
      "training loss: 0.157734 , training accuracy: [0.95229375]\n",
      "testing loss 0.157188 test accuracy is [0.95002007]\n",
      "iteration 465\n",
      "training loss: 0.157896 , training accuracy: [0.95177603]\n",
      "testing loss 0.159168 test accuracy is [0.95002007]\n",
      "iteration 485\n",
      "training loss: 0.157222 , training accuracy: [0.95233357]\n",
      "testing loss 0.157245 test accuracy is [0.95162582]\n",
      "Epoch:  15\n",
      "iteration 5\n",
      "training loss: 0.158128 , training accuracy: [0.95141762]\n",
      "testing loss 0.157457 test accuracy is [0.9492172]\n",
      "iteration 25\n",
      "training loss: 0.156891 , training accuracy: [0.95257246]\n",
      "testing loss 0.15725 test accuracy is [0.95122439]\n",
      "iteration 45\n",
      "training loss: 0.15682 , training accuracy: [0.9526521]\n",
      "testing loss 0.157179 test accuracy is [0.9510237]\n",
      "iteration 65\n",
      "training loss: 0.156965 , training accuracy: [0.95237339]\n",
      "testing loss 0.157648 test accuracy is [0.95082295]\n",
      "iteration 85\n",
      "training loss: 0.156963 , training accuracy: [0.95269191]\n",
      "testing loss 0.157447 test accuracy is [0.9510237]\n",
      "iteration 105\n",
      "training loss: 0.157996 , training accuracy: [0.95217425]\n",
      "testing loss 0.157867 test accuracy is [0.95062226]\n",
      "iteration 125\n",
      "training loss: 0.157756 , training accuracy: [0.95261228]\n",
      "testing loss 0.158103 test accuracy is [0.95082295]\n",
      "iteration 145\n",
      "training loss: 0.15742 , training accuracy: [0.95237339]\n",
      "testing loss 0.158424 test accuracy is [0.95042151]\n",
      "iteration 165\n",
      "training loss: 0.156871 , training accuracy: [0.95269191]\n",
      "testing loss 0.157519 test accuracy is [0.95062226]\n",
      "iteration 185\n",
      "training loss: 0.156574 , training accuracy: [0.95305032]\n",
      "testing loss 0.157472 test accuracy is [0.95022082]\n",
      "iteration 205\n",
      "training loss: 0.157713 , training accuracy: [0.95209461]\n",
      "testing loss 0.158145 test accuracy is [0.95082295]\n",
      "iteration 225\n",
      "training loss: 0.156676 , training accuracy: [0.95237339]\n",
      "testing loss 0.157536 test accuracy is [0.95122439]\n",
      "iteration 245\n",
      "training loss: 0.157249 , training accuracy: [0.95193535]\n",
      "testing loss 0.157455 test accuracy is [0.95122439]\n",
      "iteration 265\n",
      "training loss: 0.157138 , training accuracy: [0.95233357]\n",
      "testing loss 0.15757 test accuracy is [0.95042151]\n",
      "iteration 285\n",
      "training loss: 0.156581 , training accuracy: [0.95261228]\n",
      "testing loss 0.156649 test accuracy is [0.95122439]\n",
      "iteration 305\n",
      "training loss: 0.1564 , training accuracy: [0.9526521]\n",
      "testing loss 0.156644 test accuracy is [0.95122439]\n",
      "iteration 325\n",
      "training loss: 0.156646 , training accuracy: [0.95201498]\n",
      "testing loss 0.157252 test accuracy is [0.9510237]\n",
      "iteration 345\n",
      "training loss: 0.157917 , training accuracy: [0.95261228]\n",
      "testing loss 0.159459 test accuracy is [0.95002007]\n",
      "iteration 365\n",
      "training loss: 0.1564 , training accuracy: [0.95305032]\n",
      "testing loss 0.156572 test accuracy is [0.95162582]\n",
      "iteration 385\n",
      "training loss: 0.156781 , training accuracy: [0.95273179]\n",
      "testing loss 0.156349 test accuracy is [0.95182657]\n",
      "iteration 405\n",
      "training loss: 0.15597 , training accuracy: [0.95313001]\n",
      "testing loss 0.156354 test accuracy is [0.95222801]\n",
      "iteration 425\n",
      "training loss: 0.156118 , training accuracy: [0.95269191]\n",
      "testing loss 0.156394 test accuracy is [0.95202732]\n",
      "iteration 445\n",
      "training loss: 0.156003 , training accuracy: [0.95253265]\n",
      "testing loss 0.15668 test accuracy is [0.95222801]\n",
      "iteration 465\n",
      "training loss: 0.156081 , training accuracy: [0.95237339]\n",
      "testing loss 0.157277 test accuracy is [0.95122439]\n",
      "iteration 485\n",
      "training loss: 0.159001 , training accuracy: [0.95149732]\n",
      "testing loss 0.1597 test accuracy is [0.94821358]\n",
      "Epoch:  16\n",
      "iteration 5\n",
      "training loss: 0.1561 , training accuracy: [0.95261228]\n",
      "testing loss 0.157026 test accuracy is [0.95082295]\n",
      "iteration 25\n",
      "training loss: 0.155798 , training accuracy: [0.95285124]\n",
      "testing loss 0.156707 test accuracy is [0.95082295]\n",
      "iteration 45\n",
      "training loss: 0.155857 , training accuracy: [0.9527716]\n",
      "testing loss 0.156378 test accuracy is [0.95122439]\n",
      "iteration 65\n",
      "training loss: 0.155867 , training accuracy: [0.9524132]\n",
      "testing loss 0.156696 test accuracy is [0.9510237]\n",
      "iteration 85\n",
      "training loss: 0.15609 , training accuracy: [0.95229375]\n",
      "testing loss 0.157583 test accuracy is [0.95142514]\n",
      "iteration 105\n",
      "training loss: 0.155902 , training accuracy: [0.95249283]\n",
      "testing loss 0.156598 test accuracy is [0.95122439]\n",
      "iteration 125\n",
      "training loss: 0.15637 , training accuracy: [0.95225388]\n",
      "testing loss 0.156817 test accuracy is [0.95042151]\n",
      "iteration 145\n",
      "training loss: 0.156908 , training accuracy: [0.95181584]\n",
      "testing loss 0.157024 test accuracy is [0.94981933]\n",
      "iteration 165\n",
      "training loss: 0.155547 , training accuracy: [0.95293087]\n",
      "testing loss 0.156481 test accuracy is [0.95142514]\n",
      "iteration 185\n",
      "training loss: 0.156171 , training accuracy: [0.95297068]\n",
      "testing loss 0.158386 test accuracy is [0.94981933]\n",
      "iteration 205\n",
      "training loss: 0.155542 , training accuracy: [0.9530105]\n",
      "testing loss 0.156773 test accuracy is [0.95122439]\n",
      "iteration 225\n",
      "training loss: 0.155315 , training accuracy: [0.95285124]\n",
      "testing loss 0.15641 test accuracy is [0.95082295]\n",
      "iteration 245\n",
      "training loss: 0.156017 , training accuracy: [0.9533689]\n",
      "testing loss 0.157546 test accuracy is [0.95022082]\n",
      "iteration 265\n",
      "training loss: 0.155662 , training accuracy: [0.95320964]\n",
      "testing loss 0.156625 test accuracy is [0.95042151]\n",
      "iteration 285\n",
      "training loss: 0.155196 , training accuracy: [0.95297068]\n",
      "testing loss 0.15652 test accuracy is [0.95162582]\n",
      "iteration 305\n",
      "training loss: 0.155325 , training accuracy: [0.9530105]\n",
      "testing loss 0.15574 test accuracy is [0.95082295]\n",
      "iteration 325\n",
      "training loss: 0.155501 , training accuracy: [0.95313001]\n",
      "testing loss 0.156488 test accuracy is [0.95042151]\n",
      "iteration 345\n",
      "training loss: 0.155984 , training accuracy: [0.95257246]\n",
      "testing loss 0.156861 test accuracy is [0.95002007]\n",
      "iteration 365\n",
      "training loss: 0.155033 , training accuracy: [0.95309013]\n",
      "testing loss 0.156508 test accuracy is [0.95242876]\n",
      "iteration 385\n",
      "training loss: 0.155019 , training accuracy: [0.95297068]\n",
      "testing loss 0.15634 test accuracy is [0.95062226]\n",
      "iteration 405\n",
      "training loss: 0.155332 , training accuracy: [0.95253265]\n",
      "testing loss 0.157304 test accuracy is [0.95122439]\n",
      "iteration 425\n",
      "training loss: 0.154957 , training accuracy: [0.95281142]\n",
      "testing loss 0.156248 test accuracy is [0.95122439]\n",
      "iteration 445\n",
      "training loss: 0.155712 , training accuracy: [0.95289105]\n",
      "testing loss 0.157359 test accuracy is [0.95042151]\n",
      "iteration 465\n",
      "training loss: 0.154718 , training accuracy: [0.95324945]\n",
      "testing loss 0.15633 test accuracy is [0.95162582]\n",
      "iteration 485\n",
      "training loss: 0.154968 , training accuracy: [0.95360786]\n",
      "testing loss 0.155619 test accuracy is [0.95142514]\n",
      "Epoch:  17\n",
      "iteration 5\n",
      "training loss: 0.154627 , training accuracy: [0.9533689]\n",
      "testing loss 0.155918 test accuracy is [0.9510237]\n",
      "iteration 25\n",
      "training loss: 0.154769 , training accuracy: [0.95348835]\n",
      "testing loss 0.156512 test accuracy is [0.9510237]\n",
      "iteration 45\n",
      "training loss: 0.154641 , training accuracy: [0.95368749]\n",
      "testing loss 0.156006 test accuracy is [0.95062226]\n",
      "iteration 65\n",
      "training loss: 0.155524 , training accuracy: [0.9530105]\n",
      "testing loss 0.156304 test accuracy is [0.95082295]\n",
      "iteration 85\n",
      "training loss: 0.154759 , training accuracy: [0.95340872]\n",
      "testing loss 0.156775 test accuracy is [0.95062226]\n",
      "iteration 105\n",
      "training loss: 0.155338 , training accuracy: [0.95328927]\n",
      "testing loss 0.157555 test accuracy is [0.95122439]\n",
      "iteration 125\n",
      "training loss: 0.154734 , training accuracy: [0.95324945]\n",
      "testing loss 0.156955 test accuracy is [0.95142514]\n",
      "iteration 145\n",
      "training loss: 0.154486 , training accuracy: [0.95332909]\n",
      "testing loss 0.156334 test accuracy is [0.95202732]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 165\n",
      "training loss: 0.155124 , training accuracy: [0.95281142]\n",
      "testing loss 0.156893 test accuracy is [0.95162582]\n",
      "iteration 185\n",
      "training loss: 0.155041 , training accuracy: [0.95253265]\n",
      "testing loss 0.15692 test accuracy is [0.95122439]\n",
      "iteration 205\n",
      "training loss: 0.154272 , training accuracy: [0.95376712]\n",
      "testing loss 0.155729 test accuracy is [0.95202732]\n",
      "iteration 225\n",
      "training loss: 0.154483 , training accuracy: [0.95360786]\n",
      "testing loss 0.15523 test accuracy is [0.95082295]\n",
      "iteration 245\n",
      "training loss: 0.154613 , training accuracy: [0.95396626]\n",
      "testing loss 0.155872 test accuracy is [0.95182657]\n",
      "iteration 265\n",
      "training loss: 0.154312 , training accuracy: [0.95376712]\n",
      "testing loss 0.155524 test accuracy is [0.95242876]\n",
      "iteration 285\n",
      "training loss: 0.154485 , training accuracy: [0.95344853]\n",
      "testing loss 0.156178 test accuracy is [0.95082295]\n",
      "iteration 305\n",
      "training loss: 0.154176 , training accuracy: [0.95352817]\n",
      "testing loss 0.155206 test accuracy is [0.95082295]\n",
      "iteration 325\n",
      "training loss: 0.154529 , training accuracy: [0.95344853]\n",
      "testing loss 0.156292 test accuracy is [0.95122439]\n",
      "iteration 345\n",
      "training loss: 0.154023 , training accuracy: [0.95384675]\n",
      "testing loss 0.155463 test accuracy is [0.95162582]\n",
      "iteration 365\n",
      "training loss: 0.153971 , training accuracy: [0.95368749]\n",
      "testing loss 0.156171 test accuracy is [0.95222801]\n",
      "iteration 385\n",
      "training loss: 0.154401 , training accuracy: [0.95313001]\n",
      "testing loss 0.157043 test accuracy is [0.95242876]\n",
      "iteration 405\n",
      "training loss: 0.153794 , training accuracy: [0.95348835]\n",
      "testing loss 0.156226 test accuracy is [0.95202732]\n",
      "iteration 425\n",
      "training loss: 0.153903 , training accuracy: [0.95412552]\n",
      "testing loss 0.156487 test accuracy is [0.95222801]\n",
      "iteration 445\n",
      "training loss: 0.153844 , training accuracy: [0.95396626]\n",
      "testing loss 0.156234 test accuracy is [0.95082295]\n",
      "iteration 465\n",
      "training loss: 0.154304 , training accuracy: [0.95313001]\n",
      "testing loss 0.155919 test accuracy is [0.95042151]\n",
      "iteration 485\n",
      "training loss: 0.153841 , training accuracy: [0.95344853]\n",
      "testing loss 0.155261 test accuracy is [0.95022082]\n",
      "Epoch:  18\n",
      "iteration 5\n",
      "training loss: 0.154356 , training accuracy: [0.95313001]\n",
      "testing loss 0.157286 test accuracy is [0.95122439]\n",
      "iteration 25\n",
      "training loss: 0.153641 , training accuracy: [0.95376712]\n",
      "testing loss 0.156006 test accuracy is [0.9510237]\n",
      "iteration 45\n",
      "training loss: 0.153687 , training accuracy: [0.9537273]\n",
      "testing loss 0.155728 test accuracy is [0.95142514]\n",
      "iteration 65\n",
      "training loss: 0.154051 , training accuracy: [0.95368749]\n",
      "testing loss 0.155133 test accuracy is [0.95122439]\n",
      "iteration 85\n",
      "training loss: 0.153739 , training accuracy: [0.95344853]\n",
      "testing loss 0.155441 test accuracy is [0.95082295]\n",
      "iteration 105\n",
      "training loss: 0.154213 , training accuracy: [0.95305032]\n",
      "testing loss 0.156322 test accuracy is [0.95142514]\n",
      "iteration 125\n",
      "training loss: 0.153468 , training accuracy: [0.95368749]\n",
      "testing loss 0.155443 test accuracy is [0.95162582]\n",
      "iteration 145\n",
      "training loss: 0.153564 , training accuracy: [0.95380694]\n",
      "testing loss 0.155365 test accuracy is [0.95162582]\n",
      "iteration 165\n",
      "training loss: 0.153423 , training accuracy: [0.95376712]\n",
      "testing loss 0.155393 test accuracy is [0.95122439]\n",
      "iteration 185\n",
      "training loss: 0.155372 , training accuracy: [0.95293087]\n",
      "testing loss 0.156342 test accuracy is [0.94961864]\n",
      "iteration 205\n",
      "training loss: 0.153437 , training accuracy: [0.95356804]\n",
      "testing loss 0.155128 test accuracy is [0.9510237]\n",
      "iteration 225\n",
      "training loss: 0.15407 , training accuracy: [0.95348835]\n",
      "testing loss 0.156254 test accuracy is [0.95162582]\n",
      "iteration 245\n",
      "training loss: 0.153606 , training accuracy: [0.95340872]\n",
      "testing loss 0.155378 test accuracy is [0.95022082]\n",
      "iteration 265\n",
      "training loss: 0.153849 , training accuracy: [0.95360786]\n",
      "testing loss 0.156015 test accuracy is [0.95022082]\n",
      "iteration 285\n",
      "training loss: 0.15311 , training accuracy: [0.95368749]\n",
      "testing loss 0.155363 test accuracy is [0.95142514]\n",
      "iteration 305\n",
      "training loss: 0.153002 , training accuracy: [0.95408571]\n",
      "testing loss 0.155584 test accuracy is [0.95182657]\n",
      "iteration 325\n",
      "training loss: 0.153305 , training accuracy: [0.95348835]\n",
      "testing loss 0.155962 test accuracy is [0.95122439]\n",
      "iteration 345\n",
      "training loss: 0.153583 , training accuracy: [0.95305032]\n",
      "testing loss 0.156787 test accuracy is [0.9510237]\n",
      "iteration 365\n",
      "training loss: 0.153324 , training accuracy: [0.95340872]\n",
      "testing loss 0.156589 test accuracy is [0.95182657]\n",
      "iteration 385\n",
      "training loss: 0.153394 , training accuracy: [0.9533689]\n",
      "testing loss 0.155845 test accuracy is [0.95062226]\n",
      "iteration 405\n",
      "training loss: 0.153642 , training accuracy: [0.95356804]\n",
      "testing loss 0.156438 test accuracy is [0.95022082]\n",
      "iteration 425\n",
      "training loss: 0.153041 , training accuracy: [0.95309013]\n",
      "testing loss 0.156191 test accuracy is [0.95042151]\n",
      "iteration 445\n",
      "training loss: 0.153089 , training accuracy: [0.9537273]\n",
      "testing loss 0.155537 test accuracy is [0.95022082]\n",
      "iteration 465\n",
      "training loss: 0.152688 , training accuracy: [0.95376712]\n",
      "testing loss 0.1554 test accuracy is [0.95182657]\n",
      "iteration 485\n",
      "training loss: 0.152679 , training accuracy: [0.95388657]\n",
      "testing loss 0.155143 test accuracy is [0.9510237]\n",
      "Epoch:  19\n",
      "iteration 5\n",
      "training loss: 0.153371 , training accuracy: [0.95328927]\n",
      "testing loss 0.15659 test accuracy is [0.95122439]\n",
      "iteration 25\n",
      "training loss: 0.153383 , training accuracy: [0.95368749]\n",
      "testing loss 0.156543 test accuracy is [0.95142514]\n",
      "iteration 45\n",
      "training loss: 0.154023 , training accuracy: [0.95332909]\n",
      "testing loss 0.157065 test accuracy is [0.95042151]\n",
      "iteration 65\n",
      "training loss: 0.153839 , training accuracy: [0.95285124]\n",
      "testing loss 0.157062 test accuracy is [0.95122439]\n",
      "iteration 85\n",
      "training loss: 0.153816 , training accuracy: [0.9530105]\n",
      "testing loss 0.157293 test accuracy is [0.95082295]\n",
      "iteration 105\n",
      "training loss: 0.15289 , training accuracy: [0.95360786]\n",
      "testing loss 0.155084 test accuracy is [0.95082295]\n",
      "iteration 125\n",
      "training loss: 0.152843 , training accuracy: [0.95340872]\n",
      "testing loss 0.155366 test accuracy is [0.95122439]\n",
      "iteration 145\n",
      "training loss: 0.152381 , training accuracy: [0.95368749]\n",
      "testing loss 0.155196 test accuracy is [0.95142514]\n",
      "iteration 165\n",
      "training loss: 0.152712 , training accuracy: [0.95368749]\n",
      "testing loss 0.155863 test accuracy is [0.95062226]\n",
      "iteration 185\n",
      "training loss: 0.154288 , training accuracy: [0.95261228]\n",
      "testing loss 0.155928 test accuracy is [0.95002007]\n",
      "iteration 205\n",
      "training loss: 0.152844 , training accuracy: [0.95332909]\n",
      "testing loss 0.154974 test accuracy is [0.95122439]\n",
      "iteration 225\n",
      "training loss: 0.152684 , training accuracy: [0.95368749]\n",
      "testing loss 0.155357 test accuracy is [0.95082295]\n",
      "iteration 245\n",
      "training loss: 0.152745 , training accuracy: [0.95396626]\n",
      "testing loss 0.154867 test accuracy is [0.95202732]\n",
      "iteration 265\n",
      "training loss: 0.152982 , training accuracy: [0.95436442]\n",
      "testing loss 0.155595 test accuracy is [0.95162582]\n",
      "iteration 285\n",
      "training loss: 0.15238 , training accuracy: [0.95444411]\n",
      "testing loss 0.155059 test accuracy is [0.95142514]\n",
      "iteration 305\n",
      "training loss: 0.152204 , training accuracy: [0.95380694]\n",
      "testing loss 0.154488 test accuracy is [0.95162582]\n",
      "iteration 325\n",
      "training loss: 0.153222 , training accuracy: [0.95332909]\n",
      "testing loss 0.155256 test accuracy is [0.95002007]\n",
      "iteration 345\n",
      "training loss: 0.152705 , training accuracy: [0.95440429]\n",
      "testing loss 0.155757 test accuracy is [0.9528302]\n",
      "iteration 365\n",
      "training loss: 0.152119 , training accuracy: [0.95456356]\n",
      "testing loss 0.154685 test accuracy is [0.95242876]\n",
      "iteration 385\n",
      "training loss: 0.152474 , training accuracy: [0.95408571]\n",
      "testing loss 0.15489 test accuracy is [0.9528302]\n",
      "iteration 405\n",
      "training loss: 0.152319 , training accuracy: [0.95412552]\n",
      "testing loss 0.154333 test accuracy is [0.95222801]\n",
      "iteration 425\n",
      "training loss: 0.152678 , training accuracy: [0.95324945]\n",
      "testing loss 0.154532 test accuracy is [0.95142514]\n",
      "iteration 445\n",
      "training loss: 0.151746 , training accuracy: [0.95448393]\n",
      "testing loss 0.154215 test accuracy is [0.95162582]\n",
      "iteration 465\n",
      "training loss: 0.151933 , training accuracy: [0.95388657]\n",
      "testing loss 0.154166 test accuracy is [0.95062226]\n",
      "iteration 485\n",
      "training loss: 0.151673 , training accuracy: [0.95428479]\n",
      "testing loss 0.154285 test accuracy is [0.9510237]\n",
      "Epoch:  20\n",
      "iteration 5\n",
      "training loss: 0.152094 , training accuracy: [0.9543246]\n",
      "testing loss 0.154772 test accuracy is [0.95202732]\n",
      "iteration 25\n",
      "training loss: 0.151818 , training accuracy: [0.95404589]\n",
      "testing loss 0.154445 test accuracy is [0.95142514]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 45\n",
      "training loss: 0.152518 , training accuracy: [0.95388657]\n",
      "testing loss 0.154726 test accuracy is [0.9510237]\n",
      "iteration 65\n",
      "training loss: 0.1519 , training accuracy: [0.95408571]\n",
      "testing loss 0.154333 test accuracy is [0.95122439]\n",
      "iteration 85\n",
      "training loss: 0.151899 , training accuracy: [0.95404589]\n",
      "testing loss 0.154753 test accuracy is [0.95022082]\n",
      "iteration 105\n",
      "training loss: 0.152787 , training accuracy: [0.95364767]\n",
      "testing loss 0.155066 test accuracy is [0.9510237]\n",
      "iteration 125\n",
      "training loss: 0.151789 , training accuracy: [0.95424497]\n",
      "testing loss 0.154794 test accuracy is [0.95162582]\n",
      "iteration 145\n",
      "training loss: 0.152117 , training accuracy: [0.95364767]\n",
      "testing loss 0.155145 test accuracy is [0.95222801]\n",
      "iteration 165\n",
      "training loss: 0.151599 , training accuracy: [0.95416534]\n",
      "testing loss 0.154762 test accuracy is [0.95182657]\n",
      "iteration 185\n",
      "training loss: 0.15156 , training accuracy: [0.95436442]\n",
      "testing loss 0.154818 test accuracy is [0.95182657]\n",
      "iteration 205\n",
      "training loss: 0.15333 , training accuracy: [0.95348835]\n",
      "testing loss 0.15801 test accuracy is [0.95082295]\n",
      "iteration 225\n",
      "training loss: 0.151927 , training accuracy: [0.95440429]\n",
      "testing loss 0.155779 test accuracy is [0.95222801]\n",
      "iteration 245\n",
      "training loss: 0.151725 , training accuracy: [0.95428479]\n",
      "testing loss 0.155286 test accuracy is [0.95242876]\n",
      "iteration 265\n",
      "training loss: 0.151721 , training accuracy: [0.95376712]\n",
      "testing loss 0.154218 test accuracy is [0.95022082]\n",
      "iteration 285\n",
      "training loss: 0.151547 , training accuracy: [0.95404589]\n",
      "testing loss 0.153931 test accuracy is [0.95162582]\n",
      "iteration 305\n",
      "training loss: 0.151156 , training accuracy: [0.95456356]\n",
      "testing loss 0.15396 test accuracy is [0.95242876]\n",
      "iteration 325\n",
      "training loss: 0.151153 , training accuracy: [0.95468301]\n",
      "testing loss 0.154256 test accuracy is [0.95182657]\n",
      "iteration 345\n",
      "training loss: 0.151626 , training accuracy: [0.95428479]\n",
      "testing loss 0.155203 test accuracy is [0.95222801]\n",
      "iteration 365\n",
      "training loss: 0.151546 , training accuracy: [0.95452374]\n",
      "testing loss 0.155052 test accuracy is [0.95182657]\n",
      "iteration 385\n",
      "training loss: 0.151237 , training accuracy: [0.95416534]\n",
      "testing loss 0.154022 test accuracy is [0.95222801]\n",
      "iteration 405\n",
      "training loss: 0.153022 , training accuracy: [0.9533689]\n",
      "testing loss 0.154974 test accuracy is [0.95042151]\n",
      "iteration 425\n",
      "training loss: 0.151363 , training accuracy: [0.95416534]\n",
      "testing loss 0.153536 test accuracy is [0.95122439]\n",
      "iteration 445\n",
      "training loss: 0.151009 , training accuracy: [0.95464319]\n",
      "testing loss 0.153976 test accuracy is [0.95162582]\n",
      "iteration 465\n",
      "training loss: 0.151322 , training accuracy: [0.95452374]\n",
      "testing loss 0.154407 test accuracy is [0.95122439]\n",
      "iteration 485\n",
      "training loss: 0.151322 , training accuracy: [0.9537273]\n",
      "testing loss 0.154877 test accuracy is [0.95242876]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for s in np.arange(learning_epoch):   \n",
    "    \n",
    "    # compute the initial training, testing loss and accuracy\n",
    "    if s == 0:\n",
    "        trl, tel = sess.run([train_loss, test_loss])\n",
    "        tr_acc = sess.run([acc], feed_dict={x: train_data, y: train_label})\n",
    "        te_acc = sess.run([acc], feed_dict={x: test_data, y: test_label})\n",
    "        print('iteration', s)\n",
    "        print('training loss:', trl, ', training accuracy:', tr_acc)\n",
    "        print('testing loss', tel, 'test accuracy is', te_acc)\n",
    "        \n",
    "        track_train_loss = np.hstack((track_train_loss, trl))\n",
    "        track_test_loss = np.hstack((track_test_loss, tel))\n",
    "    \n",
    "    # decay the learning rate every epoch\n",
    "    print('Epoch: ', s + 1)\n",
    "    learning_rate *= 0.8\n",
    "    \n",
    "    re_order = np.random.permutation(hm_train_data)\n",
    "    for i in np.arange(int(np.floor(hm_train_data / batch_size))):\n",
    "            batch_x = train_data[re_order[i * batch_size:(i + 1) * batch_size], :]\n",
    "            batch_y = train_label[re_order[i * batch_size:(i + 1) * batch_size], :]\n",
    "            \n",
    "            # stochastic gradient descent method\n",
    "            w = sess.run([weights], feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "            if i % eval_interval == 5:\n",
    "                trl, tel = sess.run([train_loss, test_loss])\n",
    "                tr_acc = sess.run([acc], feed_dict={x: train_data, y: train_label})\n",
    "                te_acc = sess.run([acc], feed_dict={x: test_data, y: test_label})\n",
    "                print('iteration', i)\n",
    "                print('training loss:', trl, ', training accuracy:', tr_acc)\n",
    "                print('testing loss', tel, 'test accuracy is', te_acc)\n",
    "        \n",
    "                track_train_loss = np.hstack((track_train_loss, trl))\n",
    "                track_test_loss = np.hstack((track_test_loss, tel))\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the weights as an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAACQEv4/G4Au0a9xNTkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14006add8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "plt.subplot(231)\n",
    "img = w[0][0:28*28, 0].reshape(28,28)\n",
    "plt.imshow(img)\n",
    "plt.title('Digit 1')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(232)\n",
    "img = w[0][0:28*28, 1].reshape(28,28)\n",
    "plt.imshow(img)\n",
    "plt.title('Digit 2')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(233)\n",
    "img = w[0][0:28*28, 2].reshape(28,28)\n",
    "plt.imshow(img)\n",
    "plt.title('Digit 3')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(234)\n",
    "img = w[0][0:28*28, 3].reshape(28,28)\n",
    "plt.imshow(img)\n",
    "plt.title('Digit 4')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(235)\n",
    "img = w[0][0:28*28, 4].reshape(28,28)\n",
    "plt.imshow(img)\n",
    "plt.title('Digit 5')\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the final classification accuracy for each digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit 1 testing accuracy: 0.9859022556390977\n",
      "Digit 2 testing accuracy: 0.9444444444444444\n",
      "Digit 3 testing accuracy: 0.9378640776699029\n",
      "Digit 4 testing accuracy: 0.9735503560528993\n",
      "Digit 5 testing accuracy: 0.9114754098360656\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(5):\n",
    "    rows, _ = np.nonzero(test_label[:, i][:, None])\n",
    "    exp_terms = np.exp(np.dot(test_data[rows, :], w[0]))\n",
    "    prob = exp_terms / np.sum(exp_terms, axis=1, keepdims=True)\n",
    "    max_ind = np.argmax(prob, axis=1)\n",
    "    one_hot_max = np.zeros([np.size(rows), hm_classes])\n",
    "    for j in np.arange(np.size(rows)):\n",
    "        one_hot_max[j, int(max_ind[j])] = 1\n",
    "        \n",
    "    error_rate = np.count_nonzero(one_hot_max - test_label[rows, :]) / (2 * np.size(rows))\n",
    "    print('Digit', i + 1, 'testing accuracy:', 1 - error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Plot the learning curves w.r.t. the training and testing loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XmcXHWZ7/HPc2rrvbMDSYAEZEtCCBCWCGhwgQAKil52\nHRgBxXF0rgMDeBXGmTsjXrwMMooMm3hBQYZFUaIiCgLKkhCQJYQQQghZoDt7713Lc/84p5ui6aXS\n6erqdH3fr1e9uuqcX53z/ApST/2W8zvm7oiIiAAEpQ5ARERGDiUFERHppqQgIiLdlBRERKSbkoKI\niHRTUhARkW5KCiJDyMyOMbNXSxzDN8zs5lLGIDsv03UKUmpmtgo4390fLnUsQ63YdTOz+cAd7j61\nGMeX8qOWgsgIZSH9G5Vhpf/hZEQzswvMbIWZbTKzB8xscrTdzOw/zKzBzLaZ2YtmNivad6KZLTWz\nJjNba2YX93LclJlt6XpPtG2imbWZ2SQzm2Bmv47KbDKzxwv5gjaz+Wa2Jnp+O7AH8Cszazazf4q2\nH2lmf4mO/dfo137X+x81s38zsz8DrcBeZnaemb0S1WelmX0xKlsN/AaYHB2/2cwmm9k/m9kdecc8\n2cxejs73qJkdkLdvlZldbGYvmNlWM/u5mVVs138kGVWUFGTEMrOPAN8BTgN2A94E7op2Hwd8CNgX\nqI/KbIz23QJ80d1rgVnAH3se2907gPuAM/M2nwb8yd0bgH8E1gATgV2AbwDb1dfq7p8DVgOfdPca\nd/8/ZjYFeBD438A44GLgXjObmPfWzwEXArVRnRuATwB1wHnAf5jZIe7eApwArIuOX+Pu6/JjMLN9\ngTuBf4jqspAwSSV71HsBMB2YDZy7PfWU0UVJQUays4Fb3X1J9CV+OTDPzKYBacIvzf0Jx8Zecff1\n0fvSwAwzq3P3ze6+pI/j/ww4I+/1WdG2rmPsBuzp7ml3f9yHZgDuHGChuy9095y7/x5YDJyYV+Y2\nd3/Z3TPRuR9099c99CfgIeCYAs93OvCgu//e3dPA94BK4IN5Za5z93Xuvgn4FTBnRyspOy8lBRnJ\nJhP+UgbA3ZsJWwNT3P2PwA+AHwINZnajmdVFRT9D+CX7ppn9yczm9XH8R4AqMzsiSjRzgPujfVcD\nK4CHoi6by4aoTnsC/yPqytliZluAowkTUJe38t9gZieY2VNRN9aWqG4TCjxfz88wFx1/Sl6Zt/Oe\ntwI1BddGRh0lBRnJ1hF+iQLdfejjgbUA7n6dux8KzCDsRrok2r7I3U8BJgG/AO7u7eDuno32nRk9\nfu3uTdG+Jnf/R3ffCzgZ+LqZfXQQdejZungLuN3dx+Q9qt39qt7eY2Yp4F7CX/i7uPsYwi4g6+P4\nPfX8DA3YnegzFOlJSUFGioSZVeQ94oR94eeZ2Zzoy/HfgafdfZWZHRb9wk8ALUA7kDOzpJmdbWb1\nUXfJNiDXz3l/RtjFcjbvdh1hZp8wsw9EX6JbgewAx+nLO8Beea/vAD5pZsebWSyq63wz62tKaRJI\nAY1AxsxOIBxPyT/+eDOr7+P9dwMnmdlHo8/qH4EO4C+DqIuUASUFGSkWAm15j3+O5vZ/i/CX8npg\nb94dA6gDbgI2E3aPbCTs8oFwoHaVmW0DvkT4hd8rd3+aMKlMJpzJ02Uf4GGgGXgSuN7dHwEws9+Y\n2TcKrNd3gG9GXUUXu/tbwCmEA9eNhC2HS+jj32LUcvkq4Zf7ZsJxjwfy9i8jTJ4ro3NM7vH+VwnH\nMf4T2AB8knDgu7PA+KXM6OI1ERHpppaCiIh0U1IQEZFuSgoiItJNSUFERLrFSx3A9powYYJPmzat\n1GGIiOxUnn322Q3uPnGgcjtdUpg2bRqLFy8udRgiIjsVM3tz4FLqPhIRkTxKCiIi0k1JQUREuu10\nYwoiMrKl02nWrFlDe3t7qUMpSxUVFUydOpVEIjGo9yspiMiQWrNmDbW1tUybNo1wPUEZLu7Oxo0b\nWbNmDdOnTx/UMdR9JCJDqr29nfHjxyshlICZMX78+B1qpRUtKZjZrdH9c1/qp8x8M3s+un/sn4oV\ni4gMLyWE0tnRz76YLYXbCO/72iszGwNcD5zs7jOB/1HEWHhj6SKevPnrbGrQvUVERPpStKTg7o8B\nm/opchZwn7uvjso3FCsWgE2rXmLemlvYtnH9wIVFZKe0ZcsWrr/++kG998QTT2TLli39lrniiit4\n+OGHB3X8nqZNm8aGDRuG5FhDqZRjCvsCY83sUTN71sw+X8yTBbGwqtlMppinEZES6i8pZAb4t79w\n4ULGjBnTb5l/+Zd/4WMf+9ig49sZlDIpxIFDgZOA44Fvmdm+vRU0swvNbLGZLW5sbBzUySwIp2fl\nskoKIqPVZZddxuuvv86cOXO45JJLePTRRznmmGM4+eSTmTFjBgCf+tSnOPTQQ5k5cyY33nhj93u7\nfrmvWrWKAw44gAsuuICZM2dy3HHH0dbWBsC5557LPffc013+yiuv5JBDDuHAAw9k2bJlADQ2NvLx\nj3+cmTNncv7557PnnnsO2CK45pprmDVrFrNmzeLaa68FoKWlhZNOOomDDjqIWbNm8fOf/7y7jjNm\nzGD27NlcfPHFQ/sBUtopqWuAje7eArSY2WPAQcDyngXd/UbgRoC5c+cO6lZxQSwWHiunpCAyXL79\nq5dZum7bkB5zxuQ6rvzkzF73XXXVVbz00ks8//zzADz66KMsWbKEl156qXuK5q233sq4ceNoa2vj\nsMMO4zOf+Qzjx49/z3Fee+017rzzTm666SZOO+007r33Xs4555z3nW/ChAksWbKE66+/nu9973vc\nfPPNfPvb3+YjH/kIl19+Ob/97W+55ZZb+q3Ps88+y49//GOefvpp3J0jjjiCD3/4w6xcuZLJkyfz\n4IMPArB161Y2btzI/fffz7JlyzCzAbu7BqOULYVfAkebWdzMqoAjgFeKdrYgTApqKYiUl8MPP/w9\nc/avu+46DjroII488kjeeustXnvttfe9Z/r06cyZMweAQw89lFWrVvV67FNPPfV9ZZ544gnOOCO8\nlfiCBQsYO3Zsv/E98cQTfPrTn6a6upqamhpOPfVUHn/8cQ488EB+//vfc+mll/L4449TX19PfX09\nFRUVfOELX+C+++6jqqpqez+OARWtpWBmdwLzgQlmtga4EkgAuPsN7v6Kmf0WeAHIATe7e5/TV3dU\nEFP3kchw6+sX/XCqrq7ufv7oo4/y8MMP8+STT1JVVcX8+fN7ndOfSqW6n8dise7uo77KxWKxAccs\ntte+++7LkiVLWLhwId/85jf56Ec/yhVXXMEzzzzDH/7wB+655x5+8IMf8Mc//nFIz1u0pODuZxZQ\n5mrg6mLFkM/UUhAZ9Wpra2lqaupz/9atWxk7dixVVVUsW7aMp556ashjOOqoo7j77ru59NJLeeih\nh9i8eXO/5Y855hjOPfdcLrvsMtyd+++/n9tvv51169Yxbtw4zjnnHMaMGcPNN99Mc3Mzra2tnHji\niRx11FHstddeQx5/2SxzEcTDqnouW+JIRKRYxo8fz1FHHcWsWbM44YQTOOmkk96zf8GCBdxwww0c\ncMAB7Lfffhx55JFDHsOVV17JmWeeye233868efPYddddqa2t7bP8IYccwrnnnsvhhx8OwPnnn8/B\nBx/M7373Oy655BKCICCRSPCjH/2IpqYmTjnlFNrb23F3rrnmmiGP39wHNW5bMnPnzvXB3GTn1UUP\ns9+Dn+Gv82/loPmfKUJkIgLwyiuvcMABB5Q6jJLp6OggFosRj8d58sknueiii7oHvodLb/8NzOxZ\nd5870HvLpqVgsailoO4jESmi1atXc9ppp5HL5Ugmk9x0002lDmm7lE1SCAJNSRWR4ttnn3147rnn\nSh3GoJXNKqlBPJx95FmNKYiI9KVskkJMLQURkQGVTVIIYpp9JCIykPJJCtGUVF2nICLSt/JJCmop\niIx6O7J0NsC1115La2tr9+tCltMuxKpVq5g1a9YOH2c4lF1SQElBZNQa6qRQyHLao03ZJIVYd0tB\n3Ucio1XPpbMBrr76ag477DBmz57NlVdeCfS+LPV1113HunXrOPbYYzn22GOBwpbTXrRoEbNnz+4+\n50Atgvb2ds477zwOPPBADj74YB555BEAXn75ZQ4//HDmzJnD7Nmzee211/pcPruYyuY6hVg0JVUt\nBZFh9JvL4O0Xh/aYux4IJ1zV666eS2c/9NBDvPbaazzzzDO4OyeffDKPPfYYjY2N71uWur6+nmuu\nuYZHHnmECRMmvO/YfS2nfd5553HTTTcxb948LrvssgHD/+EPf4iZ8eKLL7Js2TKOO+44li9fzg03\n3MDXvvY1zj77bDo7O8lmsyxcuPB9cRZb+bQUAl3RLFJuHnroIR566CEOPvhgDjnkEJYtW8Zrr73W\n67LUA+ltOe0tW7bQ1NTEvHnzADjrrLMGPM4TTzzRfW+G/fffnz333JPly5czb948/v3f/53vfve7\nvPnmm1RWVg4qzh1VNi2FIBG1FFwtBZFh08cv+uHi7lx++eV88YtffN++3pal7k+hy2kP1llnncUR\nRxzBgw8+yIknnsh//dd/8ZGPfGS749xRZdRSiC5eU0tBZNTquXT28ccfz6233kpzczMAa9eupaGh\ngXXr1lFVVcU555zDJZdcwpIlS3p9/0DGjBlDbW0tTz/9NAB33XXXgO855phj+OlPfwrA8uXLWb16\nNfvttx8rV65kr7324qtf/SqnnHIKL7zwQp9xFlPZtBS6xxTUUhAZtXounX311VfzyiuvdHfv1NTU\ncMcdd7BixYr3LUsNcOGFF7JgwQImT57cPQA8kFtuuYULLriAIAj48Ic/PGAXz5e//GUuuugiDjzw\nQOLxOLfddhupVIq7776b22+/nUQiwa677so3vvENFi1a1GucxVQ2S2dn0x3E/m0Sf9nzIj54Xmmb\ntCKjWbktnd3c3ExNTQ0QDnSvX7+e73//+yWNSUtnF6BrlVTNPhKRofTggw/yne98h0wmw5577slt\nt91W6pB2SDHv0Xwr8Amgwd37nLhrZocBTwJnuPs9RYunOyloTEFEhs7pp5/O6aefXuowhkwxB5pv\nAxb0V8DMYsB3gYeKGEfXych4gGtMQaTodrZu6dFkRz/7oiUFd38M2DRAsb8H7gUaihVHviwxLJcb\njlOJlK2Kigo2btyoxFAC7s7GjRupqKgY9DFKNqZgZlOATwPHAocNxzmzBODqPhIppqlTp7JmzRoa\nGxtLHUpZqqioYOrUqYN+fykHmq8FLnX3nJn1W9DMLgQuBNhjjz0GfcKsBRpoFimyRCLB9OnTSx2G\nDFIpk8Jc4K4oIUwATjSzjLv/omdBd78RuBHCKamDPWGOGKakICLSp5IlBXfv/ilhZrcBv+4tIQyl\nsPtISUFEpC/FnJJ6JzAfmGBma4ArgQSAu99QrPP2J0eAKSmIiPSpaEnB3c/cjrLnFiuOfOo+EhHp\nX9ksiAeQM7UURET6U15JgZjGFERE+lFWSSFrMbUURET6UVZJwTXQLCLSr7JKCmFLQctciIj0payS\ngltAoGUuRET6VFZJIYdaCiIi/SmvpGAxTC0FEZE+lWFSUEtBRKQvZZUUnIBAs49ERPpUXknBYgRq\nKYiI9KmskkLOAgLUUhAR6UtZJQUnwFBLQUSkL+WVFCyG6b6xIiJ9KrOkYGopiIj0o6ySAgRqKYiI\n9KOskoJaCiIi/SurpIAFGGopiIj0paySgi5eExHpX9GSgpndamYNZvZSH/vPNrMXzOxFM/uLmR1U\nrFi6uFoKIiL9KmZL4TZgQT/73wA+7O4HAv8K3FjEWEIWEGhMQUSkT/FiHdjdHzOzaf3s/0vey6eA\nqcWKpfucaimIiPRrpIwpfAH4TV87zexCM1tsZosbGxsHfxbTlFQRkf6UPCmY2bGESeHSvsq4+43u\nPtfd506cOHFHzqa1j0RE+lG07qNCmNls4GbgBHffWOzz5Sym7iMRkX6UrKVgZnsA9wGfc/flw3PO\ngEBJQUSkT0VrKZjZncB8YIKZrQGuBBIA7n4DcAUwHrjezAAy7j63WPFA10CzZh+JiPSlmLOPzhxg\n//nA+cU6f6/UUhAR6VfJB5qHlZnu0Swi0o+ySgpuATF1H4mI9KmskgKafSQi0q8ySwqmMQURkX6U\nWVLQ7CMRkf6UXVJQS0FEpG8FTUk1synAnvnl3f2xYgVVNFolVUSkXwMmBTP7LnA6sBS6Fw5yYCdN\nCo67E10wJyIieQppKXwK2M/dO4odTLG5xQjI4Q7KCSIi71fImMJKouUpdnZmATFzslo+W0SkV4W0\nFFqB583sD0B3a8Hdv1q0qIrFwhyYy+UgVl5j7CIihSgkKTwQPXZ+UVLwnAabRUR6M2BScPefmFkS\n2Dfa9Kq7p4sbVpFEAwm5XAZIljYWEZERqJDZR/OBnwCrAAN2N7O/2TmnpMYAyGZ19zURkd4U0n30\nf4Hj3P1VADPbF7gTOLSYgRVFkDemICIi71PIaGuiKyEARHdJ2ylnI1n3mIJaCiIivSmkpbDYzG4G\n7ohenw0sLl5IRdQ9+0hTUkVEelNIUrgI+Dugawrq48D1RYuomLqSQjZT4kBEREamQmYfdQDXRI+C\nmdmtwCeABnef1ct+A74PnEh4LcS57r5ke86x3dR9JCLSrz7HFMzs7ujvi2b2Qs9HAce+DVjQz/4T\ngH2ix4XAjwoPe3AsCGcfaaBZRKR3/bUUvhb9/cRgDuzuj5nZtH6KnAL8P3d34CkzG2Nmu7n7+sGc\nryCBLl4TEelPny2FvC/nL7v7m/kP4MtDcO4pwFt5r9dE297HzC40s8VmtrixsXHwZ+waU3B1H4mI\n9KaQKakf72XbCUMdSH/c/UZ3n+vucydOnDjo43RPSc2qpSAi0ps+u4/M7CLCFsHePcYQaoG/DMG5\n1wK7572eGm0rGuu+eE2zj0REetPfmMLPgN8A3wEuy9ve5O6bhuDcDwBfMbO7gCOArUUdT4DuZS40\npiAi0rs+k4K7bwW2mtn3gU3u3gRgZnVmdoS7P93fgc3sTmA+MMHM1gBXEl0J7e43AAsJp6OuIJyS\net6OV2cAWuZCRKRfhVy89iPgkLzXzb1sex93P3OA/U54Udyw6R5T0ECziEivChlotugLHAB3z1FY\nMhlxzNRSEBHpT0G34zSzr5pZInp8jfAWnTufQGMKIiL9KSQpfAn4IOHMoDWEg8IXFjOoYrHoJjuu\ntY9ERHpVyNpHDcAZwxBL0VkQVjfsARMRkZ4KufPaROACYFp+eXf/2+KFVRym2UciIv0qZMD4l4TL\nZT8M7NzTdjT7SESkX4UkhSp3v7TokQyDrtlHqKUgItKrQgaaf21mJxY9kmHQvXR2Vi0FEZHeFJIU\nvkaYGNrMbJuZNZnZtmIHVgxdYwoaaBYR6V0hs49qhyOQ4WDd1ymopSAi0ptCZh99qLft7v7Y0IdT\nXO8uc+EDlBQRKU+FDDRfkve8AjgceBb4SFEiKqLu7iO1FEREelVI99En81+b2e7AtUWLqJhMt+MU\nEelPIQPNPa0BDhjqQIZDEI0poJaCiEivChlT+E+gqxM+AOYAS4oZVLFo9pGISP8KGVNYnPc8A9zp\n7n8uUjxF1T37SElBRKRX/d2j+Q/u/lFgxqi5orlr7SNdvCYi0qv+Wgq7mdkHgZOj+yhb/k533+m6\nkIIoKaCWgohIr/pLClcA3wKmAtf02OcUMCXVzBYA3wdiwM3uflWP/fXAHcAeUSzfc/cfFxz9djLd\nZEdEpF99JgV3vwe4x8y+5e7/ur0HNrMY8EPg44QzlhaZ2QPuvjSv2N8BS939k9ES3a+a2U/dvXN7\nz1dYTF0tBXUfiYj0ZsApqYNJCJHDgRXuvjL6kr8LOKXn4YFaC2+JVgNsIhzMLgoNNIuI9G8w1ykU\nagrwVt7rNdG2fD8gvOZhHfAi8DXv5RvbzC40s8VmtrixsXHQAQXdSUEtBRGR3hQzKRTieOB5YDLh\n9Q8/MLO6noXc/UZ3n+vucydOnDjok727zIXWPhIR6c2AScHM9jazVPR8vpl91czGFHDstcDuea+n\nRtvynQfc56EVwBvA/oWFvv3evcmOWgoiIr0ppKVwL5A1sw8ANxJ+0f+sgPctAvYxs+lmlgTOAB7o\nUWY18FEAM9sF2A9YWWDs2y2IRctcqPtIRKRXhVzRnHP3jJl9GvhPd/9PM3tuoDdF7/kK8DvCKam3\nuvvLZvalaP8NwL8Ct5nZi4TXQVzq7hsGXZsBxONhdTNZDTSLiPSmkKSQNrMzgb8BulZMTRRycHdf\nCCzsse2GvOfrgOMKC3XHJWJhdXVFs4hI7wrpPjoPmAf8m7u/YWbTgduLG1ZxJBLRPZo1piAi0qtC\n7qewFPgqgJmNBWrd/bvFDqwYurqPcpl0iSMRERmZCpl99KiZ1ZnZOMIls28ys57LXuwU4okKADyr\npCAi0ptCuo/q3X0bcCrw/9z9COBjxQ2rOCyeDJ8oKYiI9KqQpBA3s92A04BfFzme4oqFScGzRVla\nSURkp1dIUvgXwmmlr7v7IjPbC3ituGEVSZQUTElBRKRXhQw0/zfw33mvVwKfKWZQRRPEyBKo+0hE\npA+FDDRPNbP7zawhetxrZlOHI7hiyBDHcmopiIj0ppDuox8TLk8xOXr8Ktq2U0oTV/eRiEgfCkkK\nE939x+6eiR63AYNfqrTEMpbAcuo+EhHpTSFJYaOZnWNmsehxDrCx2IEVS8biBEoKIiK9KiQp/C3h\ndNS3gfXAZ4FzixhTUWXVUhAR6VMht+N8091PdveJ7j7J3T/Fzjr7CLUURET6M9g7r319SKMYRllL\nEHMlBRGR3gw2KdiQRjGMcpZQS0FEpA+DTQo77U2Os4FaCiIifenzimYza6L3L38DKosWUZHlLEEs\nlyl1GCIiI1KfScHda4czkOGSCxLEvbXUYYiIjEiD7T4qiJktMLNXzWyFmV3WR5n5Zva8mb1sZn8q\nZjwQJoWYq6UgItKbQu7RPChmFgN+CHwcWAMsMrMHoju5dZUZA1wPLHD31WY2qVjxdPEgQQKNKYiI\n9KaYLYXDgRXuvtLdO4G7gFN6lDkLuM/dVwO4e0MR4wHAY0niaimIiPSqmElhCvBW3us10bZ8+wJj\no1t+Pmtmn+/tQGZ2oZktNrPFjY2NOxSUBwniKCmIiPSmqGMKBYgDhwInAccD3zKzfXsWcvcb3X2u\nu8+dOHHH1uLzWIoEGbK5nXZWrYhI0RQzKawFds97PTXalm8N8Dt3b3H3DcBjwEFFjIkgkSRBhpZO\ntRZERHoqZlJYBOxjZtPNLAmcQXhfhny/BI42s7iZVQFHAK8UMSZi8SgpdCgpiIj0VLTZR+6eMbOv\nEN7fOQbc6u4vm9mXov03uPsrZvZb4AUgB9zs7i8VKyaAWCJFigwNSgoiIu9TtKQA4O4LgYU9tt3Q\n4/XVwNXFjCNfPJkiZWma25UURER6KvVA87CLpWoAaG9tKnEkIiIjT9klhXhlHQDtLdtKHImIyMhT\ndkkhURku6dTZurXEkYiIjDzllxSqoqTQ1lziSERERp6ySwoV1WH3UbZNYwoiIj2VXVJIVdUDkG1X\nUhAR6anskoJFs4+yHeo+EhHpqeySAslqAFwtBRGR9ynDpBC1FNrVUhAR6alsk0KuQy0FEZGeyi8p\nxJOkLQGdLaWORERkxCm/pACkg0qCtLqPRER6Ksuk0JmopTLbRDqbK3UoIiIjSlkmhXRqHONoYnNL\nZ6lDEREZUcoyKXjlOMZZExuVFERE3qMsk0JQM5Gx1sTb29pLHYqIyIhS1JvsjFSV9ZOoYRtrN7WW\nOhQRkRGlLFsKlWN2ocLSNGzaXOpQRERGlKImBTNbYGavmtkKM7usn3KHmVnGzD5bzHi6BNXjAdi6\ncf1wnE5EZKdRtKRgZjHgh8AJwAzgTDOb0Ue57wIPFSuW96meAEDb5reH7ZQiIjuDYrYUDgdWuPtK\nd+8E7gJO6aXc3wP3Ag1FjOW96qcCkNn8Fu4+bKcVERnpipkUpgBv5b1eE23rZmZTgE8DP+rvQGZ2\noZktNrPFjY2NOx5Z/e4AjE+/zZrNbTt+PBGRUaLUA83XApe6e7+XFrv7je4+193nTpw4ccfPWjmG\nTLKOqdbI0vXbdvx4IiKjRDGTwlpg97zXU6Nt+eYCd5nZKuCzwPVm9qkixtQtGLsne1gjS9cpKYiI\ndClmUlgE7GNm080sCZwBPJBfwN2nu/s0d58G3AN82d1/UcSYugXjprF3opFX1FIQEelWtKTg7hng\nK8DvgFeAu939ZTP7kpl9qVjnLdjE/ZmSW8+KdRtKHYmIyIhR1Cua3X0hsLDHthv6KHtuMWN5n0kz\niJGjcuvrNDS1M6m2YlhPLyIyEpV6oLl0dpkJwH62mseWq7UgIgLlnBTGfwBP1vLBijd45NXhu0RC\nRGQkK9+kEMSwPY7g6MRyHl/eSEY33BERKeOkALDHPHbteIOgfTNPrdxU6mhEREquvJPCnkcB8NGq\n17n5iZUlDkZEpPTKOylMOQRiKc7ZbS2PvqprFkREyjspxFMw7Shmt/yZqmTATY+ptSAi5a28kwLA\nzFOJbVnFxQds5YG/ruONDS2ljkhEpGSUFGZ+CirHcnbnf5OKB3zrFy+Ry2k5bREpT0oKqVqY93ek\n3niYa47O8cSKDfzgkRWljkpEpCSUFAAO/yJUjee4Ndfx6TmT+Y+Hl+uCNhEpS0oKABV18NErsdVP\nctUHXmb/Xev40u3P8qflQ3BDHxGRnYiSQpeDPwdTDyf1+29w52d3Ye+JNVzwk8X87OnVumWniJQN\nJYUuQQCfuQkMxvzqC/zsvIOYO20s37j/RT5/6zOs26LbdorI6KekkG/sNPj0f8HbLzDml+dyx+dm\n8a+nzGTxqs0c/x+PceczqzUzSURGNSWFnvY7AT55Hax8hOCnp/K5fTL87h8+xMwpdVx+34scf+1j\n3PTYSjY0d5Q6UhGRIWc7W3/53LlzffHixcU/0Yv3wINfh3Q7zPsyuaP/kV8s3crtT73Jc6u3EBjs\nWlfBFZ8D6U1DAAAQbElEQVScwXEzdiUIrPgxiYgMkpk96+5zByynpNCPpnfgoW/Ci/8N9bvD0f8A\nc87mtU1p7ntuLT9f9BabWjqZUJNkyphKPj9vGgtm7Up1qqg3tBMR2W4jIimY2QLg+0AMuNndr+qx\n/2zgUsCAJuAid/9rf8cc1qTQZfVT8JtLYf3zEK+A2afBtA/Rsa2B3yY/zh9XtvD0yk28va2dRMyY\nUJPi2P0nccwHJjBjch1Tx1YRU0tCREqo5EnBzGLAcuDjwBpgEXCmuy/NK/NB4BV332xmJwD/7O5H\n9HfckiQFAHd4/Y+w9Bfwwt2QaQ+373cSzDmLDAGL7UAefaOFFQ1NPLFiA+3p8MY9Y6oSHDl9PNMn\nVnPw7mPYrb6S+soEu9ZXkIxrWEdEim8kJIV5hF/yx0evLwdw9+/0UX4s8JK7T+nvuCVLCvk6W2HD\ncnjudnj2Nshlwu0V9bDHPBi3N5kJB7A0OZOlbeN55s3NLHlzM2u3tJHOvvt516TiVKdiHPWBCewz\nqZapYyvZfVwVu4+tZFx1EjO1LkRkaIyEpPBZYIG7nx+9/hxwhLt/pY/yFwP7d5Xvse9C4EKAPfbY\n49A333yzKDEPSmcrrHsOGpbCmkWw9lnYuhYy0XUNNbuEiaJyDOnKSaxJfYCGdAWNmUqe3FLHyi2w\n7O1tbG5Nv+ewVckYk2pTVCRiTB1byW71lSTjAZNqU0yqS1FXkSARC5g+oZrxNUmqkhrHEJG+FZoU\nRsQ3iZkdC3wBOLq3/e5+I3AjhC2FYQxtYMkqmHZU+Dj8gnBbLgcbXoU3/xKOR7z1FHS2kmjdyHSc\n6dFbPxFLQpCAaYeRrtuDrbFxNDCOhs4UjS1p3krX0+BjeGljmpdWvU1H1tnSaXiPmcSBQTIeYBhT\nx1ay5/hqaivi1FbEqatIUFcZpzIZJx4Y46qTTB1bSVUyTlUyRn1lglhgpLM5YoGRiseG9/MTkRGl\nmElhLbB73uup0bb3MLPZwM3ACe6+sYjxDJ8ggEkHhI/DvvDu9pYNsGU1tG2C1k3w1jPQ/A5sfoPE\nO0uZ0NLIBJwZvR7UIHBy9XWkqyeTiVVgnc2srz2Qd2wC2WyOllg9r7ek2NAQY2MmyeoOeKuziuZc\nBRXWSQ1trPPxbKGGAKeCTlqoCI9NmFymTahmXFUySioJ4oGxrT1DWzrDtPHV7FZfQUUiRioeUJmM\nU5OKUZNKUJ2KUZOKU5WKk4wFJGJGPBZQEQ+IxwKyOddgu8hOoJhJYRGwj5lNJ0wGZwBn5Rcwsz2A\n+4DPufvyIsYyMlRPCB9dZp/23v3ZNDQ3QPtW8Bw0vw3b1kPT2+HAdhAj2LyKVNtmUuk2qBvP3usf\nZ+/WDX2fM9l/SFlL0JyaRFtyAjnPsTFbDVs6yGWzNHo9TVQxKdjG3tk3eGLNTJ5O780Ymmmmkq1e\nTRNVxMiyzavJECNH2JJJ0clutomtVNMZq2JXb6Sx8gPUWgvbrI7GxBQqknEqEjFaO7N0pLPsk2ig\nLr0R2+NI2lqbiaWqqKlMUpNKUF8ZJp5UPCAVj/4mgu5xl8CMZCwgGTeSsRjJePDuIxb+TUXPg8Bo\nT2eJB2HiEpF3FXtK6onAtYRTUm91938zsy8BuPsNZnYz8Bmga5AgM1Cf14gYaB5psulwsLttM3Q0\nQ7ol/JvthJZGSLeFU2mT1bBxRbjdAogloXUjbFsXtljMoG1LWNaCcFvHNkjVwZjdYe0S6GwekpA7\nLUWGGFmLkbEkcc9QndtGgLONaupooYMErVSQ8YAOErR5ihZStHoFaeI4xi62mQQZWqig1SvC/VTQ\n6uFfgDHWjOE4Rpwsy3wa9TSTsxjpIEXGEhCvZIJtYavV02kpzCDrRqcbaUuRqKylPuFU0MGk3Ns0\nJSYxKb2OTakptKQmEpiRjVcTM8ficQgSxMyp3PAiQe0uVIzdjVS2ldbULlEkTpDtIJZtp7EjztjJ\ne5Ne/zIka5g67QNkE7V0tLWydcsGJo+vJ/AssbpJVMbCzsMKb6czNZbNTS3sMa6KuGeIx2PEYnHe\neWdd+NnEWwjqp9KZc2rqxtOeyVIRjxEExpbWTqqS8ffMfnN3TW4YxUo+0FwsSgollM3AljfDGxNl\n2sME0tkCQTxq3WQhlw1bOfEU1O4aJqXOlijJNEDl2PDvljfDsrnMu9N76yaH71m7JHyeboN0K57N\nkE23k+1owTua8c4WyHbiuRzpyonk4hUE6VbobCFIh49YppVYphU8R3tyDE6Au5PItlKV3kyOGBa1\na3ZWrZ6igk4CG7gOjV5PkjRxsjRbFWO8mS3Usjk2jjpvIkOMTA4I4jQFtWABU7JryRImzk6SZGMV\ndFiURIHx2UbagyraghriZIh7hqR3EJCjJR5+5liAmxHPdTKu/S3erDsES9WQSm9ll02L2Vw1nc7q\nXUlmmmmihk7iTM6upTM5FsulScer6aiYRLxjE6lkilSmCXA6KiaSTtYRz7QTz7YSz7bjQQwDsvEq\nOisnYrk0AU6QyxDPtBD3NEGqmlyyhkTr2+SSdWRTY/B0G/FEkk4StLdspSoZJx5PkqyswTyDZzrJ\npjuIeZZ4zTiCTBudm1aT62wjFk+S2+0gEt7JttZ2xoybRCKAWCJFSxqscxvJMZOJxxMEsTgWS0Au\nQ6aznSDbTjaTJpFIkd38Jtm3l5KcdiTU7Rb+EGvbjGN0ekCqdkL4I23cXjB5zqD+f1FSEOlNLhe2\npOLvjqWQbglbWxX1YcvJw+tL8Fy4PdMRtpDiqbB1VTU+THZjp0WtrAbAw5loZlGyS+PZNDZhHzKd\nbTRvXEcuXknQugHMMAyPpyCWoMI72LZ+Ban6XWjd0khbFuLpZoglCFI1dGSyOHGsdQOZXBbLpmkL\nqkm2vU0siNEWVNERVJHLZrFcJ8n6XcjmnK25SmqbVpLMtRKkW7B4is5MliDdSrZyHPGWBhLprXTG\nqsOWiDnpbI6a3DaCXJqG5B4YjmXaSXgHsWwHKTqIezhTLktAzLPkMDLESJOggyTg1HozuHcn3sBz\nJC1DnW8j8CxNVLGNairoJEmatMepDdoIyNGcqyBhWdo8SaV1sgub2UgdMbJ0EiakMTRTYWk6PE4b\nKdpIESeDE1BHCxX23tl8rZ6ikzgp0lRaJ2mPEZAjZk7ObcDEmnMjS0DCsgC87WNp8yTjrYk6ax2i\n/zkH9vzun2POF34wqPfuVLOPRIZNEIQtnXyx+nef1+5a2HG6xobG7x0+etHVERMn7H3rT8VB4d/6\n/osNq95rNTTy69n1w9TMcHfqck4664wLwmtGmzoz1KXiNDZ14B5ueyeXIeeOWwyPjuEOOYdNuTSW\nbiNrccimyQUpckGMjiy0dWYh2xG9N4ll24jF4rR0ZMKkl0xCPEF7Z4bWlmayliCWTBFPJMlmc7S3\nbCWXyzJm7ESqUnGWt7XS2byJplyK6mSMtqbNpHNONt1BdcKwZCXZ5k1ksxk8myaXzZDOGcmKalo9\njgVxstlOOjJGrn53Mk0bSLU3EuvYRluinpR3UhNLk+5sY2tsArNnzizif5WQkoKIlFT+OIaZEY8Z\n+TOjKxLhi6ljq4Y7tF5M7WXbnsMeRTFp6oWIiHRTUhARkW5KCiIi0k1JQUREuikpiIhINyUFERHp\npqQgIiLdlBRERKTbTrfMhZk18u4CettrAtDPkqKjkupcHlTn8rAjdd7T3ScOVGinSwo7wswWF7L2\nx2iiOpcH1bk8DEed1X0kIiLdlBRERKRbuSWFG0sdQAmozuVBdS4PRa9zWY0piIhI/8qtpSAiIv1Q\nUhARkW5lkxTMbIGZvWpmK8zsslLHM1TM7FYzazCzl/K2jTOz35vZa9HfsXn7Lo8+g1fN7PjSRD14\nZra7mT1iZkvN7GUz+1q0fTTXucLMnjGzv0Z1/na0fdTWuYuZxczsOTP7dfR6VNfZzFaZ2Ytm9ryZ\nLY62DW+dw9vYje4HEANeB/YCksBfgRmljmuI6vYh4BDgpbxt/we4LHp+GfDd6PmMqO4pYHr0mcRK\nXYftrO9uwCHR81pgeVSv0VxnA2qi5wngaeDI0VznvLp/HfgZ8Ovo9aiuM7AKmNBj27DWuVxaCocD\nK9x9pbt3AncBp5Q4piHh7o8Bm3psPgX4SfT8J8Cn8rbf5e4d7v4GsILws9lpuPt6d18SPW8CXgGm\nMLrr7O7eHL1MRA9nFNcZwMymAicBN+dtHtV17sOw1rlcksIU4K2812uibaPVLu6+Pnr+NrBL9HxU\nfQ5mNg04mPCX86iuc9SN8jzQAPze3Ud9nYFrgX8CcnnbRnudHXjYzJ41swujbcNa5/iOHkBGNnd3\nMxt1847NrAa4F/gHd9+Wf/P30Vhnd88Cc8xsDHC/mc3qsX9U1dnMPgE0uPuzZja/tzKjrc6Ro919\nrZlNAn5vZsvydw5HnculpbAW2D3v9dRo22j1jpntBhD9bYi2j4rPwcwShAnhp+5+X7R5VNe5i7tv\nAR4BFjC663wUcLKZrSLs7v2Imd3B6K4z7r42+tsA3E/YHTSsdS6XpLAI2MfMpptZEjgDeKDEMRXT\nA8DfRM//Bvhl3vYzzCxlZtOBfYBnShDfoFnYJLgFeMXdr8nbNZrrPDFqIWBmlcDHgWWM4jq7++Xu\nPtXdpxH+e/2ju5/DKK6zmVWbWW3Xc+A44CWGu86lHm0frgdwIuFMldeB/1XqeIawXncC64E0YZ/i\nF4DxwB+A14CHgXF55f9X9Bm8CpxQ6vgHUd+jCftdXwCejx4njvI6zwaei+r8EnBFtH3U1rlH/efz\n7uyjUVtnwtmRf40eL3d9Tw13nbXMhYiIdCuX7iMRESmAkoKIiHRTUhARkW5KCiIi0k1JQUREuikp\nSNkxs+bo7zQzO2uIj/2NHq//MpTHFyk2JQUpZ9OA7UoKZjbQ0jDvSQru/sHtjEmkpJQUpJxdBRwT\nrV3/P6NF5642s0Vm9oKZfRHAzOab2eNm9gCwNNr2i2jRspe7Fi4zs6uAyuh4P422dbVKLDr2S9F6\n+afnHftRM7vHzJaZ2U+jq7Yxs6ssvG/EC2b2vWH/dKQsaUE8KWeXARe7+ycAoi/3re5+mJmlgD+b\n2UNR2UOAWR4uUQzwt+6+KVp2YpGZ3evul5nZV9x9Ti/nOhWYAxwETIje81i072BgJrAO+DNwlJm9\nAnwa2N/dvWuZC5FiU0tB5F3HAZ+Plqh+mnB5gX2ifc/kJQSAr5rZX4GnCBcl24f+HQ3c6e5Zd38H\n+BNwWN6x17h7jnDZjmnAVqAduMXMTgVad7h2IgVQUhB5lwF/7+5zosd0d+9qKbR0FwqXcv4YMM/d\nDyJcl6hiB87bkfc8C8TdPUO4QuY9wCeA3+7A8UUKpqQg5ayJ8JaeXX4HXBQtzY2Z7RutVtlTPbDZ\n3VvNbH/CW2N2SXe9v4fHgdOjcYuJhLdR7XNFy+h+EfXuvhD4n4TdTiJFpzEFKWcvANmoG+g24PuE\nXTdLosHeRt699WG+3wJfivr9XyXsQupyI/CCmS1x97Pztt8PzCNcAdOBf3L3t6Ok0pta4JdmVkHY\ngvn64Koosn20SqqIiHRT95GIiHRTUhARkW5KCiIi0k1JQUREuikpiIhINyUFERHppqQgIiLd/j+l\n8V2ygL4fUwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x140084780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xl = np.arange(np.size(track_train_loss))\n",
    "plt.plot(xl, track_train_loss, label='training loss')\n",
    "plt.plot(xl, track_test_loss, label='testing loss')\n",
    "plt.title('Loss vs. iteration')\n",
    "\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss function')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the weights for evaluation in the desired format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "filehandler = open(\"multiclass_parameters.txt\",\"wb\")\n",
    "pickle.dump(w[0], filehandler)\n",
    "filehandler.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
